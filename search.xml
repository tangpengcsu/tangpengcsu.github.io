<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[scala 模式匹配]]></title>
      <url>%2F2017%2F03%2F14%2Fscala-pattern%2F</url>
      <content type="text"><![CDATA[概念scala 中的模式并不是指设计模式，它是数据结构上的模式，是用于描述一个结构的组成。 模式常量模式(constant patterns)包含常量变量和常量字面量12345scala&gt; val site = "alibaba.com"scala&gt; site match &#123; case "alibaba.com" =&gt; println("ok") &#125;scala&gt; val ALIBABA="alibaba.com"//注意这里常量必须以大写字母开头scala&gt; def foo(s:String) &#123; s match &#123; case ALIBABA =&gt; println("ok") &#125; &#125; 常量模式和普通的 if 比较两个对象是否相等(equals) 没有区别，并没有感觉到什么威力 变量模式(variable patterns)确切的说单纯的变量模式没有匹配判断的过程，只是把传入的对象给起了一个新的变量名。 1&gt; site match &#123; case whateverName =&gt; println(whateverName) &#125; 上面把要匹配的 site 对象用 whateverName 变量名代替，所以它总会匹配成功。不过这里有个约定，对于变量，要求必须是以小写字母开头，否则会把它对待成一个常量变量，比如上面的 whateverName 如果写成 WhateverName 就会去找这个 WhateverName 的变量，如果找到则比较相等性，找不到则出错。 变量模式通常不会单独使用，而是在多种模式组合时使用，比如 1List(1,2) match&#123; case List(x,2) =&gt; println(x) &#125; 里面的 x 就是对匹配到的第一个元素用变量 x 标记。 通配符模式(wildcard patterns)通配符用下划线表示：”_“ ，可以理解成一个特殊的变量或占位符。 单纯的通配符模式通常在模式匹配的最后一行出现，case _ =&gt; 它可以匹配任何对象，用于处理所有其它匹配不成功的情况。通配符模式也常和其他模式组合使用： 1&gt; List(1,2,3) match&#123; case List(_,_,3) =&gt; println("ok") &#125; 上面的 List(_,_,3) 里用了2个通配符表示第一个和第二个元素，这2个元素可以是任意类型通配符通常用于代表所不关心的部分，它不像变量模式可以后续的逻辑中使用这个变量。 构造器模式(constructor patterns)这个是真正能体现模式匹配威力的一个模式！ 我们来定义一个二叉树： 1234567scala&gt; :paste//抽象节点trait Node//具体的节点实现，有两个子节点case class TreeNode(v:String, left:Node, right:Node) extends Node//Tree，构造参数是根节点case class Tree(root:TreeNode) 这样我们构造一个根节点含有2个子节点的数： 1scala&gt;val tree = Tree(TreeNode("root",TreeNode("left",null,null),TreeNode("right",null,null))) 如果我们期望一个树的构成是根节点的左子节点值为”left”，右子节点值为”right”并且右子节点没有子节点那么可以用下面的方式匹配： 1234scala&gt; tree.root match &#123; case TreeNode(_, TreeNode("left",_,_), TreeNode("right",null,null)) =&gt; println("bingo") &#125; 只要一行代码就可以很清楚的描述，如果用java实现，是不是没这么直观呢？ 类型模式(type patterns)类型模式很简单，就是判断对象是否是某种类型： 1scala&gt; "hello" match &#123; case _:String =&gt; println("ok") &#125; 跟 isInstanceOf 判断类型的效果一样，需要注意的是scala匹配泛型时要注意，比如 1234scala&gt; def foo(a:Any) = a match &#123; case a :List[String] =&gt; println("ok"); case _ =&gt; &#125; 如果使用了泛型，它会被擦拭掉，如同java的做法，所以上面的 List[String] 里的String运行时并不能检测 foo(List(“A”)) 和 foo(List(2)) 都可以匹配成功。实际上上面的语句编译时就会给出警告，但并不出错。 通常对于泛型直接用通配符替代，上面的写为 case a : List[_] =&gt; … 变量绑定模式 (variable binding patterns)这个和前边的变量模式有什么不同？看一下代码就清楚了： 依然是上面的TreeNode，如果我们希望匹配到左边节点值为”left”就返回这个节点的话： 123scala&gt; tree.root match &#123; case TreeNode(_, leftNode@TreeNode("left",_,_), _) =&gt; leftNode &#125; 用@符号绑定 leftNode变量到匹配到的左节点上，只有匹配成功才会绑定 模式匹配方法 面向对象的分解 (decomposition) 访问器模式 (visitor) 类型测试/类型造型 (type-test/type-cast) typecase 样本类 (case class) 抽取器 (extractor) 样本类(case class)本质上case class是个语法糖，对你的类构造参数增加了getter访问，还有toString, hashCode, equals 等方法； 最重要的是帮你实现了一个伴生对象，这个伴生对象里定义了apply 方法和 unapply 方法。 apply方法是用于在构造对象时，减少new关键字；而unapply方法则是为模式匹配所服务。 这两个方法可以看做两个相反的行为，apply是构造(工厂模式)，unapply是分解(解构模式)。 case class在暴露了它的构造方式，所以要注意应用场景：当我们想要把某个类型暴露给客户，但又想要隐藏其数据表征时不适宜。 抽取器(extrator)抽取器是指定义了 unapply 方法的 object。在进行模式匹配的时候会调用该方法。 unapply 方法接受一个数据类型，返回另一数据类型，表示可以把入参的数据解构为返回的数据。 比如 12345class Aclass B(val a:A)object TT &#123; def unapply(b:B) = Some(new A)&#125; 这样定义了抽取器 TT 后，看看模式匹配： 12val b = new B(new A);b match&#123; case TT(a) =&gt; println(a) &#125; 直观上以为 要拿 b 和 TT 类型匹配，实际被翻译为 1TT.unapply(b) match&#123; case Some(…) =&gt; … &#125; 它与上面的 case class 相比，相当于自己手动实现 unapply，这也带来了灵活性。 后续会专门介绍一下 extrator，这里先看一下 extractor 怎么实现 case class 无法实现的“表征独立”(representation independence) 比如我们想要暴露的类型为 A 1234567891011121314151617181920//定义为抽象类型trait A//然后再实现一个具体的子类，有2个构造参数class B (val p1:String, val p2:String) extends A//定义一个抽取器object MM&#123; //抽取器中apply方法是可选的，这里是为了方便构造A的实例 def apply(p1:String, p2:String) : A = new B(p1,p2); //把A分解为(String,String) def unapply(a:A) : Option[(String, String)] = &#123; if (a.isInstanceOf[B]) &#123; val b = a.asInstanceOf[B] return Some(b.p1, b.p2) &#125; None &#125;&#125; 这样客户只需要通过 MM(x,y) 来构造和模式匹配了。客户只需要和 MM 这个工厂/解构角色打交道，A 的实现怎么改变都不受影响。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring MVC]]></title>
      <url>%2F2017%2F03%2F02%2Fspring-mvc%2F</url>
      <content type="text"><![CDATA[控制器（controller）控制器作为应用程序逻辑的处理入口，它会负责去调用你已经实现的一些服务。通常，一个控制器会接收并解析用户的请求，然后把它转换成一个模型交给视图，由视图渲染出页面最终呈现给用户。Spring对控制器的定义非常宽松，这意味着你在实现控制器时非常自由。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[scala 类型系统]]></title>
      <url>%2F2017%2F03%2F02%2Fscala-type-system%2F</url>
      <content type="text"><![CDATA[upper bounds 上界 标识： &lt;: 123def pr(list : List[_ &lt;: Any]) &#123; list.foreach(print)&#125; lower bounds 下界 标识： &gt;: 123def append[T &gt;: String] (buf : ListBuffer[T]) = &#123; buf.append( "hi")&#125; view bounds 标识 &lt;% &lt;% 的意思是 “view bounds” (视界)，它比 &lt;: 适用的范围更广，除了所有的子类型，还允许隐式转换过去的类型 适用于 方法与类 ，但是不适用于 trait（特征） 1def method [A &lt;% B](arglist): R = ... context boundsmutiple bounds不能同时有多个upper bounds 或 lower bounds，变通的方式是使用复合类型 12T &lt;: A with BT &gt;: A with B 可以同时有upper bounds 和 lower bounds 1T &gt;: A &lt;: B 这种情况 lower bounds 必须写在前边，upper bounds写在后边，位置不能反。同时A要符合B的子类型，A与B不能是两个无关的类型。 可以同时有多个view bounds 1T &lt;% A &lt;% B 这种情况要求必须同时存在 T=&gt;A的隐式转换，和T=&gt;B的隐式转换。 1234567scala&gt; implicit def string2A(s:String) = new Ascala&gt; implicit def string2B(s:String) = new Bscala&gt; def foo[ T &lt;% A &lt;% B](x:T) = println("OK")scala&gt; foo("test")OK 可以同时有多个context bounds 1T : A : B 这种情况要求必须同时存在A[T]类型的隐式值，和B[T]类型的隐式值。 12345678910class A[T];class B[T];implicit val a = new A[Int]implicit val b = new B[Int]def foo[ T : A : B ](i:T) = println("OK")foo(2)OK 协变、逆变 如果一个类型是协变或者逆变的，那么这个类型即为可变-variance类型，否则为不可变-invariance的。 在类型定义申明时，+ 表示协变，- 表示逆变 协变：covariance1trait List[+T] // 在类型定义时(declaration-site)声明为协变 如 A 继承 B，便有 List[A] 继承 List[B] 逆变：contravariance如 A 继承 B，便有 List[B] 继承 List[A] 要注意 variance 并不会被继承，父类声明为 variance，子类如果想要保持，仍需要声明: 123456789101112trait A[+T]class C[T] extends A[T] // C是invariant的class X; class Y extends X;val t:C[X] = new C[Y]&lt;console&gt;:11: error: type mismatch; found : C[Y] required: C[X]Note: Y &lt;: X, but class C is invariant in type T.You may wish to define T as +T instead. (SLS 4.5) 必须也对 C 声明为协变的才行： 1234class C[+T] extends A[T]val t:C[X] = new C[Y]t: C[X] = C@6a079142 =&gt; 在 scala 中的含义 匿名函数定义， 左边是参数，右边是函数实现体：(x: Int)=&gt;{} 函数类型的声明,左边是参数类型，右边是方法返回值类型：(Int)=&gt;(Int) By-name-parameter：f(p :=&gt;Int) case 语句中 case x =&gt; y]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[集合类解析 笔记]]></title>
      <url>%2F2017%2F02%2F27%2Fjava-collection%2F</url>
      <content type="text"><![CDATA[一、集合与数组数组（可以存储基本数据类型）是用来存现对象的一种容器，但是数组的长度固定，不适合在对象数量未知的情况下使用。 集合（只能存储对象，对象类型可以不一样）的长度可变，可在多数情况下使用。 二、层次关系如图所示：图中，实线边框的是实现类，折线边框的是抽象类，而点线边框的是接口 Collection 接口是集合类的根接口，Java 中没有提供这个接口的直接的实现类。但是却让其被继承产生了两个接口，就是 Set 和 List。Set 中不能包含重复的元素。List 是一个有序的集合，可以包含重复的元素，提供了按索引访问的方式。 Map 是 Java.util 包中的另一个接口，它和 Collection 接口没有关系，是相互独立的，但是都属于集合类的一部分。Map 包含了 key-value 对。Map 不能包含重复的 key，但是可以包含相同的 value。 Iterator，所有的集合类，都实现了 Iterator 接口，这是一个用于遍历集合中元素的接口，主要包含以下三种方法： hasNext() 是否还有下一个元素。 next() 返回下一个元素。 remove() 删除当前元素。 三、几种重要的接口和类简介1、List（有序、可重复）List 里存放的对象是有序的，同时也是可以重复的，List 关注的是索引，拥有一系列和索引相关的方法，查询速度快。因为往 list 集合里插入或删除数据时，会伴随着后面数据的移动，所有插入删除数据速度慢。 2、Set（无序、不能重复）Set 里存放的对象是无序，不能重复的，集合中的对象不按特定的方式排序，只是简单地把对象加入集合中。 3、Map（键值对、键唯一、值不唯一）Map 集合中存储的是键值对，键不能重复，值可以重复。根据键得到值，对 map 集合遍历时先得到键的 set 集合，对 set 集合进行遍历，得到相应的值。 对比如下： 是否有序 是否允许元素重复 Collection 否 是 List 是 是 Set AbstractSet 否 否 HashSet 否 否 TreeSet 是（用二叉排序树） 否 Map AbstractMap 否 使用 key-value 来映射和存储数据，key 必须唯一，value 可以重复 HashMap 否 使用 key-value 来映射和存储数据，key 必须唯一，value 可以重复 TreeMap 是（用二叉排序树） 使用 key-value 来映射和存储数据，key 必须唯一，value 可以重复 四、遍历在类集中提供了以下四种的常见输出方式： Iterator：迭代输出，是使用最多的输出方式。 ListIterator：是 Iterator 的子接口，专门用于输出 List 中的内容。 foreach 输出：JDK1.5 之后提供的新功能，可以输出数组或集合。 for 循环 代码示例如下： for 的形式：for（int i=0;i&lt;arr.size();i++）{...} foreach 的形式： for（int i：arr）{...} iterator 的形式： 12Iterator it = arr.iterator();while(it.hasNext())&#123; object o =it.next(); ...&#125; 五、ArrayList 和 LinkedListArrayList 和 LinkedList 在用法上没有区别，但是在功能上还是有区别的。LinkedList 经常用在增删操作较多而查询操作很少的情况下，ArrayList 则相反。 六、Map 集合实现类：HashMap、Hashtable、LinkedHashMap 和 TreeMap HashMapHashMap 是最常用的 Map，它根据键的 HashCode 值存储数据，根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的。因为键对象不可以重复，所以 HashMap 最多只允许一条记录的键为 Null，允许多条记录的值为 Null，是非同步的 HashtableHashtable 与 HashMap 类似，是 HashMap 的线程安全版，它支持线程的同步，即任一时刻只有一个线程能写 Hashtable，因此也导致了 Hashtale 在写入时会比较慢，它继承自 Dictionary 类，不同的是它不允许记录的键或者值为 null，同时效率较低。 ConcurrentHashMap线程安全，并且锁分离。ConcurrentHashMap 内部使用段 (Segment) 来表示这些不同的部分，每个段其实就是一个小的 hash table，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。 LinkedHashMapLinkedHashMap 保存了记录的插入顺序，在用 Iteraor 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的，在遍历的时候会比 HashMap 慢，有 HashMap 的全部特性。 TreeMapTreeMap 实现 SortMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序（自然顺序），也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的。不允许 key 值为空，非同步的； map 的遍历第一种：KeySet()将 Map 中所有的键存入到 set 集合中。因为 set 具备迭代器。所有可以迭代方式取出所有的键，再根据 get 方法。获取每一个键对应的值。 keySet(): 迭代后只能通过 get() 取 key 。 取到的结果会乱序，是因为取得数据行主键的时候，使用了 HashMap.keySet() 方法，而这个方法返回的 Set 结果，里面的数据是乱序排放的。典型用法如下： 123456789101112Map map = new HashMap();map.put("key1","lisi1");map.put("key2","lisi2");map.put("key3","lisi3");map.put("key4","lisi4"); // 先获取 map 集合的所有键的 set 集合，keyset（）Iterator it = map.keySet().iterator(); // 获取迭代器while(it.hasNext())&#123; Object key = it.next(); System.out.println(map.get(key));&#125; 第二种：entrySet（）12345678910111213Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() // 返回此映射中包含的映射关系的 Set 视图。（一个关系就是一个键 - 值对），就是把 (key-value) 作为一个整体一对一对地存放到 Set 集合当中的。Map.Entry 表示映射关系。entrySet()：迭代后可以 e.getKey()，e.getValue()两种方法来取 key 和 value。返回的是 Entry 接口。典型用法如下：Map map = new HashMap();map.put("key1","lisi1");map.put("key2","lisi2");map.put("key3","lisi3");map.put("key4","lisi4");// 将 map 集合中的映射关系取出，存入到 set 集合Iterator it = map.entrySet().iterator();while(it.hasNext())&#123; Entry e =(Entry) it.next(); System.out.println("键"+e.getKey () + "的值为" + e.getValue());&#125; 推荐使用第二种方式，即 entrySet() 方法，效率较高。 对于 keySet 其实是遍历了 2 次，一次是转为 iterator，一次就是从 HashMap 中取出 key 所对于的 value。而 entryset 只是遍历了第一次，它把 key 和 value 都放到了 entry 中，所以快了。两种遍历的遍历时间相差还是很明显的。 七、主要实现类区别小结Vector 和 ArrayList vector 是线程同步的，所以它也是线程安全的，而 arraylist 是线程异步的，是不安全的。如果不考虑到线程的安全因素，一般用 arraylist 效率比较高。 如果集合中的元素的数目大于目前集合数组的长度时，vector 增长率为目前数组长度的 100%，而 arraylist 增长率为目前数组长度的 50%。如果在集合中使用数据量比较大的数据，用 vector 有一定的优势。 如果查找一个指定位置的数据，vector 和 arraylist 使用的时间是相同的，如果频繁的访问数据，这个时候使用 vector 和 arraylist 都可以。而如果移动一个指定位置会导致后面的元素都发生移动，这个时候就应该考虑到使用 linklist, 因为它移动一个指定位置的数据时其它元素不移动。 ArrayList 和 Vector 是采用数组方式存储数据，此数组元素数大于实际存储的数据以便增加和插入元素，都允许直接序号索引元素，但是插入数据要涉及到数组元素移动等内存操作，所以索引数据快，插入数据慢，Vector 由于使用了 synchronized 方法（线程安全）所以性能上比 ArrayList 要差，LinkedList 使用双向链表实现存储，按序号索引数据需要进行向前或向后遍历，但是插入数据时只需要记录本项的前后项即可，所以插入数度较快。 arraylist 和 linkedlist ArrayList 是实现了基于动态数组的数据结构，LinkedList 基于链表的数据结构。 对于随机访问 get 和 set，ArrayList 觉得优于 LinkedList，因为 LinkedList 要移动指针。 对于新增和删除操作 add 和 remove，LinedList 比较占优势，因为 ArrayList 要移动数据。 这一点要看实际情况的。若只对单条数据插入或删除，ArrayList 的速度反而优于 LinkedList。但若是批量随机的插入删除数据，LinkedList 的速度大大优于 ArrayList. 因为 ArrayList 每插入一条数据，要移动插入点及之后的所有数据。 HashMap 与 TreeMap HashMap 通过 hashcode 对其内容进行快速查找，而 TreeMap 中所有的元素都保持着某种固定的顺序，如果你需要得到一个有序的结果你就应该使用 TreeMap（HashMap 中元素的排列顺序是不固定的）。 在 Map 中插入、删除和定位元素，HashMap 是最好的选择。但如果您要按自然顺序或自定义顺序遍历键，那么 TreeMap 会更好。使用 HashMap 要求添加的键类明确定义了 hashCode() 和 equals() 的实现。 两个 map 中的元素一样，但顺序不一样，导致 hashCode() 不一样。 同样做测试： 在 HashMap 中，同样的值的 map, 顺序不同，equals 时，false; 而在 treeMap 中，同样的值的 map, 顺序不同, equals 时，true，说明，treeMap 在 equals() 时是整理了顺序了的。 HashTable 与 HashMap 同步性: Hashtable 是线程安全的，也就是说是同步的，而 HashMap 是线程序不安全的，不是同步的。 HashMap 允许存在一个为 null 的 key，多个为 null 的 value 。 hashtable 的 key 和 value 都不允许为 null。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective Java 笔记]]></title>
      <url>%2F2017%2F02%2F27%2Fjava-effective%2F</url>
      <content type="text"><![CDATA[1. 考虑静态工厂方法代替构造器优势: 它们有名称。 不必在每次调用它们的时候都创建一个新对象。 它们可以返回原返回类型的任何子类型的对象。 在创建参数化类型实例的时候，它们使代码变得更加简洁。 缺点： 类如果含共有的或者受保护的构造器，就不能被子类化。 它们与其他的静态方法实际上没有任何区别。 2. 遇到多个构造器参数时要考虑用构建器 静态工厂和构造器有个共同的局限性：它们都不能很好的扩展到大量的可选参数。 重叠构造器重叠构造器模式可行，但是当有血多参数的时候，客户端代码会很难编写，并且仍然较难阅读。 JavaBeans 模式在这种模式下，调用一个无参构造器来创建对象，然后调用 setter 方法来设置每个必要的参数，以及每个相关的可选参数。 这种模式弥补了重叠构造器模式的不足。 但是，JavaBeans 模式自身有着很严重的缺点。因为构造过程被分到了几个调用中，在构造过程中 JavaBean 可能处于不一致的状态。类无法仅仅通过校验构造器参数的有效性来保证一致性。同时，JavaBeans 模式也阻止了把类做成不可变的可能，这就需要程序员付出额外的努力来保证它的线程安全。 Builder 模式它既能保证重叠构造器模式那样的安全性，又能保证像 JavaBeans 模式那么好的可读性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class NutritionFacts &#123; private final int servingSize; private final int servings; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder&#123; //必选参数 private final int servingSize; private final int servings; //可选参数 private int calories = 0; private int fat = 0; private int carbohydrate = 0; private int sodium = 0; public Builder(int servingSize, int servings)&#123; this.servingSize = servingSize; this.servings = servings; &#125; public Builder calories(int val)&#123; calories = val; return this; &#125; public Builder fat(int val )&#123; fat = val; return this; &#125; public Builder carbohydrate(int val)&#123; carbohydrate = val; return this; &#125; public NutritionFacts build()&#123; return new NutritionFacts(this); &#125; &#125; private NutritionFacts(Builder builder)&#123; servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; &#125;&#125; 不直接生成想要的对象，而是让客户端利用所有必要的参数调用构造器（或者静态工厂），得到一个 builder 对象。然后客户端在 builder 对象上调用类似于 setter 的方法，来设置每个相关的可选参数。最后，客户端调用无参的 build 方法来生成不可变的对象。这个 builder 是他构建的类的静态成员类。 与构造器相比，builder 的微略优势在于，builder 可以有多个可变（varargs）参数。 builder 模式十分灵活，可以利用单个 builder 构建多个对象。 设置了参数的 builder 生成了一个很好的抽象方法。 总之，如果类的构造器或者静态工厂中具有多个参数，设计这种类时，Builder 模式就是中不错的选择，特别是当大多数参数都是可选的时候。与使用传统的重叠构造器模式相比，使用 Builder 模式的客户端代码将更容易阅读和编写，构建器也比 JavaBeans 更加安全。 4. 通过私有构造器强化不可实例化的能力对于自包含静态方法和静态域的。虽然名声不好，但是它们也确实特有用处。 这样的工具类不希望被实例化，实例化对它没有任何意义。 我们只需要让这个类包含私有构造器，它就不能被实例化了。 但是这样也有缺陷，它使得一个类不能被子类化。所有的构造器都必须显示或者隐式地调用超类构造器，在这种情况下，子类就没有可访问的超类构造器可调用了。 13. 使类和成员的可访问性最小封装/信息隐藏（information hiding/encapsulation）概念封装/信息隐藏：模块之间只通过它们的 API 进行通信，一个模块不需要知道其他模块的内部工作情况。 为什么要做信息隐藏它可以有效地接触组成系统的各模块之间的耦合关系，使得这些模块可以独立地开发、测试、优化、使用、理解和修改。 规则尽可能地使每个类或者成员不被外界访问。顶层的（非嵌套的）类和接口，只有两种可能的访问级别：包级私有的（package-private）和公有的（public）。 对于成员（域、方法、嵌套类和嵌套接口）有四种可能的访问级别： 私有的（private） 包级私有的（package-private） 受保护的（protected） 公有的（public） 对于共有成员，当访问级别从私有变成保护级别时，会大大增强可访问性。受保护的成员是类的导出的 API 的一部分，必须永远得到支持。导出的类受保护成员也代表了该类对于某个实现细节的公开承诺。受保护的成员应该尽量少用。 实例域决不能是公有的包含公有可变域的类并不是线程安全的。即使域是 final 引用，并且引用不可变的对象，当把这个域变成公有的时候，也就放弃了“切换到一种新的内部数据标识法”的灵活性。 同样的建议也是适用于静态域。只有一种例外情况：假设常量构成了类提供的整个抽象中的一部分，可以通过公有的静态 final 域来暴露这些常量。 注意，长度非零的数组总是可变的，所以，类具有公有的非静态 final 数组域，或者返回这种域的访问方法，这几乎总是错误的。如果类具有这样的域或者访问方法，客户端将能够修改数组中的内容。这是安全漏洞的一个常见根源。 总而言之，你应该始终尽可能地讲题可访问性。你再仔细地设计了一个最小的公有 API 之后，应该防止把任何散乱的类、接口和成员变成 API 的一部风。除了公有静态 final 域的特殊情况外，公有类都不应该包含公有域。并且要确保公有静态 final 域所应用的对象是不可变的。 15. 在公有类中使用访问方法而非公有域公有类不应该直接暴露数据域 如果类可以在它所在的包的外部进行访问，就提供访问方法，以保留将来改变该类的内部表示法的灵活性。如果公有类暴露了它的数据域，要想在将来改变其内部表示法是不可能的，因为共有类的客户端代码已经遍布各处了。 然而，如果类是包级私有的，或者是私有的嵌套类，直接暴露它的数据域并没有本职的错误。 让公有类直接暴露域虽然重来都不是种好办法，但是如果域是不可变的，这种危害就比较小一些。 总之，公有类永远都不应该暴露可变的域。虽然还是有问题，但是让公有类暴露不可变的域其危害比较小。但是，有时候会需要包级私有的或者私有的嵌套类来暴露域，无论这个类是可变的还是不可变的。 16. 复合优于继承继承是实现代码重用的有力手段。 继承打破了封装性子类依赖于超类中特定的实现细节。超类的实现有可能会随着发行版本的不同而有所变化，如果真的发生了变化，子类可能会遭到破坏，即使它的代码完全没有改变。因而，子类必须要跟着其超类的更新而演变，除非超类是专门为了扩展而设计的，并且具有很好的文档说明。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java 笔记]]></title>
      <url>%2F2017%2F02%2F27%2Fjava-note%2F</url>
      <content type="text"><![CDATA[协变、逆变与不变 逆变 与 协变 用来 描述类型转换（type transformation）后的继承 关系，其定义：如果 X、Y 表示类型，f(⋅) 表示类型转换，≤ 表示继承关系（比如，A≤B 表示 A 是由 B 派生出来的子类）。 f(⋅) 是协变（Covariant）的，当 X≤Y 时，f(X)≤f(Y) 成立；如数组，当然，泛型也可以通过通配符（extends、super）来实现协变与逆变 f(⋅) 是逆变（Contravariant）的，当 X≤Y 时，f(Y)≤f(X) 成立 f(⋅) 是不变（Invariant）的，当 X≤Y 时上述两个式子均不成立，即 f(X) 与 f(Y) 相互之间没有继承关系。如泛型]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论进阶 - Paxos]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-paxos%2F</url>
      <content type="text"><![CDATA[引言分布式系统理论基础 – 一致性、2PC 和 3PC 一文介绍了一致性、达成一致性需要面临的各种问题以及 2PC、3PC 模型，Paxos 协议在节点宕机恢复、消息无序或丢失、网络分化的场景下能保证决议的一致性，是被讨论最广泛的一致性协议。 Paxos 协议同时又以其 “艰深晦涩” 著称，下面结合 Paxos Made Simple、The Part-Time Parliament 两篇论文，尝试通过 Paxos 推演、学习和了解 Paxos 协议。 Basic Paxos何为一致性问题？简单而言，一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点之间如何达成决议的问题，作为解决一致性问题的协议，Paxos 的核心是节点间如何确定并只确定一个值 (value)。 也许你会疑惑只确定一个值能起什么作用，在 Paxos 协议里确定并只确定一个值是确定多值的基础，如何确定多值将在第二部分 Multi Paxos 中介绍，这部分我们聚焦在 “Paxos 如何确定并只确定一个值” 这一问题上。 和 2PC 类似，Paxos 先把节点分成两类，发起提议 (proposal) 的一方为 proposer，参与决议的一方为 acceptor。假如只有一个 proposer 发起提议，并且节点不宕机、消息不丢包，那么 acceptor 做到以下这点就可以确定一个值： P1. 一个 acceptor 接受它收到的第一项提议当然上面要求的前提条件有些严苛，节点不能宕机、消息不能丢包，还只能由一个 proposer 发起提议。我们尝试放宽条件，假设多个 proposer 可以同时发起提议，又怎样才能做到确定并只确定一个值呢？ 首先 proposer 和 acceptor 需要满足以下两个条件： proposer 发起的每项提议分别用一个 ID 标识，提议的组成因此变为 (ID, value) acceptor 可以接受 (accept) 不止一项提议，当多数(quorum) acceptor 接受一项提议时该提议被确定(chosen) 注意以上 “接受” 和“确定”的区别 我们约定后面发起的提议的 ID 比前面提议的 ID 大，并假设可以有多项提议被确定，为做到确定并只确定一个值 acceptor 要做到以下这点： P2. 如果一项值为 v 的提议被确定，那么后续只确定值为 v 的提议 乍看这个条件不太好理解，谨记目标是 “确定并只确定一个值” 由于一项提议被确定 (chosen) 前必须先被多数派 acceptor 接受(accepted)，为实现 P2，实质上 acceptor 需要做到： P2a. 如果一项值为 v 的提议被确定，那么 acceptor 后续只接受值为 v 的提议满足 P2a 则 P2 成立 (P2a =&gt; P2)。 目前在多个 proposer 可以同时发起提议的情况下，满足 P1、P2a 即能做到确定并只确定一个值。如果再加上节点宕机恢复、消息丢包的考量呢？ 假设 acceptor c 宕机一段时间后恢复，c 宕机期间其他 acceptor 已经确定了一项值为 v 的决议但 c 因为宕机并不知晓；c 恢复后如果有 proposer 马上发起一项值不是 v 的提议，由于条件 P1，c 会接受该提议，这与 P2a 矛盾。为了避免这样的情况出现，进一步地我们对 proposer 作约束： P2b. 如果一项值为 v 的提议被确定，那么 proposer 后续只发起值为 v 的提议满足 P2b 则 P2a 成立 (P2b =&gt; P2a =&gt; P2)。 P2b 约束的是提议被确定 (chosen) 后 proposer 的行为，我们更关心提议被确定前 proposer 应该怎么做： P2c. 对于提议 (n,v)，acceptor 的多数派 S 中，如果存在 acceptor 最近一次(即 ID 值最大) 接受的提议的值为 v’，那么要求 v = v’；否则 v 可为任意值满足 P2c 则 P2b 成立 (P2c =&gt; P2b =&gt; P2a =&gt; P2)。 条件 P2c 是 Basic Paxos 的核心，光看 P2c 的描述可能会觉得一头雾水，我们通过 The Part-Time Parliament 中的例子加深理解： 提议 ID 提议值 Acceptor A B C D E 2 a x x x o - 5 b x x o - x 14 ? - o - x o 27 ? o - o o - 29 ? - o x x - 假设有 A~E 5 个 acceptor， - 表示 acceptor 因宕机等原因缺席当次决议 x 表示 acceptor 不接受提议 o 表示接受提议 多数派 acceptor 接受提议后提议被确定，以上表格对应的决议过程如下： ID 为 2 的提议最早提出，根据 P2c 其提议值可为任意值，这里假设为 a acceptor A/B/C/E 在之前的决议中没有接受 (accept) 任何提议，因而 ID 为 5 的提议的值也可以为任意值，这里假设为 b acceptor B/D/E，其中 D 曾接受 ID 为 2 的提议，根据 P2c，该轮 ID 为 14 的提议的值必须与 ID 为 2 的提议的值相同，为 a acceptor A/C/D，其中 D 曾接受 ID 为 2 的提议、C 曾接受 ID 为 5 的提议，相比之下 ID 5 较 ID 2 大，根据 P2c，该轮 ID 为 27 的提议的值必须与 ID 为 5 的提议的值相同，为 b；该轮决议被多数派 acceptor 接受，因此该轮决议得以确定 acceptor B/C/D，3 个 acceptor 之前都接受过提议，相比之下 C、D 曾接受的 ID 27 的 ID 号最大，该轮 ID 为 29 的提议的值必须与 ID 为 27 的提议的值相同，为 b 以上提到的各项约束条件可以归纳为 3 点，如果 proposer/acceptor 满足下面 3 点，那么在少数节点宕机、网络分化隔离的情况下，在 “确定并只确定一个值” 这件事情上可以保证一致性(consistency)： B1(ß): ß 中每一轮决议都有唯一的 ID 标识 B2(ß): 如果决议 B 被 acceptor 多数派接受，则确定决议 B B3(ß): 对于 ß 中的任意提议 B(n,v)，acceptor 的多数派中如果存在 acceptor 最近一次 (即 ID 值最大) 接受的提议的值为 v’，那么要求 v = v’；否则 v 可为任意值 希腊字母 ß 表示多轮决议的集合，字母 B 表示一轮决议 另外为保证 P2c，我们对 acceptor 作两个要求： 记录曾接受的 ID 最大的提议，因 proposer 需要问询该信息以决定提议值 在回应提议 ID 为 n 的 proposer 自己曾接受过 ID 最大的提议时，acceptor 同时保证 (promise) 不再接受 ID 小于 n 的提议 至此，proposer/acceptor 完成一轮决议可归纳为 prepare 和 accept 两个阶段。prepare 阶段 proposer 发起提议问询提议值、acceptor 回应问询并进行 promise；accept 阶段完成决议，图示如下： 还有一个问题需要考量，假如 proposer A 发起 ID 为 n 的提议，在提议未完成前 proposer B 又发起 ID 为 n+1 的提议，在 n+1 提议未完成前 proposer C 又发起 ID 为 n+2 的提议…… 如此 acceptor 不能完成决议、形成活锁 (livelock)，虽然这不影响一致性，但我们一般不想让这样的情况发生。解决的方法是从 proposer 中选出一个 leader，提议统一由 leader 发起。 最后我们再引入一个新的角色：learner，learner 依附于 acceptor，用于习得已确定的决议。以上决议过程都只要求 acceptor 多数派参与，而我们希望尽量所有 acceptor 的状态一致。如果部分 acceptor 因宕机等原因未知晓已确定决议，宕机恢复后可经本机 learner 采用 pull 的方式从其他 acceptor 习得。 Multi Paxos通过以上步骤分布式系统已经能确定一个值，“只确定一个值有什么用？这可解决不了我面临的问题。” 你心中可能有这样的疑问。 其实不断地进行 “确定一个值” 的过程、再为每个过程编上序号，就能得到具有全序关系 (total order) 的系列值，进而能应用在数据库副本存储等很多场景。我们把单次 “确定一个值” 的过程称为实例(instance)，它由 proposer/acceptor/learner 组成，下图说明了 A/B/C 三机上的实例： 不同序号的实例之间互相不影响，A/B/C 三机输入相同、过程实质等同于执行相同序列的状态机 (state machine) 指令 ，因而将得到一致的结果。 proposer leader 在 Multi Paxos 中还有助于提升性能，常态下统一由 leader 发起提议，可节省 prepare 步骤 (leader 不用问询 acceptor 曾接受过的 ID 最大的提议、只有 leader 提议也不需要 acceptor 进行 promise) 直至发生 leader 宕机、重新选主。 小结以上介绍了 Paxos 的推演过程、如何在 Basic Paxos 的基础上通过状态机构建 Multi Paxos。Paxos 协议比较 “艰深晦涩”，但多读几遍论文一般能理解其内涵，更难的是如何将 Paxos 真正应用到工程实践。 微信后台开发同学实现并开源了一套基于Paxos协议的多机状态拷贝类库 PhxPaxos，PhxPaxos 用于将单机服务扩展到多机，其经过线上系统验证并在一致性保证、性能等方面作了很多考量。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论基础 -- 一致性、2PC 和 3PC]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-consistency%2F</url>
      <content type="text"><![CDATA[引言狭义的分布式系统 指 由网络连接的计算机系统，每个节点独立地承担计算或存储任务，节点间通过网络协同工作。广义的分布式系统 是一个相对的概念，正如 Leslie Lamport 所说： What is a distributed systeme. Distribution is in the eye of the beholder.To the user sitting at the keyboard, his IBM personal computer is a nondistributed system.To a flea crawling around on the circuit board, or to the engineer who designed it, it’s very much a distributed system. 一致性 是分布式理论中的 根本性问题，近半个世纪以来，科学家们围绕着一致性问题提出了很多理论模型，依据这些理论模型，业界也出现了很多工程实践投影。下面我们从一致性问题、特定条件下解决一致性问题的两种方法 (2PC、3PC) 入门，了解最基础的分布式系统理论。 一致性 (consensus)何为一致性问题？简单而言，一致性问题 就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行 数据库事务提交 (commit transaction)、Leader 选举、序列号生成 等都会遇到一致性问题。这个问题在我们的日常生活中也很常见，比如牌友怎么商定几点在哪打几圈麻将： 假设一个具有 N 个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性： 全认同 (agreement): 所有 N 个节点都认同一个结果 值合法 (validity): 该结果必须由 N 个节点中的节点提出 可结束 (termination): 决议过程在一定时间内结束，不会无休止地进行下去 有人可能会说，决定什么时候在哪搓搓麻将，4 个人商量一下就 ok，这不很简单吗？ 但就这样看似简单的事情，分布式系统实现起来并不轻松，因为它面临着这些问题： 消息传递异步无序(asynchronous): 现实网络不是一个可靠的信道，存在消息延时、丢失，节点间消息传递做不到同步有序 (synchronous) 节点宕机(fail-stop): 节点持续宕机，不会恢复 节点宕机恢复(fail-recover): 节点宕机一段时间后恢复，在分布式系统中最常见 网络分化(network partition)**: 网络链路出现问题，将 N 个节点隔离成多个部分 拜占庭将军问题(byzantine failure): 节点或宕机或逻辑失败，甚至不按套路出牌抛出干扰决议的信息 假设现实场景中也存在这样的问题，我们看看结果会怎样： 1234567891011121314我: 老王，今晚 7 点老地方，搓够 48 圈不见不散！……（第二天凌晨 3 点） 隔壁老王: 没问题！ // 消息延迟我: ……----------------------------------------------我: 小张，今晚 7 点老地方，搓够 48 圈不见不散！小张: No …… （两小时后……）小张: No problem！ // 宕机节点恢复我: ……-----------------------------------------------我: 老李头，今晚 7 点老地方，搓够 48 圈不见不散！老李: 必须的，大保健走起！ // 拜占庭将军（这是要打麻将呢？还是要大保健？还是一边打麻将一边大保健……） 还能不能一起愉快地玩耍… 我们把以上所列的问题称为 系统模型 (system model)，讨论分布式系统理论和工程实践的时候，必先划定模型。例如有以下 两种模型： 异步环境 (asynchronous) 下，节点宕机(fail-stop) 异步环境 (asynchronous) 下，节点宕机恢复(fail-recover)、网络分化(network partition) 2 比 1 多了节点恢复、网络分化的考量，因而对这两种模型的理论研究和工程解决方案必定是不同的，在还没有明晰所要解决的问题前谈解决方案都是一本正经地耍流氓。 一致性 还具备两个属性，一个是 强一致(safety)，它要求 所有节点状态一致、共进退；一个是 可用 (liveness)，它要求 分布式系统 24\7 无间断对外服务*。FLP 定理 (FLP impossibility) 已经证明在一个收窄的模型中 (异步环境并只存在节点宕机)，不能同时满足 safety 和 liveness。 FLP 定理是分布式系统理论中的基础理论，正如物理学中的能量守恒定律彻底否定了永动机的存在，FLP 定理 否定 了同时满足 safety 和 liveness 的一致性协议的存在。 工程实践上根据具体的业务场景，或保证强一致 (safety)，或在节点宕机、网络分化的时候保证可用 (liveness)。2PC、3PC 是相对简单的解决 一致性 问题的协议，下面我们就来了解 2PC 和 3PC。 2PC2PC(two phase commit)两阶段提交，顾名思义它分成 两个阶段，先由一方进行提议 (propose) 并收集其他节点的反馈 (vote)，再根据反馈决定提交(commit) 或中止 (abort) 事务。我们将提议的节点称为 协调者(coordinator)，其他参与决议节点称为 参与者(participants，或 cohorts)： 2PC, phase one在阶段 1 中，coordinator 发起一个提议，分别问询各 participant 是否接受。 2PC, phase two在阶段 2 中，coordinator 根据 participant 的反馈，提交或中止事务，如果 participant 全部同意则提交，只要有一个 participant 不同意就中止。 在异步环境 (asynchronous) 并且没有节点宕机 (fail-stop) 的模型下，2PC 可以满足全认同、值合法、可结束，是解决一致性问题的一种协议。但如果再加上节点宕机 (fail-recover) 的考虑，2PC 是否还能解决一致性问题呢？ coordinator 如果在发起提议后宕机，那么 participant 将进入阻塞 (block) 状态、一直等待 coordinator 回应以完成该次决议。这时需要另一角色把系统从不可结束的状态中带出来，我们把新增的这一角色叫协调者备份 (coordinator watchdog)。coordinator 宕机一定时间后，watchdog 接替原 coordinator 工作，通过问询(query) 各 participant 的状态，决定阶段 2 是提交还是中止。这也要求 coordinator/participant 记录(logging) 历史状态，以备 coordinator 宕机后 watchdog 对 participant 查询、coordinator 宕机恢复后重新找回状态。 从 coordinator 接收到一次事务请求、发起提议到事务完成，经过 2PC 协议后增加了 2 次 RTT(propose+commit)，带来的时延 (latency) 增加相对较少。 3PC3PC(three phase commit) 即三阶段提交，既然 2PC 可以在异步网络 + 节点宕机恢复的模型下实现一致性，那还需要 3PC 做什么，3PC 是什么鬼？ 在 2PC 中一个 participant 的状态只有它自己和 coordinator 知晓，假如 coordinator 提议后自身宕机，在 watchdog 启用前一个 participant 又宕机，其他 participant 就会进入既不能回滚、又不能强制 commit 的阻塞状态，直到 participant 宕机恢复。这引出两个疑问： 能不能去掉阻塞，使系统可以在 commit/abort 前回滚 (rollback) 到决议发起前的初始状态 当次决议中，participant 间能不能相互知道对方的状态，又或者 participant 间根本不依赖对方的状态 相比 2PC，3PC 增加了一个准备提交 (prepare to commit) 阶段来解决以上问题： coordinator 接收完 participant 的反馈 (vote) 之后，进入阶段 2，给各个 participant 发送准备提交 (prepare to commit) 指令。participant 接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator 接收完确认 (ACK) 后进入阶段 3、进行 commit/abort，3PC 的阶段 3 与 2PC 的阶段 2 无异。协调者备份 (coordinator watchdog)、状态记录(logging) 同样应用在 3PC。 participant 如果在不同阶段宕机，我们来看看 3PC 如何应对： 阶段 1: coordinator 或 watchdog 未收到宕机 participant 的 vote，直接中止事务；宕机的 participant 恢复后，读取 logging 发现未发出赞成 vote，自行中止该次事务 阶段 2: coordinator 未收到宕机 participant 的 precommit ACK，但因为之前已经收到了宕机 participant 的赞成反馈 (不然也不会进入到阶段 2)，coordinator 进行 commit；watchdog 可以通过问询其他 participant 获得这些信息，过程同理；宕机的 participant 恢复后发现收到 precommit 或已经发出赞成 vote，则自行 commit 该次事务 阶段 3: 即便 coordinator 或 watchdog 未收到宕机 participant 的 commit ACK，也结束该次事务；宕机的 participant 恢复后发现收到 commit 或者 precommit，也将自行 commit 该次事务 因为有了准备提交 (prepare to commit) 阶段，3PC 的事务处理延时也增加了 1 个 RTT，变为 3 个 RTT(propose+precommit+commit)，但是它防止 participant 宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。 小结以上介绍了分布式系统理论中的部分基础知识，阐述了一致性 (consensus) 的定义和实现一致性所要面临的问题，最后讨论在异步网络 (asynchronous)、节点宕机恢复(fail-recover) 模型下 2PC、3PC 怎么解决一致性问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论基础 -- 选举、多数派和租约]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-election%2F</url>
      <content type="text"><![CDATA[选举 (election) 是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的 leader(或叫 master、coordinator)有助于实现事务原子性、提升决议效率。 多数派 (quorum) 的思路帮助我们在网络分化的情况下达成决议一致性，在 leader 选举的场景下帮助我们选出唯一 leader。租约 (lease) 在一定期限内给予节点特定权利，也可以用于实现 leader 选举。 下面我们就来学习分布式系统理论中的选举、多数派和租约。 选举 (electioin)一致性问题 (consistency) 是独立的节点间如何达成决议的问题，选出大家都认可的 leader 本质上也是一致性问题，因而如何应对宕机恢复、网络分化等在 leader 选举中也需要考量。 Bully 算法是最常见的选举算法，其要求每个节点对应一个序号，序号最高的节点为 leader。leader 宕机后次高序号的节点被重选为 leader，过程如下： (a). 节点 4 发现 leader 不可达，向序号比自己高的节点发起重新选举，重新选举消息中带上自己的序号 (b)(c). 节点 5、6 接收到重选信息后进行序号比较，发现自身的序号更大，向节点 4 返回 OK 消息并各自向更高序号节点发起重新选举 (d). 节点 5 收到节点 6 的 OK 消息，而节点 6 经过超时时间后收不到更高序号节点的 OK 消息，则认为自己是 leader (e). 节点 6 把自己成为 leader 的信息广播到所有节点 回顾 分布式系统理论基础 - 一致性、2PC 和 3PC 就可以看到，Bully 算法中有 2PC 的身影，都具有提议 (propose) 和收集反馈 (vote) 的过程。 在一致性算法 Paxos、ZAB、Raft 中，为提升决议效率均有节点充当 leader 的角色。ZAB、Raft 中描述了具体的 leader 选举实现，与 Bully 算法类似 ZAB 中使用 zxid 标识节点，具有最大 zxid 的节点表示其所具备的事务 (transaction) 最新、被选为 leader。 多数派 (quorum)在网络分化的场景下以上 Bully 算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个 leader，这时候就需要引入多数派 (quorum)。多数派的思路在分布式系统中很常见，其确保网络分化情况下决议唯一。 多数派的原理说起来很简单，假如节点总数为 2f+1，则一项决议得到多于 f 节点赞成则获得通过。leader 选举中，网络分化场景下只有具备多数派节点的部分才可能选出 leader，这避免了多 leader 的产生。 多数派的思路还被应用于副本 (replica) 管理，根据业务实际读写比例调整写副本数 Vw、读副本数 Vr，用以在可靠性和性能方面取得平衡。 租约 (lease)选举中很重要的一个问题，以上尚未提到：怎么判断 leader 不可用、什么时候应该发起重新选举？最先可能想到会通过心跳 (heart beat) 判别 leader 状态是否正常，但在网络拥塞或瞬断的情况下，这容易导致出现双主。 租约 (lease) 是解决该问题的常用方法，其最初提出时用于解决分布式缓存一致性问题，后面在分布式锁 等很多方面都有应用。 租约的原理同样不复杂，中心思想是每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约。假设我们有租约颁发节点 Z，节点 0、1 和 2 竞选 leader，租约过程如下： (a). 节点 0、1、2 在 Z 上注册自己，Z 根据一定的规则 (例如先到先得) 颁发租约给节点，该租约同时对应一个有效时长；这里假设节点 0 获得租约、成为 leader (b). leader 宕机时，只有租约到期 (timeout) 后才重新发起选举，这里节点 1 获得租约、成为 leader 租约机制确保了一个时刻最多只有一个 leader，避免只使用心跳机制产生双主的问题。在实践应用中，zookeeper、ectd 可用于租约颁发。 小结在分布式系统理论和实践中，常见 leader、quorum 和 lease 的身影。分布式系统内不一定事事协商、事事民主，leader 的存在有助于提升决议效率。 本文以 leader 选举作为例子引入和讲述 quorum、lease，当然 quorum 和 lease 是两种思想，并不限于 leader 选举应用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论进阶 - Paxos 变种和优化]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-paxos-advance%2F</url>
      <content type="text"><![CDATA[引言分布式系统理论进阶 - Paxos 中我们了解了 Basic Paxos、Multi Paxos 的基本原理，但如果想把 Paxos 应用于工程实践，了解基本原理还不够。 有很多基于 Paxos 的优化，在保证一致性协议正确 (safety) 的前提下，减少 Paxos 决议通信步骤、避免单点故障、实现节点负载均衡，从而降低时延、增加吞吐量、提升可用性，下面我们就来了解这些 Paxos 变种。 Multi Paxos首先我们来回顾一下 Multi Paxos，Multi Paxos 在 Basic Paxos 的基础上确定一系列值，其决议过程如下： phase1a: leader 提交提议给 acceptor phase1b: acceptor 返回最近一次接受的提议 (即曾接受的最大的提议 ID 和对应的 value)，未接受过提议则返回空 phase2a: leader 收集 acceptor 的应答，分两种情况处理 phase2a.1: 如果应答内容都为空，则自由选择一个提议 value phase2a.2: 如果应答内容不为空，则选择应答里面 ID 最大的提议的 value phase2b: acceptor 将决议同步给 learner Multi Paxos 中 leader 用于避免活锁，但 leader 的存在会带来其他问题，一是如何选举和保持唯一 leader(虽然无 leader 或多 leader 不影响一致性，但影响决议进程 progress)，二是充当 leader 的节点会承担更多压力，如何均衡节点的负载。Mencius提出节点轮流担任 leader，以达到均衡负载的目的；租约 (lease) 可以帮助实现唯一 leader，但 leader 故障情况下可导致服务短期不可用。 Fast Paxos在 Multi Paxos 中，proposer -&gt; leader -&gt; acceptor -&gt; learner，从提议到完成决议共经过 3 次通信，能不能减少通信步骤？ 对 Multi Paxos phase2a，如果可以自由提议 value，则可以让 proposer 直接发起提议、leader 退出通信过程，变为 proposer -&gt; acceptor -&gt; learner，这就是 Fast Paxos 的由来。 Multi Paxos 里提议都由 leader 提出，因而不存在一次决议出现多个 value，Fast Paxos 里由 proposer 直接提议，一次决议里可能有多个 proposer 提议、出现多个 value，即出现提议冲突 (collision)。leader 起到初始化决议进程(progress) 和解决冲突的作用，当冲突发生时 leader 重新参与决议过程、回退到 3 次通信步骤。 Paxos 自身隐含的一个特性也可以达到减少通信步骤的目标，如果 acceptor 上一次确定 (chosen) 的提议来自 proposerA，则当次决议 proposerA 可以直接提议减少一次通信步骤。如果想实现这样的效果，需要在 proposer、acceptor 记录上一次决议确定 (chosen) 的历史，用以在提议前知道哪个 proposer 的提议上一次被确定、当次决议能不能节省一次通信步骤。 EPaxos除了从减少通信步骤的角度提高 Paxos 决议效率外，还有其他方面可以降低 Paxos 决议时延，比如 Generalized Paxos 提出不冲突的提议 (例如对不同 key 的写请求) 可以同时决议、以降低 Paxos 时延。 更进一步地，EPaxos(Egalitarian Paxos) 提出一种既支持不冲突提议同时提交降低时延、还均衡各节点负载、同时将通信步骤减少到最少的 Paxos 优化方法。 为达到这些目标，EPaxos 的实现有几个要点。一是 EPaxos 中没有全局的 leader，而是每一次提议发起提议的 proposer 作为当次提议的 leader(command leader)；二是不相互影响 (interfere) 的提议可以同时提交；三是跳过 prepare，直接进入 accept 阶段。EPaxos 决议的过程如下： 左侧展示了互不影响的两个 update 请求的决议过程，右侧展示了相互影响的两个 update 请求的决议。Multi Paxos、Mencius、EPaxos 时延和吞吐量对比： 为判断决议是否相互影响，实现 EPaxos 得记录决议之间的依赖关系。 小结以上介绍了几个基于 Paxos 的变种，Mencius 中节点轮流做 leader、均衡节点负载，Fast Paxos 减少一次通信步骤，Generalized Paxos 允许互不影响的决议同时进行，EPaxos 无全局 leader、各节点平等分担负载。 优化无止境，对Paxos也一样，应用在不同场景和不同范围的Paxos变种和优化将继续不断出现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论进阶 - Raft、Zab]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-raft-and-zab%2F</url>
      <content type="text"><![CDATA[引言分布式系统理论进阶 - Paxos 介绍了一致性协议 Paxos，今天我们来学习另外两个常见的一致性协议——Raft 和 Zab。通过与 Paxos 对比，了解 Raft 和 Zab 的核心思想、加深对一致性协议的认识。 RaftPaxos 偏向于理论、对如何应用到工程实践提及较少。理解的难度加上现实的骨感，在生产环境中基于 Paxos 实现一个正确的分布式系统非常难： There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system. In order to build a real-world system, an expert needs to use numerous ideas scattered in the literature and make several relatively small protocol extensions. The cumulative effort will be substantial and the final system will be based on an unproven protocol. Raft 在 2013 年提出，提出的时间虽然不长，但已经有很多系统基于 Raft 实现。相比 Paxos，Raft 的买点就是更利于理解、更易于实行。 为达到更容易理解和实行的目的，Raft 将问题分解和具体化：Leader 统一处理变更操作请求，一致性协议的作用具化为保证节点间操作日志副本 (log replication) 一致，以 term 作为逻辑时钟 (logical clock) 保证时序，节点运行相同状态机 (state machine) 得到一致结果。Raft 协议具体过程如下： Client 发起请求，每一条请求包含操作指令 请求交由 Leader 处理，Leader 将操作指令 (entry) 追加 (append) 至操作日志，紧接着对 Follower 发起 AppendEntries 请求、尝试让操作日志副本在 Follower 落地 如果 Follower 多数派 (quorum) 同意 AppendEntries 请求，Leader 进行 commit 操作、把指令交由状态机处理 状态机处理完成后将结果返回给 Client 指令通过 log index(指令 id) 和 term number 保证时序，正常情况下 Leader、Follower 状态机按相同顺序执行指令，得出相同结果、状态一致。 宕机、网络分化等情况可引起 Leader 重新选举 (每次选举产生新 Leader 的同时，产生新的 term)、Leader/Follower 间状态不一致。Raft 中 Leader 为自己和所有 Follower 各维护一个 nextIndex 值，其表示 Leader 紧接下来要处理的指令 id 以及将要发给 Follower 的指令 id，LnextIndex 不等于 FnextIndex 时代表 Leader 操作日志和 Follower 操作日志存在不一致，这时将从 Follower 操作日志中最初不一致的地方开始，由 Leader 操作日志覆盖 Follower，直到 LnextIndex、FnextIndex 相等。 Paxos 中 Leader 的存在是为了提升决议效率，Leader 的有无和数目并不影响决议一致性，Raft 要求具备唯一 Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较 Paxos 而言更容易理解、更容易实现的目标。 ZabZab 的全称是 Zookeeper atomic broadcast protocol，是 Zookeeper 内部用到的一致性协议。相比 Paxos，Zab 最大的特点是保证强一致性 (strong consistency，或叫线性一致性 linearizable consistency)。 和 Raft 一样，Zab 要求唯一 Leader 参与决议，Zab 可以分解成 discovery、sync、broadcast 三个阶段： discovery: 选举产生 PL(prospective leader)，PL 收集 Follower epoch(cepoch)，根据 Follower 的反馈 PL 产生 newepoch(每次选举产生新 Leader 的同时产生新 epoch，类似 Raft 的 term) sync: PL 补齐相比 Follower 多数派缺失的状态、之后各 Follower 再补齐相比 PL 缺失的状态，PL 和 Follower 完成状态同步后 PL 变为正式 Leader(established leader) broadcast: Leader 处理 Client 的写操作，并将状态变更广播至 Follower，Follower 多数派通过之后 Leader 发起将状态变更落地 (deliver/commit) Leader 和 Follower 之间通过心跳判别健康状态，正常情况下 Zab 处在 broadcast 阶段，出现 Leader 宕机、网络隔离等异常情况时 Zab 重新回到 discovery 阶段。 了解完 Zab 的基本原理，我们再来看 Zab 怎样保证强一致性，Zab 通过约束事务先后顺序达到强一致性，先广播的事务先 commit、FIFO，Zab 称之为 primary order(以下简称 PO)。实现 PO 的核心是 zxid。 Zab 中每个事务对应一个 zxid，它由两部分组成：，e 即 Leader 选举时生成的 epoch，c 表示当次 epoch 内事务的编号、依次递增。假设有两个事务的 zxid 分别是 z、z’，当满足 z.e &lt; z’.e 或者 z.e = z’.e &amp;&amp; z.c &lt; z’.c 时，定义 z 先于 z’发生 (z &lt; z’)。 为实现 PO，Zab 对 Follower、Leader 有以下约束： 有事务 z 和 z’，如果 Leader 先广播 z，则 Follower 需保证先 commit z 对应的事务 有事务 z 和 z’，z 由 Leader p 广播，z’由 Leader q 广播，Leader p 先于 Leader q，则 Follower 需保证先 commit z 对应的事务 有事务 z 和 z’，z 由 Leader p 广播，z’由 Leader q 广播，Leader p 先于 Leader q，如果 Follower 已经 commit z，则 q 需保证已 commit z 才能广播 z’ 第 1、2 点保证事务 FIFO，第 3 点保证 Leader 上具备所有已 commit 的事务。 相比 Paxos，Zab 约束了事务顺序、适用于有强一致性需求的场景。 Paxos、Raft、Zab 再比较除 Paxos、Raft 和 Zab 外，Viewstamped Replication(简称 VR)也是讨论比较多的一致性协议。这些协议包含很多共同的内容 (Leader、quorum、state machine 等)，因而我们不禁要问：Paxos、Raft、Zab 和 VR 等分布式一致性协议区别到底在哪，还是根本就是一回事？ Paxos、Raft、Zab 和 VR 都是解决一致性问题的协议，Paxos 协议原文倾向于理论，Raft、Zab、VR 倾向于实践，一致性保证程度等的不同也导致这些协议间存在差异。下图帮助我们理解这些协议的相似点和区别： 相比 Raft、Zab、VR，Paxos 更纯粹、更接近一致性问题本源，尽管 Paxos 倾向理论，但不代表 Paxos 不能应用于工程。基于 Paxos 的工程实践，须考虑具体需求场景 (如一致性要达到什么程度)，再在 Paxos 原始语意上进行包装。 小结以上介绍分布式一致性协议Raft、Zab的核心思想，分析Raft、Zab与Paxos的异同。实现分布式系统时，先从具体需求和场景考虑，Raft、Zab、VR、Paxos等协议没有绝对地好与不好，只是适不适合。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论基础 - 时间、时钟和事件顺序]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-time%2F</url>
      <content type="text"><![CDATA[十六号…… 四月十六号。一九六零年四月十六号下午三点之前的一分钟你和我在一起，因为你我会记住这一分钟。从现在开始我们就是一分钟的朋友，这是事实，你改变不了，因为已经过去了。我明天会再来。 – 《阿飞正传》 现实生活中时间是很重要的概念，时间可以记录事情发生的时刻、比较事情发生的先后顺序。分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序，但不同于日常生活使用物理时钟记录时间，分布式系统使用逻辑时钟记录事件顺序关系，下面我们来看分布式系统中几种常见的逻辑时钟。 物理时钟 vs 逻辑时钟可能有人会问，为什么分布式系统不使用物理时钟 (physical clock) 记录事件？每个事件对应打上一个时间戳，当需要比较顺序的时候比较相应时间戳就好了。 这是因为现实生活中物理时间有统一的标准，而分布式系统中每个节点记录的时间并不一样，即使设置了 NTP 时间同步节点间也存在毫秒级别的偏差。因而分布式系统需要有另外的方法记录事件顺序关系，这就是逻辑时钟 (logical clock)。 Lamport timestampsLeslie Lamport 在 1978 年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为 Lamport 时间戳 (Lamport timestamps)。 分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport 时间戳原理如下： 每个事件对应一个 Lamport 时间戳，初始值为 0 如果事件在节点内发生，时间戳加 1 如果事件属于发送事件，时间戳加 1 并在消息中带上该时间戳 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1 假设有事件 a、b，C(a)、C(b) 分别表示事件 a、b 对应的 Lamport 时间戳，如果 C(a) &lt; C(b)，则有 a 发生在 b 之前 (happened before)，记作 a -&gt; b，例如图 1 中有 C1 -&gt; B1。通过该定义，事件集中 Lamport 时间戳不等的事件可进行比较，我们获得事件的偏序关系 (partial order)。 如果 C(a) = C(b)，那 a、b 事件的顺序又是怎样的？假设 a、b 分别在节点 P、Q 上发生，Pi、Qj 分别表示我们给 P、Q 的编号，如果 C(a) = C(b) 并且 Pi &lt; Qj，同样定义为 a 发生在 b 之前，记作 a =&gt; b。假如我们对图 1 的 A、B、C 分别编号 Ai = 1、Bj = 2、Ck = 3，因 C(B4) = C(C3) 并且 Bj &lt; Ck，则 B4 =&gt; C3。 通过以上定义，我们可以对所有事件排序、获得事件的全序关系 (total order)。上图例子，我们可以从 C1 到 A4 进行排序。 Vector clockLamport 时间戳帮助我们得到事件顺序关系，但还有一种顺序关系不能用 Lamport 时间戳很好地表示出来，那就是同时发生关系 (concurrent)。例如图 1 中事件 B4 和事件 C3 没有因果关系，属于同时发生事件，但 Lamport 时间戳定义两者有先后顺序。 Vector clock 是在 Lamport 时间戳基础上演进的另一种逻辑时钟方法，它通过 vector 结构不但记录本节点的 Lamport 时间戳，同时也记录了其他节点的 Lamport 时间戳。Vector clock 的原理与 Lamport 时间戳类似，使用图例如下： 假设有事件 a、b 分别在节点 P、Q 上发生，Vector clock 分别为 Ta、Tb，如果 Tb[Q] &gt; Ta[Q] 并且 Tb[P] &gt;= Ta[P]，则 a 发生于 b 之前，记作 a -&gt; b。到目前为止还和 Lamport 时间戳差别不大，那 Vector clock 怎么判别同时发生关系呢？ 如果 Tb[Q] &gt; Ta[Q] 并且 Tb[P] &lt; Ta[P]，则认为 a、b 同时发生，记作 a &lt;-&gt; b。例如图 2 中节点 B 上的第 4 个事件 (A:2，B:4，C:1) 与节点 C 上的第 2 个事件 (B:3，C:2) 没有因果关系、属于同时发生事件。 Version vector基于 Vector clock 我们可以获得任意两个事件的顺序关系，结果或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突 (detect conflict)。 分布式系统中数据一般存在多个副本 (replication)，多个副本可能被同时更新，这会引起副本间数据不一致 [7]，Version vector 的实现与 Vector clock 非常类似 [8]，目的用于发现数据冲突 [9]。下面通过一个例子说明 Version vector 的用法 [10]： client 端写入数据，该请求被 Sx 处理并创建相应的 vector ([Sx, 1])，记为数据 D1 第 2 次请求也被 Sx 处理，数据修改为 D2，vector 修改为 ([Sx, 2]) 第 3、第 4 次请求分别被 Sy、Sz 处理，client 端先读取到 D2，然后 D3、D4 被写入 Sy、Sz 第 5 次更新时 client 端读取到 D2、D3 和 D4 3 个数据版本，通过类似 Vector clock 判断同时发生关系的方法可判断 D3、D4 存在数据冲突，最终通过一定方法解决数据冲突并写入 D5 Vector clock 只用于发现数据冲突，不能解决数据冲突。如何解决数据冲突因场景而异，具体方法有以最后更新为准 (last write win)，或将冲突的数据交给 client 由 client 端决定如何处理，或通过 quorum 决议事先避免数据冲突的情况发生。 由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock 和 Version vector 在实际应用中可能面临的一个问题是 vector 过大，用于数据管理的元数据 (meta data) 甚至大于数据本身。 解决该问题的方法是使用 server id 取代 client id 创建 vector (因为 server 的数量相对 client 稳定)，或设定最大的 size、如果超过该 size 值则淘汰最旧的 vector 信息。 小结以上介绍了分布式系统里逻辑时钟的表示方法，通过Lamport timestamps可以建立事件的全序关系，通过Vector clock可以比较任意两个事件的顺序关系并且能表示无因果关系的事件，将Vector clock的方法用于发现数据版本冲突，于是有了Version vector。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统理论基础 -- CAP]]></title>
      <url>%2F2017%2F01%2F17%2Fdc-cap%2F</url>
      <content type="text"><![CDATA[引言CAP 是分布式系统、特别是分布式存储领域中被讨论最多的理论，“什么是 CAP 定理？” 在 Quora 分布式系统分类下排名 FAQ 的 No.1。CAP 在程序员中也有较广的普及，它不仅仅是 “C、A、P 不能同时满足，最多只能 3 选 2”，以下尝试综合各方观点，从发展历史、工程实践等角度讲述 CAP 理论。希望大家透过本文对 CAP 理论有更多地了解和认识。 CAP 定理CAP 由 Eric Brewer 在 2000 年 PODC 会议上提出，是 Eric Brewer 在 Inktomi 期间研发搜索引擎、分布式 web 缓存时得出的关于数据一致性(consistency)、服务可用性(availability)、分区容错性(partition-tolerance) 的猜想： It is impossible for a web service to provide the three following guarantees : Consistency, Availability and Partition-tolerance. 该猜想在提出两年后被证明成立，成为我们熟知的 CAP 定理： 数据一致性 (consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性 (strong consistency) (又叫原子性 atomic、线性一致性 linearizable consistency) 服务可用性 (availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待 分区容错性 (partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务 在某时刻如果满足 AP，分隔的节点同时对外服务但不能相互通信，将导致状态不一致，即不能满足 C；如果满足 CP，网络分区的情况下为达成 C，请求只能一直等待，即不满足 A；如果要满足 CA，在一定时间内要达到节点状态一致，要求不能出现网络分区，则不能满足 P。 C、A、P 三者最多只能满足其中两个，和 FLP 定理一样，CAP 定理也指示了一个不可达的结果 (impossibility result)。 CAP 的工程启示CAP 理论提出 7、8 年后，NoSql 圈将 CAP 理论当作对抗传统关系型数据库的依据、阐明自己放宽对数据一致性 (consistency) 要求的正确性，随后引起了大范围关于 CAP 理论的讨论。 CAP 理论看似给我们出了一道 3 选 2 的选择题，但在工程实践中存在很多现实限制条件，需要我们做更多地考量与权衡，避免进入 CAP 认识误区。 关于 P 的理解Partition 字面意思是网络分区，即因网络因素将系统分隔为多个单独的部分，有人可能会说，网络分区的情况发生概率非常小啊，是不是不用考虑 P，保证 CA 就好。要理解 P，我们看回 CAP 证明中 P 的定义： In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another. 网络分区 的情况符合该定义，网络丢包 的情况也符合以上定义，另外 节点宕机，其他节点发往宕机节点的包也将丢失，这种情况同样符合定义。现实情况下我们面对的是一个不可靠的网络、有一定概率宕机的设备，这两个因素都会导致 Partition，因而分布式系统实现中 P 是一个必须项，而不是可选项。 对于分布式系统工程实践，CAP 理论更合适的描述是：在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性 In a network subject to communication failures, it is impossible for any web service to implement an atomic read/write shared memory that guarantees a response to every request. CA 非 0/1 的选择P 是必选项，那 3 选 2 的选择题不就变成数据一致性 (consistency)、服务可用性 (availability) 2 选 1？工程实践中一致性有不同程度，可用性也有不同等级，在保证分区容错性的前提下，放宽约束后可以兼顾一致性和可用性，两者不是非此即彼。 CAP 定理证明中的一致性指 强一致性，强一致性要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。如果放宽这些要求，还有其他一致性类型： 序列一致性(sequential consistency)：不要求时序一致，A 操作先于 B 操作，在 B 操作后如果所有调用端读操作得到 A 操作的结果，满足序列一致性 最终一致性(eventual consistency)：放宽对时间的要求，在被调完成操作响应后的某个时间点，被调多个节点的数据最终达成一致 可用性在 CAP 定理里指所有读写操作必须要能终止，实际应用中从主调、被调两个不同的视角，可用性具有不同的含义。当 P(网络分区) 出现时，主调可以只支持读操作，通过牺牲部分可用性达成数据一致。 工程实践中，较常见的做法是通过异步拷贝副本 (asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致、被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用(或被网络分隔) 的效果。 跳出 CAPCAP 理论对实现分布式系统具有指导意义，但 CAP 理论并没有涵盖分布式工程实践中的所有重要因素。 例如延时 (latency)，它是衡量系统可用性、与用户体验直接相关的一项重要指标。CAP 理论中的可用性要求操作能终止、不无休止地进行，除此之外，我们还关心到底需要多长时间能结束操作，这就是延时，它值得我们设计、实现分布式系统时单列出来考虑。 延时与数据一致性也是一对 “冤家”，如果要达到强一致性、多个副本数据一致，必然增加延时。加上延时的考量，我们得到一个 CAP 理论的修改版本 PACELC：如果出现 P(网络分区)，如何在 A(服务可用性)、C(数据一致性) 之间选择；否则，如何在 L(延时)、C(数据一致性)之间选择。 小结以上介绍了 CAP 理论的源起和发展，介绍了 CAP 理论给分布式系统工程实践带来的启示。 CAP 理论对分布式系统实现有非常重大的影响，我们可以根据自身的业务特点，在数据一致性和服务可用性之间作出倾向性地选择。通过放松约束条件，我们可以实现在不同时间点满足 CAP(此 CAP 非 CAP 定理中的 CAP，如 C 替换为最终一致性)。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 安全]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-security%2F</url>
      <content type="text"><![CDATA[Spark 当前支持使用共享密钥来认证。可以通过配置 spark.authenticate 选项来开启认证。这个参数用来控制 Spark 的通讯协议是否使用共享密钥来执行认证。这个认证是一个基本的握手，用来保证两侧具有相同的共享密钥然后允许通讯。如果共享密钥无法识别，他们将不能通讯。创建共享密钥的方法如下 : 对于 Spark on YARN 的部署方式，配置 spark.authenticate 为 true，会自动生成和分发共享密钥。每个 application 会使用一个单独的共享密钥 对于 Spark 的其他部署方式，需要在每一个 nodes 配置 spark.authenticate.secret 参数。这个密码将会被所有的 Master/Workers 和 aplication 使用 WEB UISpark UI 可以通过配置 javax servlet filters 的 spark.ui.filters，或配置 https/SSL 的 spark.ui.https.enabled 进行安全保护。 认证当 UI 中有一些不允许其他用户看到的数据时，用户可能想对 UI 进行安全防护。用户指定的 javax servlet filter 可以对用户进行身份认证，用户一旦登入，Spark 可以比较与这个用户相对应的视图 ACLs 来确认是否授权用户访问 UI。配置项 spark.acls,enable,spark.ui.view.acls 和 spark.ui.view.acls.groups 控制 ACLs 的行为。注意，启动 application 的用户具有使用 UI 视图的访问权限。在 YARN 上，Spark UI 使用标准的 YARN web application 代理机制可以通过任何已安装的 Hadoop filters 进行认证。 Spark 也支持修改 ACLs 来控制谁具有修改一个正在运行的 Spark application 的权限。这些权限包括 kill 这个 application 或者一个 task。以上功能通过选项 spark.acls.enable，spark.modify.acls 和 spark.modify.acls.groups 来控制。注意如果你已经登入 UI，为了使用 UI 上的 kill 按钮，必须先添加这个用户到 modify acls 和 view acls。在 YARN 上，modify acls 通过 YARN 接口传入并控制谁具有 modify 权限。Spark 允许在 acls 中指定多个对所有的 application 都具有 view 和 modify 权限的管理员，通过选项 spark.admin.acls 和 spark.admin.acls.groups 来控制。这样做对于一个有帮助用户调试 applications 的管理员或者支持人员的共享集群，非常有帮助。 事件日志如果你的 applications 使用事件日志，事件日志的的位置必须手动创建同时具有适当的访问权限（spark.eventLog.dir）。如果想对这些日志文件做安全保护，目录的权限应该设置为 drwxrwxrwxt。目录的属主应该是启动 history server 的高级用户，同时组权限应该仅限于这个高级用户组。这将允许所有的用户向目录中写入文件，同时防止删除不是属于自己的文件（权限字符 t 为防删位）。事件日志将会被 Spark 创建，只有特定的用户才就有读写权限。 加密Spark 支持 HTTP SSL 协议。SASL 加密用于块传输服务和 RPC 端。 Spark 本地临时存储的数据（如 shuffle 文件，缓存的数据，其他和 application 相关文件）当前不支持加密。如果有这种需求，一个变通的方式是配置 cluster manager 将 application 数据存储到加密磁盘上。 SSL 配置SSL 配置是分级组织的。用户可以对所有的通讯协议配置默认的 SSL 设置，除非被特定协议的设置覆盖掉。这样用户可以很容易的为所有的协议提供通用的设置，无需禁用每个单独配置的能力。通用的 SSL 设置在 Spark 配置文件的 spark.ssl 命名空间中。下表描述了用于覆盖特定组件默认配置的命名空间 config Namespace（配置命名空间） Component（组件） spark.ssl.fs HTTP file server and broadcast server spark.ssl.ui Spark application Web UI spark.ssl.standalone Standalone Master / Worker Web UI spark.ssl.historyServer History Server Web UI 所有的 SSL 选项可以在 配置页面 里查询到。SSL 必须在所有的节点进行配置，对每个使用特定协议通讯的组件生效。 YARN 模式key-store 可以在客户端侧准备好，然后作为 application 的一部分通过 executor 进行分发和使用。因为用户能够通过使用 spark.yarn.dist.files 或者 spark.yarn.dis.archives 配置选项，在 YARN 上 启动 application 之前部署文件。加密和传输这些文件的责任在 YARN 一侧，与 Spark 无关。 为了一个长期运行的应用比如 Spark Streaming 应用能够写 HDFS，需要通过分别设置 –principal 和 –keytab 参数传递 principal 和 keytab 给 spark-submit。传入的 keytab 会通过 Hadoop Distributed Cache 拷贝到运行 Application 的 Master（这是安全的 - 如果 YARN 使用 SSL 配置同时启用了 HDFS 加密）。Kerberos 会使用这个 principal 和 keytab 周期的更新登陆信息，同时 HDFS 需要的授权 tokens 会周期生成，因此应用可以持续的写入 HDFS。 STANDALONE 模式用户需要为 master 和 slaves 提供 key-stores 和相关配置选项。必须通过附加恰当的 Java 系统属性到 SPARK_MASTER_OPTS 和 SPARK_WORKER_OPTS 环境变量中，或者只在 SPARK_DAEMON_JAVA_OPTS 中。在这种模式下，用户可以允许 executors 使用从派生它的 worker 继承来的 SSL 设置。可以通过设置 spark.ssl.useNodeLocalConf 为 true 来完成。如果设置这个参数，用户在客户端侧提供的设置不会被 executors 使用。 准备 key-store可以使用 keytool 程序生成 key-stores。这个工具的使用说明在这里。为 standalone 部署方式配置 key-stores 和 trust-store 最基本的步骤如下 : 为每个 node 生成 keys pair 在每个 node 上导出 key pair 到一个文件 导入以上生成的所有的公钥到一个单独的 trust-store 在所有的节点上分发 trust-store 配置 SASL 加密当认证（sprk.authenticate）开启的时候，块传输支持 SASL 加密。通过在 application 配置中设置 spark.authenticate.enableSaslEncryption 为 true 来开启 SASL 加密。 当使用外部 shuffle 服务，需要在 shuffle 服务配置中通过设置 spark.network.sasl.serverAlwaysEncrypt 为 true 来禁用未加密的连接。如果这个选项开启，未设置使用 SASL 加密的 application 在链接到 shuffer 服务的时候会失败。 针对网络安全配置端口Spark 严重依赖 network，同时一些环境对使用防火墙有严格的要求。下面使一些 Spark 网络通讯使用的主要的端口和配置方式。 仅对 Standalone 模式 From To Default Port（默认端口） Purpose（目的） Configuration Setting（配置设置） Notes（注意） Browser Standalone Master 8080 Web UI spark.master.ui.port /SPARK_MASTER_WEBUI_PORT Jetty-based. Standalone mode only. Browser Standalone Worker 8081 Web UI spark.worker.ui.port / SPARK_WORKER_WEBUI_PORT Jetty-based. Standalone mode only. Driver / Standalone Worker Standalone Master 7077 Submit job to cluster / Join cluster spark.worker.ui.port / SPARK_WORKER_WEBUI_PORT Set to “0” to choose a port randomly. Standalone mode only. Standalone Master Standalone Worker (random) Schedule executors SPARK_WORKER_PORT Set to “0” to choose a port randomly. Standalone mode only. 所有的集群管理器（cluster managers) From To Default Port（默认端口） Purpose（目的） Configuration Setting（配置设置） Notes（注意） Browser Application 4040 Web UI spark.ui.port Jetty-based Browser History Server 18080 Web UI spark.history.ui.port Jetty-based Executor / Standalone Master Driver (random) Connect to application / Notify executor state changes spark.driver.port Set to “0” to choose a port randomly. Executor / Driver Executor / Driver (random) Block Manager port spark.blockManager.port Raw socket via ServerSocketChannel 更多关于安全配置参数方面的细节请参考配置页面。安全方面的实施细节参考 org.apache.spark.SecurityManager]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark on YARN]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-running-on-yarn%2F</url>
      <content type="text"><![CDATA[运行 Spark on YARN支持在 YARN (Hadoop NextGen) 上运行是在 Spark 0.6.0 版本中加入到 Spark 中的，并且在后续的版本中得到改进的。 启动 Spark on YARN确保 HADOOP_CONF_DIR 或者 YARN_CONF_DIR 指向包含 Hadoop 集群的（客户端）配置文件的目录。这些配置被用于写入 HDFS 并连接到 YARN ResourceManager 。此目录中包含的配置将被分发到 YARN 集群，以便应用程序 (application) 使用的所有的所有容器 ( containers ) 都使用相同的配置。如果配置引用了 Java 系统属性或者未由 YARN 管理的环境变量，则还应在 Spark 应用程序的配置（驱动程序 (driver)，执行程序 (executors)，和在客户端模式下运行时的 AM ）。 有两种部署模式可以用于在 YARN 上启动 Spark 应用程序。在集群模式下， Spark 驱动程序 (Spark driver) 运行在集群上由 YARN 管理的应用程序主进程 (master process) 内，并且客户端可以在初始化应用程序后离开。在客户端模式下，驱动程序在客户端进程中运行，并且应用程序主服务器仅用于从 YARN 请求资源。 与 Spark 独立模式 和 Mesos 模式不同，在这两种模式中，master 的地址在 –master 参数中指定，在 YARN 模式下， ResourceManager 的地址从 Hadoop 配置中选取。因此， –master 参数是 yarn 。 在集群模式下启动 Spark 应用程序： 1234$ ./bin/spark-submit --class path.to.your.Class \ --master yarn \ --deploy-mode cluster \ [options] &lt;app jar&gt; [app options] 例如： 123456789$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 1 \ --queue thequeue \ lib/spark-examples*.jar \ 10 以上启动一个 YARN 客户端程序，启动默认的主应用程序（Application Master）。然后 SparkPi 将作为 Application Master 的子进程运行。客户端将定期轮询 Application Master 以获取状态的更新并在控制台中显示它们。一旦您的应用程序完成运行后，客户端将退出。请参阅下面的 “调试您的应用程序 (Debugging your Application)” 部分，了解如何查看驱动程序 ( driver ) 和 执行程序日志 ( executor logs )。 要在客户端模式下启动 Spark 应用程序，请执行相同的操作，但是将 client 替换 cluster 。下面展示了如何在客户端模式下运行 spark-shell ： 12$ ./bin/spark-shell --master yarn \ --deploy-mode client 添加其他的 JARs 在集群模式下，驱动程序（driver）在与客户端不同的机器上运行，因此 SparkContext.addJar 将不会立即使用客户端本地的文件运行。要使客户端上的文件可用于 SparkContext.addJar ，请在启动命令中使用 –jars 选项来包含这些文件。 123456$ ./bin/spark-submit --class my.main.Class \ --master yarn \ --deploy-mode cluster \ --jars my-other-jar.jar,my-other-other-jar.jar \ my-main-jar.jar \ app_arg1 app_arg2 准备在 YARN 上运行 Spark 需要使用 YARN 支持构建的二进制分布式的 Spark （a binary distribution of Spark）。二进制分布式（binary distributions）可以从项目网站的 下载页面 下载。要自己构建 Spark ，请参考 构建 Spark 。 要使 Spark 运行时 jars 可以从 YARN 端访问，您可以指定 spark.yarn.archive 或者 spark.yarn.jars 。有关详细的信息，请参阅 Spark 属性 。如果既没有指定 spark.yarn.archive 也没有指定 spark.yarn.jars ，Spark 将在 $SPARK_HOME/jars 目录下创建一个包含所有 jar 的 zip 文件，并将其上传到分布式缓存（distributed cache）中。 Spark on YARN 配置对于 Spark on YARN 和其他的部署模式，大多数的配置是相同的。有关这些的更多信息，请参阅 配置页面 。这些是特定于 YARN 上的 Spark 的配置。 调试您的应用（Debugging your Application）在 YARN 术语中，执行者（executors）和应用程序（application） masters 在 “容器（containers）” 中运行。在应用程序执行完成后，YARN 提供两种模式处理容器日志（container logs）。如果启用日志聚合（aggregation）（使用 yarn.log-aggregation-enable 配置），容器日志（container logs）将复制到 HDFS 并在本地计算机上删除。可以使用 yarn logs 命令从集群中的任何位置查看这些日志。 1yarn logs -applicationId &lt;app ID&gt; 上述命令将打印给定应用程序中所有容器（containers）的全部日志文件内容。你还可以使用 HDFS shell 或者 API 直接在 HDFS 中查看容器日志文件（container log files）。可以通过查看您的 YARN 配置（yarn.nodemanager.remote-app-log-dir 和 yarn.nodemanager.remote-app-log-dir-suffix）找到它们所在的目录。日志还可以在 Spark Web UI 的 “执行程序（Executors）” 选项卡下找到。您需要同时运行 Spark 历史记录服务器（Spark history server） 和 MapReduce 历史记录服务器（MapReduce history server），并在 yarn-site.xml 文件中正确配置 yarn.log.server.url 。 Spark 历史记录服务器 UI 上的日志将重定向您到 MapReduce 历史记录服务器以显示聚合日志（aggregated logs）。 当未启用日志聚合时，日志将在每台计算机上的本地保留在 YARN_APP_LOGS_DIR 目录下，通常配置为 /tmp/logs 或者 $HADOOP_HOME/logs/userlogs ，具体取决于 Hadoop 版本和安装。查看容器（container）的日志需要转到包含它们的主机并在此目录中查看它们。子目录根据应用程序 ID （application ID）和 容器 ID （container ID）组织日志文件。日志还可以在 Spark Web UI 的 “执行程序（Executors）” 选项卡下找到，并且不需要运行 MapReduce 历史记录服务器。 要查看每个容器的启动环境，请将 yarn.nodemanager.delete.debug-delay-sec 增加到一个较大的值（例如 36000），然后通过 yarn.nodemanager.local-dirs 访问应用程序缓存，在容器启动的节点上。此目录包含启动脚本（launch script）， JARs ，和用于启动每个容器的所有的环境变量。这个过程对于调试 classpath 问题特别有用。（请注意，启用此功能需要集群设置的管理员权限并且还需要重新启动所有的节点 (all node managers)，因此这不适用于托管集群）。 要为 application master 或者 executors 使用自定义的 log4j 配置，请选择以下选项： 使用 spark-submit 上传一个自定义的 log4j.properties ，通过将 spark-submit 添加到要与应用程序一起上传的文件的 –files 列表中。 添加 -Dlog4j.configuration=&lt;配置文件的位置（location of configuration file）&gt; 到 spark.driver.extraJavaOptions （对于驱动程序 (for the driver)）或者 spark.executor.extraJavaOptions （对于执行者 (for executors)）。请注意，如果使用文件，文件：协议（protocol ）应该被显式提供，并且该文件需要在所有节点的本地存在。 更新 $SPARK_CONF_DIR/log4j.properties 文件，并且它将与其他配置一起自动上传。请注意，如果指定了多个选项，其他 2 个选项的优先级高于此选项。 请注意，对于第一个选项，执行器（executors）和 主应用程序（application master）将共享相同的 log4j 配置，这当它们在同一个节点上运行的时候，可能会导致问题（例如，试图写入相同的日志文件）。 如果你需要引用正确的位置将日志文件放在 YARN 中，以便 YARN 可以正确显示和聚合它们，请在您的 log4j.properties 中使用 spark.yarn.app.container.log.dir 。例如，log4j.appender.file_appender.File=${spark.yarn.app.container.log.dir}/spark.log 。对于流应用程序（streaming applications），配置 RollingFileAppender 并将文件位置设置为 YARN 的日志目录将避免由于大型日志文件导致的磁盘溢出，并且可以使用 YARN 的日志实用程序（YARN’s log utility）访问日志。 要为主应用程序（application master）和 执行器（executors），请更新 $SPARK_CONF_DIR/metrics.properties 文件。它将自动与其他配置一起上传，因此您不需要使用 –files 手动指定它。 Spark 属性（Properties） 属性名称 默认 含义 spark.yarn.am.memory 512m 在客户端模式下用于 YARN Application Master 的内存量，与 JVM 内存字符串格式相同（例如，512m，2g）。在集群模式下，请改用 spark.driver.memory 。使用小写字母尾后缀，例如，k，m，g，t 和 p，分别表示 kibi- ，mebi- ，gibi- ，tebi- ，和 pebibytes 。 spark.driver.memory 1g 用于驱动程序进程（driver process）的内存量，即初始化 SparkContext 的位置。（例如 1g，2g）。注意：在客户端模式下，不能通过 SparkConf 直接在应用程序中设置此配置，因为驱动程序 JVM 已在此时启动。相反，请通过 –driver-memory 命令选项或者在默认属性文件中进行设置。 spark.driver.cores 1 驱动程序在 YARN 集群模式下使用的核数。由于驱动程序在集群模式下在与 YARN Application Master 相同的 JVM 中运行，因此还会控制 YARN Application Master 使用的核数。在客户端模式下，使用 spark.yarn.am.cores 来控制 YARN Application Master 使用的核数。 spark.yarn.am.cores 1 在客户端模式下用于 YARN Application Master 的核数。在集群模式下，请改用 spark.driver.cores 。 spark.yarn.am.waitTime 100s 在集群模式下，YARN Application Master 等待 SparkContext 初始化的时间。在客户端模式下， YARN Application Master 等待驱动程序（driver）连接到它的时间。 spark.yarn.submit.file.replication HDFS 默认的备份数（通常为 3） 用于应用程序上传到 HDFS 的文件的 HDFS 副本级别。这些包括诸如 Spark jar ，app jar， 和任何分布式缓存文件 / 归档之类的东西。 spark.yarn.stagingDir 文件系统中当前用户的主目录 提交应用程序时使用的临时目录。 spark.yarn.preserve.staging.files false 设置为 true 以便在作业结束保留暂存文件（Spark jar， app jar，分布式缓存文件），而不是删除它们。 spark.yarn.scheduler.heartbeat.interval-ms 3000 Spark application master 心跳到 YARN ResourceManager 中的间隔（以毫秒为单位）。该值的上限为到期时间间隔的 YARN 配置值的一半，即 yarn.am.liveness-monitor.expiry-interval-ms 。 spark.yarn.scheduler.initial-allocation.interval 200ms 当存在未决容器分配请求时， Spark application master 心跳到 YARN ResourceManager 的初始间隔。它不应大于 spark.yarn.scheduler.heartbeat.interval-ms 。如果挂起的容器仍然存在，直到达到 spark.yarn.scheduler.heartbeat.interval-ms ，则连续的心跳的分配间隔将加倍。 spark.yarn.max.executor.failures numExecutors * 2, 最小值为 3 在应用程序失败（failing the application）之前，执行器失败（executor failures）的最大数量。 spark.yarn.historyServer.address （无） Spark 历史记录服务器（history server）的地址，例如 host.com:18080 。地址不应包含 scheme （http://）。默认为未设置，因为历史记录服务器是可选服务。当 Spark 应用程序完成将应用程序从 ResourceManager UI 链接到 Spark 历史记录服务器 UI 时，此地址将提供给 YARN ResourceManager 。对于此属性， YARN 属性可用作变量，这些属性在运行时由 Spark 替换。例如，如果 Spark 历史记录服务器在与 YARN ResourceManager 相同的节点上运行，则可以将其设置为 ${hadoopconf-yarn.resourcemanager.hostname}:18080 。 spark.yarn.dist.archives （无） 逗号分隔的要提取到每个执行器（executor）的工作目录（working directory）中的归档列表。 spark.yarn.dist.files （无） 要放到每个执行器（executor）的工作目录（working directory）中的以逗号分隔的文件列表。 spark.yarn.dist.jars | （无） | 要放到每个执行器（executor）的工作目录（working directory）中的以逗号分隔的 jar 文件列表。 || spark.executor.cores | 在 YARN 模式下是 1 ，在独立模式下 worker 机器上的所有的可用的核。 | 每个执行器（executor）上使用的核数。仅适用于 YARN 和独立（standalone ）模式。 || spark.executor.instances | 2 | 静态分配的执行器（executor）数量。使用 spark.dynamicAllocation.enabled ，初始的执行器（executor）集至少会是这么大。spark.executor.memory 1g 每个执行器（executor）进程使用的内存量（例如 2g ，8g）。 || spark.yarn.executor.memoryOverhead | 执行器内存（executorMemory） 0.10 ，最小值为 384 | 要为每个执行器（executor）分配的堆外（off-heap）内存量（以兆字节为单位）。这是内存，例如 VM 开销，内部字符串，其他本机开销等。这往往随着执行器（executor）大小（通常为 6-10%）增长。 || spark.yarn.driver.memoryOverhead | 驱动程序内存（driverMemory） 0.10，最小值为 384 | 在集群模式下为每个驱动程序（driver）分配的堆外（off-heap）内存量（以兆字节为单位）。这是内存，例如 VM 开销，内部字符串，其他本机开销等。这往往随着容器（container）大小（通常为 6- 10%）增长。 || spark.yarn.am.memoryOverhead | AM 内存（AM Memory） * 0.10，最小 384 | 与 spark.yarn.driver.memoryOverhead 相同，但是适用于客户端模式下的 YARN Application Master 。 || spark.yarn.am.port | （随机） | 被 YARN Application Master 监听的端口。在 YARN 客户端模式下，这用于在网关（gateway）上运行的 Spark 驱动程序（driver）和在 YARN 上运行的 YARN Application Master 之间进行通信。在 YARN 集群模式下，这用于动态执行器（executor）功能，其中它处理从调度程序后端的 kill 。 || spark.yarn.queue | 默认（default） | 提交应用程序（application）的 YARN 队列名称。 || spark.yarn.jars | （无） | 包含要分发到 YARN 容器（container）的 Spark 代码的库（libraries）列表。默认情况下， Spark on YARN 将使用本地安装的 Spark jar ，但是 Spark jar 也可以在 HDFS 上的一个任何位置都可读的位置。这允许 YARN 将其缓存在节点上，使得它不需要在每次运行应用程序时分发。例如，要指向 HDFS 上的 jars ，请将此配置设置为 hdfs:///some/path 。允许使用 globs 。| spark.yarn.archive | （无）| 包含所需的 Spark jar 的归档（archive），以分发到 YARN 高速缓存。如果设置，此配置将替换 spark.yarn.jars ，并且归档（archive）在所有的应用程序（application）的容器（container）中使用。归档（archive）应该在其根目录中包含 jar 文件。与以前的选项一样，归档也可以托管在 HDFS 上以加快文件分发速度。 || spark.yarn.access.namenodes | （无） | 以逗号分隔的 Spark 应用程序要访问的安全的 HDFS namenodes 列表。例如：spark.yarn.access.namenodes=hdfs://nn1.com:8032,hdfs://nn2.com:8032, webhdfs://nn3.com:50070 。Spark 应用程序必须具有对列出的 namenode 的访问权限，并且 Kerberos 必须正确配置为能够访问它们（在同一领域（realm）或者在可信领域（trusted realm））。 Spark 为每个 namenode 获取安全令牌（security tokens），以便 Spark application 可以访问这些远程的 HDFS 集群。 || spark.yarn.appMasterEnv.[EnvironmentVariableName] | （无） | 将由 EnvironmentVariableName 指定的环境变量添加到在 YARN 上启动的 Application Master 进程。用户可以指定其中的多个并设置多个环境变量。在集群模式下，这控制 Spark 驱动程序（driver）的环境，在客户端模式下，它只控制执行器（executor）启动器（launcher）的环境。 || spark.yarn.containerLauncherMaxThreads | 25 | 在 YARN Application Master 中用于启动执行器容器（executor containers）的最大线程（thread）数。 || spark.yarn.am.extraJavaOptions | （无） | 在客户端模式下传递到 YARN Application Master 的一组额外的 JVM 选项。在集群模式下，请改用 spark.driver.extraJavaOptions 。请注意，使用此选项设置最大堆大小（-Xmx）设置是非法的。最大堆大小设置可以使用 spark.yarn.am.memory 。 || spark.yarn.am.extraLibraryPath | （无） | 设置在客户端模式下启动 YARN Application Master 时要使用的特殊库路径（special library path）。| spark.yarn.maxAppAttempts | yarn.resourcemanager.am.max-attempts 在 YARN 中 | 将要提交应用程序的最大的尝试次数。它应该不大于 YARN 配置中的全局最大尝试次数。 || spark.yarn.am.attemptFailuresValidityInterval | （无） | 定义 AM 故障跟踪（failure tracking）的有效性间隔。如果 AM 已经运行至少所定义的间隔，则 AM 故障计数将被重置。如果未配置此功能，则不启用此功能，并且仅在 Hadoop 2.6 + 版本中支持此功能。 || spark.yarn.executor.failuresValidityInterval | （无） | 定义执行器（executor）故障跟踪的有效性间隔。超过有效性间隔的执行器故障（executor failures）将被忽略。 || spark.yarn.submit.waitAppCompletion | true | 在 YARN 集群模式下，控制客户端是否等待退出，直到应用程序完成。如果设置为 true ，客户端将保持报告应用程序的状态。否则，客户端进程将在提交后退出。 || spark.yarn.am.nodeLabelExpression | （无） | 一个将调度限制节点 AM 集合的 YARN 节点标签表达式。只有大于或等于 2.6 版本的 YARN 版本支持节点标签表达式，因此在对较早版本运行时，此属性将被忽略。 || spark.yarn.executor.nodeLabelExpression | （无） | 一个将调度限制节点执行器（executor）集合的 YARN 节点标签表达式。只有大于或等于 2.6 版本的 YARN 版本支持节点标签表达式，因此在对较早版本运行时，此属性将被忽略。| spark.yarn.tags | （无） | 以逗号分隔的字符串列表，作为 YARN application 标记中显示的 YARN application 标记传递，可用于在查询 YARN apps 时进行过滤（filtering ）。 || spark.yarn.keytab | （无） | 包含上面指定的主体（principal）的 keytab 的文件的完整路径。此 keytab 将通过安全分布式缓存（Secure Distributed Cache）复制到运行 YARN Application Master 的节点，以定期更新 login tickets 和 delegation tokens 。（运行也使用 “local” master）。 |spark.yarn.principal | （无） | 在安全的 HDFS 上运行时用于登录 KDC 的主体（Principal ）。（运行也使用 “local” master）。 || spark.yarn.config.gatewayPath | （无） | 在网关主机（gateway host）（启动 Spark application 的 host）上有效的路径，但对于集群中其他节点中相同资源的路径可能不同。结合 spark.yarn.config.replacementPath ，者用于支持具有异构配置的集群，以便 Spark 可以正确启动远程进程。替换路径（replacement path）通常将包含对由 YARN 导出的某些环境变量（以及，因此对于 Spark 容器可见）的引用。例如，如果网关节点（gateway node）在 /disk1/hadoop 上安装了 Hadoop 库，并且 Hadoop 安装的位置由 YARN 作为 HADOOP_HOME 环境变量导出，则将此值设置为 /disk1/hadoop ，将替换路径（replacement path）设置为 $HADOOP_HOME 将确保用于启动远程进程的路径正确引用本地 YARN 配置。 || spark.yarn.config.replacementPath | （无）| 查看 spark.yarn.config.gatewayPath 。| spark.yarn.security.tokens.${service}.enabled | true | 控制是否在启用安全性时检索 非 HDFS 服务的 delegation tokens 。默认情况下，当配置了这些服务时，将检索所有支持的服务的 delegation tokens ，但是如果它以某种方式与正在运行的应用程序冲突，则可以禁用该行为。目前支持的服务有：hive，hbase 。 | Important notes （要点） 核心请求在调度决策中是否得到执行取决于使用的调度程序及其配置方式。 在集群模式下，Spark executors 和 Spark dirver 使用的本地目录是为 YARN（Hadoop YARN 配置 yarn.nodemanager.local-dirs）配置的本地目录。如果用户指定 spark.local.dir，它将被忽略。在客户端模式下，Spark executors 将使用为 YARN 配置的本地目录，而 Spark dirver 将使用 spark.local.dir 中定义的目录。这是因为 Spark dirver 不在客户端模式下在 YARN 集群上运行，只有 Spark executors。 –files 和–archives 选项支持用 # 指定文件名，类似于 Hadoop。 例如，你可以指定：–files localtest.txt#appSees.txt，这会将你在本地命名为 localtest.txt 的文件上传到 HDFS，但这将通过名称 appSees.txt 链接，当你的应用程序在 YARN 上运行时，你应该使用名称 appSees.txt 引用它。 –jars 选项允许你在集群模式下使用本地文件时运行 SparkContext.addJar 函数。 如果你使用 HDFS，HTTP，HTTPS 或 FTP 文件，则不需要使用它。 Running in a Secure Cluster（在一个安全的集群中运行）如 security 所讲的，Kerberos 被应用在安全的 Hadoop 集群中去验证与服务和客户端相关联的 principals。 这允许客户端请求这些已验证的服务; 向授权的 principals 授予请求服务的权利。 Hadoop 服务发出 hadoop tokens 去授权访问服务和数据。 客户端必须首先获取它们将要访问的服务的 tokens，当启动应用程序时，将它和应用程序一起发送到 YAYN 集群中。 如果 Spark 应用程序与 HDFS，HBase 和 Hive 进行交互，它必须使用启动应用程序的用户的 Kerberos 凭据获取相关 tokens，也就是说身份将成为已启动的 Spark 应用程序的 principal。 这通常在启动时完成：在安全集群中，Spark 将自动为集群的 HDFS 文件系统获取 tokens，也可能为 HBase 和 Hive 获取。 如果 HBase 在类路径中，HBase 配置声明应用程序是安全的（即 hbase-site.xml 将 hbase.security.authentication 设置为 kerberos），并且 spark.yarn.security.tokens.hbase.enabled 未设置为 false，HBase tokens 将被获得。 类似地，如果 Hive 在类路径上，其配置包括元数据存储的 URI（hive.metastore.uris），并且 spark.yarn.security.tokens.hive.enabled 未设置为 false，则将获得 Hive 令牌。 如果应用程序需要与其他安全 HDFS 集群交互，则在启动时必须显式请求访问这些集群所需的 tokens。 这是通过将它们列在 spark.yarn.access.namenodes 属性中来实现的。spark.yarn.access.namenodes hdfs://ireland.example.org:8020/,hdfs://frankfurt.example.org:8020/ Launching your application with Apache Oozie（用 Apache Oozie 运行程序）Apache Oozie 可以将启动 Spark 应用程序作为工作流的一部分。在安全集群中，启动的应用程序将需要相关的 tokens 来访问集群的服务。如果 Spark 使用 keytab 启动，这是自动的。但是，如果 Spark 在没有 keytab 的情况下启动，则设置安全性的责任必须移交给 Oozie。有关配置 Oozie 以获取安全集群和获取作业凭据的详细信息，请参阅 Oozie web site 上特定版本文档的 “Authentication” 部分。 对于 Spark 应用程序，必须设置 Oozie 工作流以使 Oozie 请求应用程序需要的所有 tokens，包括： YARN 资源管理器。 本地 HDFS 文件系统。 任何用作 I / O 的源或目标的远程 HDFS 文件系统。 Hive - 如果使用。 HBase - 如果使用。 YARN 时间轴服务器，如果应用程序与此交互。 为了避免 Spark 尝试 - 然后失败 - 获取 Hive，HBase 和远程 HDFS 令牌，必须将 Spark 配置收集这些服务 tokens 的选项设置为禁用。Spark 配置必须包含以下行： 1spark.yarn.security.tokens.hive.enabled false spark.yarn.security.tokens.hbase.enabled false 必须取消设置配置选项 spark.yarn.access.namenodes。 Troubleshooting Kerberos（Kerberos 错误排查）调试 Hadoop / Kerberos 问题可能是 “困难的”。 一个有用的技术是通过设置 HADOOP_JAAS_DEBUG 环境变量在 Hadoop 中启用对 Kerberos 操作的额外记录。 1bash export HADOOP_JAAS_DEBUG=true JDK 类可以配置为通过系统属性 sun.security.krb5.debug 和 sun.security.spnego.debug = true 启用对 Kerberos 和 SPNEGO / REST 认证的额外日志记录。 1-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true 所有这些选项都可以在 Application Master 中启用： 1spark.yarn.appMasterEnv.HADOOP_JAAS_DEBUG true spark.yarn.am.extraJavaOptions -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true 最后，如果 org.apache.spark.deploy.yarn.Client 的日志级别设置为 DEBUG，日志将包括获取的所有 tokens 的列表，以及它们的到期详细信息]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark on Mesos]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-running-on-mesos%2F</url>
      <content type="text"><![CDATA[Spark 可以在 Apache Mesos 管理的硬件集群上运行。 使用 Mesos 部署 Spark 的优点包括： Spark 和 其他框架 frameworks 之间的动态分区 Spark 的多个实例之间的可扩展分区 怎么运行在一个独立集群部署中，下图的集群管理器是 Spark master 的一个实例。当使用 Mesos 管理时，Mesos master 会替代 Spark master 作为集群的管理器。 现在，Driver 程序创建一个 job 并开始分发调度任务时，Mesos 会决定什么机器处理什么任务。 因为 Mesos 调度这些短期任务时会考虑到其他的框架，许多框架将会在同一个集群上共存，而不是借助资源的静态分区。 开始，请按照以下步骤安装 Mesos 并通过 Mesos 部署 Spark 作业。 安装 MesosSpark 2.0.1 设计用于 Mesos 0.21.0 或更新版本，不需要任何特殊的 Mesos 补丁。 如果你早已经有一个 Mesos 集群在运行，你可以跳过这个 Mesos 的安装步骤。 否则，安装 Mesos for Spark 与安装 Mesos for 其他的框架没有什么不同。你可以通过源码或者预构建软件安装包来安装 Mesos。 通过源码：通过源码安装 Apache Mesos，按照以下步骤： 从 镜像 mirror 下载 Mesos 版本 按照 Mesos 开始页面 Getting Started 来编译和安装 Mesos 注意: 如果你希望运行 Mesos 又不希望安装在系统的默认位置（例如：如果你没有默认路径的管理权限），传递 –prefix 选项进行配置来告诉它安装在什么地方。例如： 传递 –prefix=/home/me/mesos。默认情况下，前缀是：/usr/local 。 第三方包Apache Mesos 项目只发布了源码的版本，而不是二进制包。但是其他的第三方项目发布的二进制版本，可能对设置 Mesos 有帮助。 其中之一是中间层。用使用中间层提供的二进制版本安装 Mesos。 从下载页面下载 Mesos 的安装包 按照说明进行安装和配置 中间层的安装文档建议设置 zookeeper 来处理 Mesos Master 的故障转移，但是 Mesos 通过使用 Single Master 模式，可以在没有 zookeeper 的情况下运行。 验证要验证 Mesos 集群是否可用于 Spark，导航到 Mesos Master 的 Web UI 界面，端口是：5050 来确认所有预期的机器都会与从属选项卡中。 连接 Spark 到 Mesos通过使用 Spark 中的 Mesos, 你需要一个 Spark 的二进制包放到 Mesos 可以访问的地方，然后配置 Spark driver 程序来连接 Mesos。 或者你也可以在所有 Mesos slaves 位置安装 Spark，然后配置 spark.mesos.executor.home (默认是 SPARK_HOME) 来指向这个位置。 上传 Spark 包当 Mesos 第一次在 Mesos 从服务器上运行任务时，该从服务器必须有一个 Spark 二进制包来运行 Spark Mesos 执行器后端。 Spark 包可以在任何 Hadoop 可访问的 URI 上托管，包括 HTTP 通过 http://，Amazon Simple Storage Service 通过 s3n:// 或 HDFS 通过 hdfs://。 要使用预编译包： 从 Spark download page 下载 Spark 二进制包 上传到 hdfs/http/s3 要在 HDFS 上主机，请使用 Hadoop fs put 命令：hadoop fs -put spark-2.0.1.tar.gz /path/to/spark-2.0.1.tar.gz 或者如果您使用的是自定义编译版本的 Spark，则需要使用包含在 Spark 源代码 tarball / checkout 中的 dev / make-distribution.sh 脚本创建一个包。 使用 here 的说明下载并构建 Spark 使用./dev/make-distribution.sh –tgz 创建二进制包。 将归档文件上传到 http / s3 / hdfs 使用 Mesos Master 的 URLMesos 的 Master URL 以 mesos://host:5050 形式表示 single-master Mesos 集群，或者 mesos://zk://host1:2181,host2:2181,host3:2181/mesos 为一个 multi-master Mesos 集群使用 ZooKeeper。 客户端模式在客户端模式下，Spark Mesos 框架直接在客户端计算机上启动，并等待驱动程序输出。 驱动程序需要在 spark-env.sh 中进行一些配置才能与 Mesos 正常交互： 在 spark-env.sh 中设置一些环境变量： export MESOS_NATIVE_JAVA_LIBRARY= 的路径。此路径通常为 &lt; prefix&gt;/lib/libmesos.so，其中前缀默认为 / usr/local。请参阅上面的 Mesos 安装说明。在 Mac OS X 上，库称为 libmesos.dylib，而不是 libmesos.so。 export SPARK_EXECUTOR_URI=。 还将 spark.executor.uri 设置为 。 现在，当针对集群启动 Spark 应用程序时，在创建 SparkContext 时传递一个 mesos:// URL 作为主服务器。例如： Scala 12345val conf = new SparkConf() .setMaster("mesos://HOST:5050") .setAppName("My app") .set("spark.executor.uri", "&lt;path to spark-2.0.1.tar.gz uploaded above&gt;")val sc = new SparkContext(conf) （您还可以在 conf/spark-defaults.conf 文件中使用 spark-submit 并配置 spark.executor.uri。）运行 shell 时，spark.executor.uri 参数从 SPARK_EXECUTOR_URI 继承，因此不需要作为系统属性冗余传递。 1./bin/spark-shell --master mesos://host:5050 集群模式Spark on Mesos 还支持集群模式，其中驱动程序在集群中启动，客户端可以从 Mesos Web UI 中查找驱动程序的结果。 要使用集群模式，必须通过 sbin / start-mesos-dispatcher.sh 脚本启动集群中的 MesosClusterDispatcher，并传递 Mesos 主 URL（例如：mesos://host:5050）。这将启动 MesosClusterDispatcher 作为在主机上运行的守护程序。 如果你喜欢用 Marathon 运行 MesosClusterDispatcher，你需要在前台运行 MesosClusterDispatcher（即：bin/spark-class org.apache.spark.deploy.mesos.MesosClusterDispatcher）。请注意，MesosClusterDispatcher 尚不支持 HA 的多个实例。 MesosClusterDispatcher 还支持将恢复状态写入 Zookeeper。这将允许 MesosClusterDispatcher 能够在重新启动时恢复所有提交和运行的容器。为了启用此恢复模式，您可以通过配置 spark.deploy.recoveryMode 和相关的 spark.deploy.zookeeper.* 配置在 spark-env 中设置 SPARK_DAEMON_JAVA_OPTS。有关这些配置的更多信息，请参阅配置 (doc)[configurations.html#deploy]。 从客户端，您可以通过运行 spark-submit 并指定到 MesosClusterDispatcher 的 URL（例如：mesos://dispatcher:7077）的主 URL 将作业提交到 Mesos 集群。您可以在 Spark 集群 Web UI 上查看驱动程序状态。 例如： 123456789./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master mesos://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ http://path/to/examples.jar \ 1000 请注意，传递给 spark-submit 的 jar 或 python 文件应该是 Mesos 从设备可达的 URI，因为 Spark 驱动程序不会自动上传本地 jar。 Mesos 启动模式Spark 可以在两种模式下运行 Mesos：”粗粒度”（默认）和 “细粒度”（已弃用）。 粗粒度在 “粗粒度” 模式下，每个 Spark 执行程序作为单个 Mesos 任务运行。 Spark 执行程序的大小根据以下配置变量确定： 执行器内存：spark.executor.memory 执行器核心：spark.executor.cores 执行器数：spark.cores.max / spark.executor.cores 有关详细信息和默认值，请参阅 Spark Configuration 。 应用程序启动时，热切地启动执行器，直到达到 spark.cores.max。如果不设置 spark.cores.max，Spark 应用程序将保留 Mesos 提供给它的所有资源，因此我们当然敦促您在任何类型的多租户集群中设置此变量，包括运行多个并发 Spark 应用程序。 调度程序将启动执行者循环提供 Mesos 给它，但没有传播保证，因为 Mesos 不提供这样的保证提供流。粗粒度模式的好处是启动开销低得多，但是在应用程序的整个持续时间内保留 Mesos 资源的代价。要配置作业以动态调整其资源要求，请查看 Dynamic Allocation 。 细粒度（已弃用） 注意: 细粒度模式自 Spark 2.0.0 起已弃用。考虑使用 Dynamic Allocation 有一些好处。有关完整说明，请参阅 SPARK-11857 在 “细粒度” 模式下，Spark 执行程序中的每个 Spark 任务作为单独的 Mesos 任务运行。这允许 Spark（和其他框架）的多个实例以非常精细的粒度共享核心，其中每个应用程序随着其上升和下降而获得更多或更少的核心，但是它在启动每个任务时带来额外的开销。此模式可能不适合低延迟要求，如交互式查询或服务 Web 请求。 请注意，虽然细粒度的 Spark 任务会在核心终止时放弃核心，但它们不会放弃内存，因为 JVM 不会将内存回馈给操作系统。执行器在空闲时也不会终止。 要以细粒度模式运行，请在 SparkConf 中将 spark.mesos.coarse 属性设置为 false： 1conf.set("spark.mesos.coarse", "false") 您还可以使用 spark.mesos.constraints 在 Mesos 资源提供上设置基于属性的约束。默认情况下，所有资源优惠都将被接受。 1conf.set("spark.mesos.constraints", "os:centos7;us-east-1:false") 例如，假设 spark.mesos.constraints 设置为 os:centos7;us-east-1:false，那么将检查资源提供以查看它们是否满足这两个约束，然后才会被接受以启动新的执行程序。 Mesos Docker 的支持Spark 可以通过在你的 SparkConf 中 设置 spark.mesos.executor.docker.image 的属性，从而来使用 Mesos Docker 容器。 所使用的 Docker 镜像必须有一个合适的版本的 Spark 已经是映像的一部分，或者您可以通过通常的方法让 Mesos 下载 Spark。 需要 Mesos 版本 0.20.1 或更高版本。 独立于 Hadoop 运行您可以在现有的 Hadoop 集群旁边运行 Spark 和 Mesos，只需将它们作为计算机上的单独服务启动即可。 要从 Spark 访问 Hadoop 数据，需要一个完整的 hdfs:// URL（通常为 hdfs://:9000/path，但您可以在 Hadoop Namenode Web UI 上找到正确的 URL）。 此外，还可以在 Mesos 上运行 Hadoop MapReduce，以便在两者之间实现更好的资源隔离和共享。 在这种情况下，Mesos 将作为统一的调度程序，将核心分配给 Hadoop 或 Spark，而不是通过每个节点上的 Linux 调度程序共享资源。 请参考 Hadoop on Mesos 。 在任一情况下，HDFS 与 Hadoop MapReduce 分开运行，而不通过 Mesos 调度。 通过 Mesos 动态分配资源Mesos 仅支持使用粗粒度模式的动态分配，这可以基于应用程序的统计信息调整执行器的数量。 有关一般信息，请参阅 Dynamic Resource Allocation 。 要使用的外部 Shuffle 服务是 Mesos Shuffle 服务。 它在 Shuffle 服务之上提供 shuffle 数据清理功能，因为 Mesos 尚不支持通知另一个框架的终止。 要启动它，在所有从节点上运 $SPARK_HOME/sbin/start-mesos-shuffle-service.sh，并将 spark.shuffle.service.enabled 设置为 true。 这也可以通过 Marathon，使用唯一的主机约束和以下命令实现：bin/spark-class org.apache.spark.deploy.mesos.MesosExternalShuffleService。 配置有关 Spark 配置的信息，请参阅 configuration page 。 以下配置特定于 Mesos 上的 Spark。 Spark 属性 属性名称 默认值 含义 spark.mesos.coarse true 如果设置为 true，则以 “粗粒度” 共享模式在 Mesos 集群上运行，其中 Spark 在每台计算机上获取一个长期存在的 Mesos 任务。如果设置为 false，则以 “细粒度” 共享模式在 Mesos 集群上运行，其中每个 Spark 任务创建一个 Mesos 任务。’Mesos Run Modes’ 中的详细信息。 spark.mesos.extra.cores 0 设置执行程序公布的额外核心数。这不会导致分配更多的内核。它代替意味着执行器将 “假装” 它有更多的核心，以便驱动程序将发送更多的任务。使用此来增加并行度。 此设置仅用于 Mesos 粗粒度模式。 spark.mesos.mesosExecutor.cores 1.0 （仅限细粒度模式）给每个 Mesos 执行器的内核数。这不包括用于运行 Spark 任务的核心。换句话说，即使没有运行 Spark 任务，每个 Mesos 执行器将占用这里配置的内核数。 该值可以是浮点数。 spark.mesos.executor.docker.image (none) 设置 Spark 执行器将运行的 docker 映像的名称。所选映像必须安装 Spark，以及兼容版本的 Mesos 库。Spark 在图像中的安装路径可以通过 spark.mesos.executor.home 来指定; 可以使用 spark.executorEnv.MESOS_NATIVE_JAVA_LIBRARY 指定 Mesos 库的安装路径。 spark.mesos.executor.docker.volumes (none) 设置要装入到 Docker 镜像中的卷列表，这是使用 spark.mesos.executor.docker.image 设置的。此属性的格式是以逗号分隔的映射列表，后面的形式传递到 docker run -v。 这是他们采取的形式： [host_path:]container_path[:ro\ :rw] spark.mesos.executor.docker.portmaps (none) 设置由 Docker 镜像公开的入站端口的列表，这是使用 spark.mesos.executor.docker.image 设置的。此属性的格式是以逗号分隔的映射列表，格式如下：host_port:container_port[:tcp\ :udp] spark.mesos.executor.home driver sideSPARK_HOME 在 Mesos 中的执行器上设置 Spark 安装目录。默认情况下，执行器将只使用驱动程序的 Spark 主目录，它们可能不可见。请注意，这只有当 Spark 二进制包没有通过 spark.executor.uri 指定时才是相关的。 spark.mesos.executor.memoryOverhead executor memory * 0.10, with minimum of 384 以每个执行程序分配的额外内存量（以 MB 为单位）。默认情况下，开销将大于 spark.executor.memory 的 384 或 10％。如果设置，最终开销将是此值。 spark.mesos.uris (none) 当驱动程序或执行程序由 Mesos 启动时，要下载到沙箱的 URI 的逗号分隔列表。这适用于粗粒度和细粒度模式。 spark.mesos.principal (none) 设置 Spark 框架将用来与 Mesos 进行身份验证的主体。 spark.mesos.secret (none) 设置 Spark 框架将用来与 Mesos 进行身份验证的机密。 spark.mesos.role * 设置这个 Spark 框架对 Mesos 的作用。角色在 Mesos 中用于预留和资源权重共享。 spark.mesos.constraints (none) 基于属性的约束对 mesos 资源提供。 默认情况下，所有资源优惠都将被接受。有关属性的更多信息，请参阅 Mesos Attributes &amp; Resources 。标量约束与 “小于等于” 语义匹配，即约束中的值必须小于或等于资源提议中的值。范围约束与 “包含” 语义匹配，即约束中的值必须在资源提议的值内。集合约束与语义的 “子集” 匹配，即约束中的值必须是资源提供的值的子集。文本约束与 “相等” 语义匹配，即约束中的值必须完全等于资源提议的值。如果没有作为约束的一部分存在的值，则将接受具有相应属性的任何报价（没有值检查）。 spark.mesos.driver.webui.url (none) 设置 Spark Mesos 驱动程序 Web UI URL 以与框架交互。如果取消设置，它将指向 Spark 的内部 Web UI。 spark.mesos.dispatcher.webui.url (none) 设置 Spark Mesos 分派器 Web UI URL 以与框架交互。如果取消设置，它将指向 Spark 的内部 Web UI。 故障排查和调试在调试中可以看的地方 Mesos Master 的端口：5050 Slaves 应该出现在 Slavas 那一栏 Spark 应用应该出现在框架那一栏 任务应该出现在在一个框架的详情 检查失败任务沙箱的输出和错误 Mesos 的日志 Master 和 Slave 的日志默认在： /var/log/mesos 目录 常见的陷阱： Spark 装配不可达、不可访问 Slave 必须可以从你给的 http://, hdfs:// or s3n:// URL 地址下载到 Spark 的二进制包 防火墙拦截通讯 检查信息是否是连接失败。 临时禁用防火墙来调试，然后戳出适当的漏洞]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 编程指南]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-programming-guides%2F</url>
      <content type="text"><![CDATA[在一个较高的水平上，每一个 Spark 应用程序由一个在集群上运行着用户的 main 函数和执行各种并行操作的 driver program（驱动程序）组成。Spark 提供的主要抽象是一个弹性分布式数据集（RDD），它是可以执行并行操作且跨集群节点的元素的集合。RDD 可以从一个 Hadoop 文件系统（或者任何其它 Hadoop 支持的文件系统），或者一个在 driver program（驱动程序）中已存在的 Scala 集合，以及通过 transforming（转换）来创建一个 RDD。用户为了让它在整个并行操作中更高效的重用，也许会让 Spark persist（持久化）一个 RDD 到内存中。最后，RDD 会自动的从节点故障中恢复。 在 Spark 中的第二个抽象是能够用于并行操作的 shared variables（共享变量），默认情况下，当 Spark 的一个函数作为一组不同节点上的任务运行时，它将每一个变量的副本应用到每一个任务的函数中去。有时候，一个变量需要在整个任务中，或者在任务和 driver program（驱动程序）之间来共享。Spark 支持两种类型的共享变量 : broadcast variables（广播变量），它可以用于在所有节点上缓存一个值，和 accumulators（累加器），他是一个只能被 “added（增加）” 的变量，例如 counters 和 sums。 本指南介绍了每一种 Spark 所支持的语言的特性。如果您启动 Spark 的交互式 shell - 针对 Scala shell 使用 bin/spark-shell 或者针对 Python 使用 bin/pyspark 是很容易来学习的。 原文链接 : http://spark.apache.org/docs/latest/programming-guide.html Spark 依赖Spark 2.0.2 默认使用 Scala 2.11 来构建和发布直到运行。（当然，Spark 也可以与其它的 Scala 版本一起运行）。为了使用 Scala 编写应用程序，您需要使用可兼容的 Scala 版本（例如，2.11.X）。 要编写一个 Spark 的应用程序，您需要在 Spark 上添加一个 Maven 依赖。Spark 可以通过 Maven 中央仓库获取 : Scala 123groupId = org.apache.sparkartifactId = spark-core_2.11version = 2.0.2 此外，如果您想访问一个 HDFS 集群，则需要针对您的 HDFS 版本添加一个 hadoop-client（hadoop 客户端）依赖。 123groupId = org.apache.hadoopartifactId = hadoop-clientversion = &lt;your-hdfs-version&gt; 最后，您需要导入一些 Spark classes（类）到您的程序中去。添加下面几行 : Scala 12import org.apache.spark.SparkContextimport org.apache.spark.SparkConf （ 在 Spark 1.3.0 之前，您需要明确导入 org.apache.spark.SparkContext._ 来启用必不可少的隐式转换。） Spark 初始化Spark 程序必须做的第一件事情是创建一个 SparkContext 对象，它会告诉 Spark 如何访问集群。为了创建一个 SparkContext，首先需要构建一个包含应用程序的信息的 SparkConf 对象。 每一个 JVM 可能只能激活一个 SparkContext 对象。在创新一个新的对象之前，必须调用 stop() 该方法停止活跃的 SparkContext。 12val conf = new SparkConf().setAppName(appName).setMaster(master)new SparkContext(conf) 这个 appName 参数是一个在集群 UI 上展示应用程序的名称。 master 是一个 Spark，Mesos 或 YARN 群集的 URL 地址，或者指定为 “local” 字符串以在 local mode（本地模式）中运行。在实际工作中，当在集群上运行时，您不希望在程序中将 master 给硬编码，而是用 使用 spark-submit 启动应用程序 并且接收它。然而，对于本地测试和单元测试，您可以通过 “local” 来运行 Spark 进程。 Shell 的使用在 Spark Shell 中，一个特殊的 interpreter-aware（可用的解析器）SparkContext 已经为您创建好了，称之为 sc 的变量。创建您自己的 SparkContext 将不起作用。您可以使用 –master 参数设置这个 SparkContext 连接到哪一个 master 上，并且您可以通过 –jars 参数传递一个逗号分隔的列表来添加 JARs 到 classpath 中。也可以通过 –packages 参数应用一个用逗号分隔的 maven coordinates（maven 坐标）方式来添加依赖（例如，Spark 包）到您的 shell session 中去。任何额外存在且依赖的仓库（例如 Sonatype）可以传递到 –repositories 参数。例如，要明确使用四个核（CPU）来运行 bin/spark-shell，使用 : 1$ ./bin/spark-shell --master local[4] 或者，也可以添加 code.jar 到它的 classpath 中去，使用 : 1$ ./bin/spark-shell --master local[4] --jars code.jar 为了使用 maven coordinates（坐标）来包含一个依赖 : 1$ ./bin/spark-shell --master local[4] --packages "org.example:example:0.1" 有关选项的完整列表，请运行 spark-shell –help。在后台，spark-shell 调用了较一般的 spark-submit 脚本。 弹性分布式数据集（RDDS）Spark 主要以一个 弹性分布式数据集（RDD） 的概念为中心，它是一个容错且可以执行并行操作的元素的集合。有两种方法可以创建 RDD : 在你的 driver program（驱动程序）中 parallelizing 一个已存在的集合，或者在外部存储系统中引用一个数据集，例如，一个共享文件系统，HDFS，HBase，或者提供 Hadoop InputFormat 的任何数据源。 并行集合可以在您的 driver program（驱动程序）中已存在的集合上通过调用 SparkContext 的 parallelize 方法来创建并行集合。该集合的元素从一个可以并行操作的 distributed dataset（分布式数据集）中复制到另一个 dataset（数据集）中去。例如，这里是一个如何去创建一个保存数字 1 ~ 5 的并行集合。 Scala 12val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data) 在创建后，该 distributed dataset（分布式数据集）（distData）可以并行的执行操作。例如，我们可以调用 distData.reduce((a, b) =&gt; a + b) 来合计数组中的元素。后面我们将介绍 distributed dataset（分布式数据集）上的操作。 并行集合中一个很重要参数是 partitions（分区）的数量，它可用来切割 dataset（数据集）。Spark 将在集群中的每一个分区上运行一个任务。通常您希望群集中的每一个 CPU 计算 2-4 个分区。一般情况下，Spark 会尝试根据您的群集情况来自动的设置的分区的数量。当然，您也可以将分区数作为第二个参数传递到 parallelize (e.g. sc.parallelize(data, 10)) 方法中来手动的设置它。 注意：在代码中有些地方使用了 term slices（词片）（分区的同义词）以保持向后的兼容性。 外部数据集Spark 可以从 Hadoop 所支持的任何存储源中创建 distributed dataset（分布式数据集），包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3 等等。 Spark 支持文本文件，SequenceFiles，以及任何其它的 Hadoop InputFormat。 可以使用 SparkContext 的 textFile 方法来创建文本文件的 RDD。此方法需要一个文件的 URI（计算机上的本地路径 ，hdfs://，s3n:// 等等的 URI），并且读取它们作为一个 lines（行）的集合。下面是一个调用示例 : Scala 12scala&gt; val distFile = sc.textFile("data.txt")distFile: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[10] at textFile at &lt;console&gt;:26 在创建后，distFile 可以使用 dataset（数据集）的操作。例如，我们可以使用下面的 map 和 reduce 操作来合计所有行的数量 : distFile.map(s =&gt; s.length).reduce((a, b) =&gt; a + b)。 使用 Spark 来读取文件的一些注意事项 : 如果使用本地文件系统的路径，所工作节点的相同访问路径下该文件必须可以访问。复制文件到所有工作节点上，或着使用共享的网络挂载文件系统。 所有 Spark 中基于文件的输入方法，包括 textFile（文本文件），支持目录，压缩文件，或者通配符来操作。例如，您可以用 textFile(&quot;/my/directory&quot;)，textFile(&quot;/my/directory/*.txt&quot;) 和 textFile(&quot;/my/directory/*.gz&quot;)。 textFile 方法也可以通过第二个可选的参数来控制该文件的分区数量。默认情况下，Spark 为文件的每一个 block（块）创建的一个分区（HDFS 中块大小默认是 64M），当然你也可以通过传递一个较大的值来要求一个较高的分区数量。请注意，分区的数量不能够小于块的数量。 除了文本文件之外，Spark 的 Scala API 也支持一些其它的数据格式 : SparkContext.wholeTextFiles 可以读取包含多个小文本文件的目录，并返回它们的每一个 (filename, content) 对。这与 textFile 形成对比，它的每一个文件中的每一行将返回一个记录。 针对 SequenceFiles，使用 SparkContext 的 sequenceFile[K, V] 方法，其中 K 和 V 指的是它们在文件中的类型。这些应该是 Hadoop 中 Writable 接口的子类，例如 IntWritable 和 Texts 。此外，Spark 可以让您为一些常见的 Writables 指定原生类型。例如，sequenceFile[Int, String] 会自动读取 IntWritables 和 Texts。 针对其它的 Hadoop InputFormats，您可以使用 SparkContext.hadoopRDD 方法，它接受一个任意 JobConf 和 input format（输入格式）类，key 类和 value 类。通过相同的方法你可以设置你 Hadoop Job 的输入源。你还可以使用基于 “new” 的 MapReduce API（org.apache.hadoop.mapreduce）来使用 SparkContext.newAPIHadoopRDD 以设置 InputFormats。 RDD.saveAsObjectFile 和 SparkContext.objectFile 支持使用简单的序列化的 Java Object 来保存 RDD。虽然这不像 Avro 这种专用的格式一样高效，但其提供了一种更简单的方式来保存任何的 RDD。 RDD 操作RDDS 支持两种类型的操作： transformations ：在一个已存在的 dataset 上创建一个新的 dataset。 actions ：将在 dataset 上运行的计算结果返回到驱动程序。 例如： map 是一个通过让每个数据集元素都执行一个函数，并返回的新 RDD 结果的 transformation；另一方面，reduce 通过执行一些函数，聚合 RDD 中所有元素，并将最终结果给返回驱动程序（虽然也有一个并行 reduceByKey 返回一个分布式数据集）的 action。 Spark 的所有 transformations 都是 lazy，因此它不会立刻计算出结果。相反，他们只记得应用于一些基本数据集（例如：文件）的转换。只有当需要返回结果给驱动程序时，transformations 才开始计算。这种设计使 Spark 的运行更高效。例如，我们可以意识到，map 所创建的数据集将被用在 reduce 中，并且只有 reduce 的计算结果返回给驱动程序，而不是映射一个更大的数据集。 默认情况下，每次你在 RDD 运行一个 action 的时， 每个 transformed RDD 都会被重新计算。但是，您也可用 persist (或 cache) 方法将 RDD persist（持久化）到内存中；在这种情况下，Spark 为了下次查询时可以更快地访问，会把数据保存在集群上。此外，还支持持续持久化 RDDs 到磁盘，或复制到多个结点。 基础Scala为了说明 RDD 基础，考虑下面的简单程序： 123val lines = sc.textFile("data.txt")val lineLengths = lines.map(s =&gt; s.length)val totalLength = lineLengths.reduce((a, b) =&gt; a + b) 第一行从外部文件中定义了一个基本的 RDD，但这个数据集并未加载到内存中或即将被行动：line 仅仅是一个类似指针的东西，指向该文件。 第二行定义了 lineLengths 作为 map transformation 的结果。请注意，由于 laziness (延迟加载) lineLengths 不会被立即计算。 最后，我们运行 reduce，这是一个 action。在这此时，Spark 分发计算任务到不同的机器上运行，每台机器都运行在 map 的一部分并本地运行 reduce，仅仅返回它聚合后的结果给驱动程序。 如果我们也希望以后再次使用 lineLengths，我们还可以添加： 1lineLengths.persist() 在 reduce 之前，这将导致 lineLengths 在第一次计算之后就被保存在 memory 中。 传递函数给 SparkScala当驱动程序在集群上运行时，Spark 的 API 在很大程度上依赖于传递函数。有 2 种推荐的方式来做到这一点： 匿名函数的语法 Anonymous function syntax，它可以用于短的代码片断。 在全局单例对象中的静态方法。例如，你可以定义对象 MyFunctions 然后传递 MyFunctions.func1，具体如下： 123object MyFunctions &#123; def func1(s: String): String = &#123; ... &#125;&#125; myRdd.map(MyFunctions.func1)请注意，虽然也有可能传递一个类的实例（与单例对象相反）的方法的引用，这需要发送整个对象，包括类中其它方法。例如，考虑： 1234 class MyClass &#123; def func1(s: String): String = &#123; ... &#125; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(func1) &#125;&#125; 这里，如果我们创建一个 MyClass 的实例，并调用 doStuff，在 map 内有 MyClass 实例的 func1 方法的引用，所以整个对象需要被发送到集群的。 它类似于 rdd.map(x =&gt; this.func1(x))。 类似的方式，访问外部对象的字段将引用整个对象： 1234class MyClass &#123; val field = "Hello" def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(x =&gt; field + x) &#125;&#125; 相当于写 rdd.map(x =&gt; this.field + x) ，它引用这个对象所有的东西。为了避免这个问题，最简单的方法是 field（域）到本地变量，而不是从外部访问它的： 1234def doStuff(rdd: RDD[String]): RDD[String] = &#123; val field_ = this.field rdd.map(x =&gt; field_ + x)&#125; 理解闭包在集群中执行代码时，一个关于 Spark 更难的事情是理解的变量和方法的范围和生命周期。 修改其范围之外的变量 RDD 操作可以混淆的常见原因。在下面的例子中，我们将看一下使用的 foreach() 代码递增累加计数器，但类似的问题，也可能会出现其他操作上。 例如：考虑一个简单的 RDD 元素求和，以下行为可能不同，具体取决于是否在同一个 JVM 中执行。一个常见的例子是当 Spark 运行在本地模式 (–master = local[n]) 时，与部署 Spark 应用到群集（例如，通过 spark-submit 到 YARN）： Scala 1234567var counter = 0var rdd = sc.parallelize(data)// Wrong: Don't do this!!rdd.foreach(x =&gt; counter += x)println("Counter value:" + counter) 本地 VS 集群模式上面的代码行为是不确定的，并且可能无法按预期正常工作。Spark 执行作业时，会分解 RDD 操作到每个执行者里。在执行之前，Spark 计算任务的 closure（闭包）。而闭包是在 RDD 上的执行者必须能够访问的变量和方法（在此情况下的 foreach() ）。闭包被序列化并被发送到每个执行者。 closure 把变量副本发给每个 executor ，当 counter 被 foreach 函数引用的时候，它已经不再是 driver node 的 counter 了。虽然在 driver node 仍然有一个 counter 在内存中，但是对 executors 已经不可见。executor 看到的只是序列化的闭包一个副本。所以 counter 最终的值还是 0，因为对 counter 所有的操作所有操作均引用序列化的 closure 内的值。 在本地模式，在某些情况下的 foreach 功能实际上是同一 JVM 上的驱动程序中执行，并会引用同一个原始的计数器，实际上可能更新。 为了确保这些类型的场景明确的行为应该使用的 Accumulator（累加器）。当一个执行的任务分配到集群中的各个 worker 结点时，Spark 的累加器是专门提供安全更新变量的机制。本指南的累加器的部分会更详细地讨论这些。 在一般情况下，closures - constructs 像循环或本地定义的方法，不应该被用于改动一些全局状态。Spark 没有规定或保证突变的行为，以从封闭件的外侧引用的对象。一些代码，这可能以本地模式运行，但是这只是偶然和这样的代码如预期在分布式模式下不会表现。改用如果需要一些全局聚集累加器。 打印 RDD 的所有元素另一种常见的语法用于打印 RDD 的所有元素使用 rdd.foreach(println) 或 rdd.map(println)。在一台机器上，这将产生预期的输出和打印 RDD 的所有元素。然而，在集群 cluster 模式下，stdout 输出正在被执行写操作 executors 的 stdout 代替，而不是在一个驱动程序上，因此 stdout 的 driver 程序不会显示这些！要打印 driver 程序的所有元素，可以使用的 collect() 方法首先把 RDD 放到 driver 程序节点上：rdd.collect().foreach(println)。这可能会导致 driver 程序耗尽内存，虽说，因为 collect() 获取整个 RDD 到一台机器; 如果你只需要打印 RDD 的几个要素，一个更安全的方法是使用 take(): rdd.take(100).foreach(println)。 使用 Key-Value 对工作Scala虽然大多数 Spark 操作工作在包含任何类型对象的 RDDs 上，只有少数特殊的操作可用于 Key-Value 对的 RDDs。最常见的是分布式 “shuffle” 操作，如通过元素的 key 来进行 grouping 或 aggregating 操作。 在 Scala 中，这些操作时自动可用于包含 Tuple2 对象的 RDDs（在语言中内置的元组，通过简单的写 (a, b) ）。在 PairRDDFunctions 类中该 Key-Value 对的操作有效的，其中围绕元组的 RDD 自动包装。 例如，下面的代码使用的 Key-Value 对的 reduceByKey 操作统计文本文件中每一行出现了多少次： 123val lines = sc.textFile("data.txt")val pairs = lines.map(s =&gt; (s, 1))val counts = pairs.reduceByKey((a, b) =&gt; a + b) 我们也可以使用 counts.sortByKey() ，例如，在对按字母顺序排序，最后 counts.collect() 把他们作为一个数据对象返回给的驱动程序。 使用自定义对象作为 Key-Value 对操作的 key 时，您必须确保自定义 equals() 方法有一个 hashCode() 方法相匹配。有关详情，请参见这是 Object.hashCode() documentation) 中列出的约定。 Transformations （转换）下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）。 Transformation（转换） Meaning（释义） map(func) 返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中的元素应用一个函数 func 来生成。 filter(func) 返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中应用一个函数 func 且返回值为 true 的元素来生成。 flatMap(func) 与 map 类似，但是每一个输入的 item 可以被映射成 0 个或多个输出的 items（所以 func 应该返回一个 Seq 而不是一个单独的 item） mapPartitions(func) 与 map 类似，但是单独的运行在在每个 RDD 的 partition（分区，block）上，所以在一个类型为 T 的 RDD 上运行时 func 必须是 Iterator =&gt; Iterator 类型。 mapPartitionsWithIndex(func) 与 mapPartitions 类似，但是也需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 func，所以在一个类型为 T 的 RDD 上运行时 func 必须是 (Int, Iterator) =&gt; Iterator 类型。 sample(withReplacement, fraction, seed) 样本数据，设置是否放回（withReplacement）、采样的百分比（fraction）、使用指定的随机数生成器的种子（seed）。 union(otherDataset) 返回一个新的 dataset，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的并集。 intersection(otherDataset) 返回一个新的 RDD，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的交集。 distinct([numTasks])) 返回一个新的 dataset，它包含了 source dataset（源数据集）中去重的元素。 groupByKey([numTasks]) 在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable) pairs 的 dataset。注意 : 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 reduceByKey 或 aggregateByKey 来计算性能会更好。注意 : 默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 numTasks 参数来设置不同的任务数。 reduceByKey(func, [numTasks]) 在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable) pairs 的 dataset，它的值会针对每一个 key 使用指定的 reduce 函数 func 来聚合，它必须为 (V,V) =&gt; V 类型。像 groupByKey 一样，可通过第二个可选参数来配置 reduce 任务的数量。 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable) pairs 的 dataset，它的值会针对每一个 key 使用指定的 combine 函数和一个中间的 “zero” 值来聚合，它必须为 (V,V) =&gt; V 类型。为了避免不必要的配置，可以使用一个不同与 input value 类型的 aggregated value 类型。 sortByKey([ascending], [numTasks]) 在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset。 join(otherDataset, [numTasks]) 在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 leftOuterJoin，rightOuterJoin 和 fullOuterJoin 来实现。 cogroup(otherDataset, [numTasks]) 在一个 (K, V) 和的 dataset 上调用时，返回一个 (K, (Iterable, Iterable)) tuples 的 dataset。这个操作也调用了 groupWith。 cartesian(otherDataset) 在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) pairs 类型的 dataset（所有元素的 pairs，即笛卡尔积）。 pipe(command, [envVars]) 通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回。 coalesce(numPartitions) Decrease（降低） RDD 中 partitions（分区）的数量为 numPartitions。对于执行过滤后一个大的 dataset 操作是更有效的。 repartition(numPartitions) Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀。该操作总是通过网络来 shuffles 所有的数据。 repartitionAndSortWithinPartitions(partitioner) 根据给定的 partitioner（分区器）对 RDD 进行重新分区，并在每个结果分区中，按照 key 值对记录排序。这比每一个分区中先调用 repartition 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行。 Actions （动作）下面列出了一些 Spark 常用的 actions 操作。详情请参考 RDD API 文档（Scala，Java，Python，R）和 pair RDD 函数文档（Scala，Java）。 Action 释义 reduce(func) 使用函数 func 聚合数据集（dataset）中的元素，这个函数 func 输入为两个元素，返回为一个元素。这个函数应该是可交换（commutative ）和关联（associative）的，这样才能保证它可以被并行地正确计算。 collect() 在驱动程序中，以一个数组的形式返回数据集的所有元素。这在返回足够小（sufficiently small）的数据子集的过滤器（filter）或其他操作（other operation）之后通常是有用的。 count() 返回数据集中元素的个数。 first() 返回数据集中的第一个元素（类似于 take(1)）。 take(n) 将数据集中的前 n 个元素作为一个数组返回。 takeSample(withReplacement, num, [seed]) 对一个数据集随机抽样，返回一个包含 num 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。 takeOrdered(n, [ordering]) 返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 n 个元素。 saveAsTextFile(path) 将数据集中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录。 saveAsSequenceFile(path) (Java and Scala) 将数据集中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int、Double、String 等等)。 saveAsObjectFile(path) (Java and Scala) 使用 Java 序列化（serialization）以简单的格式（simple format）编写数据集的元素，然后使用 SparkContext.objectFile() 进行加载。 countByKey() 仅适用于（K,V）类型的 RDD 。返回具有每个 key 的计数的 （K , Int）对 的 hashmap。 foreach(func) 对数据集中每个元素运行函数 func 。这通常用于副作用（side effects），例如更新一个累加器（Accumulator）或与外部存储系统（external storage systems）进行交互。注意：修改除 foreach() 之外的累加器以外的变量（variables）可能会导致未定义的行为（undefined behavior）。详细介绍请阅读 理解闭包（Understanding closures） 部分。 shuffle 操作Spark 里的某些操作会触发 shuffle。shuffle 是 spak 重新分配数据的一种机制，使得这些数据可以跨不同的区域进行分组。这通常涉及在 executors 和 机器之间拷贝数据，这使得 shuffle 成为一个复杂的、代价高的操作。 性能影响shuffle 是一个代价比较高的操作，它涉及磁盘 IO、数据序列化、网络 IO。为了准备 shuffle 操作的数据，Spark 启动了一系列的 map 任务和 reduce 任务，map 任务组织数据，reduce 完成数据的聚合。这里的 map、reduce 来自 MapReduce，跟 Spark 的 map 操作和 reduce 操作没有关系。 在内部，一个 map 任务的所有结果数据会保存在内存，直到内存不能全部存储为止。然后，这些数据将基于目标分区进行排序并写入一个单独的文件中。在 reduce 时，任务将读取相关的已排序的数据块。 某些 shuffle 操作会大量消耗堆内存空间，因为 shuffle 操作在数据转换前后，需要在使用内存中的数据结构对数据进行组织。需要特别说明的是，reduceByKey 和 aggregateByKey 在 map 时会创建这些数据结构，ByKey 操作在 reduce 时创建这些数据结构。当内存满的时候，Spark 会把溢出的数据存到磁盘上，这将导致额外的磁盘 IO 开销和垃圾回收开销的增加。 shuffle 操作还会在磁盘上生成大量的中间文件。在 Spark 1.3 中，这些文件将会保留至对应的 RDD 不在使用并被垃圾回收为止。这么做的好处是，如果在 Spark 重新计算 RDD 的血统关系（lineage）时，shuffle 操作产生的这些中间文件不需要重新创建。如果 Spark 应用长期保持对 RDD 的引用，或者垃圾回收不频繁，这将导致垃圾回收的周期比较长。这意味着，长期运行 Spark 任务可能会消耗大量的磁盘空间。临时数据存储路径可以通过 SparkContext 中设置参数 spark.local.dir 进行配置。 shuffle 操作的行为可以通过调节多个参数进行设置。详细的说明请看 Configuration Guide 中的 “Shuffle Behavior” 部分。 背景为了明白 shuffle 操作的过程，我们以 reduceByKey 为例。reduceBykey 操作产生一个新的 RDD，其中 key 相同的所有的值组合成为一个 tuple - key 以及 与 key 相关联的所有值在 reduce 函数上的执行结果。但问题是，一个 key 的所有值不一定都在一个同一个分区里，甚至是不一定在同一台机器里，但是它们必须共同被计算。 在 spark 里，特定的操作需要数据不跨分区分布。在计算期间，一个任务在一个分区上执行，为了所有数据都在单个 reduceByKey 的 reduce 任务上运行，我们需要执行一个 all-to-all 操作。它必须从所有分区读取所有的 key 和 key 对应的所有的值，并且跨分区聚集去计算每个 key 的结果 - 这个过程就叫做 shuffle。 尽管每个分区新 shuffle 的数据集将是确定的，分区本身的顺序也是这样，但是这些数据的顺序是不确定的。如果希望 shuffle 后的数据是有序的，可以使用： mapPartitions 对每个分区进行排序，例如 .sorted repartitionAndSortWithinPartitions 在分区的同时对分区进行高效的排序 sortBy 做一个整体的排序 触发 shuffle 的操作包括 repartition 操作，如 repartition、coalesce；’ByKey’ 操作（除了 counting 相关操作），如 groupByKey、reduceByKey 和 join 操作，如 cogroup 和 join 。 RDD 持久化Spark 中一个很重要的能力是将数据持久化（或称为缓存），在多个操作间都可以访问这些持久化的数据。当持久化一个 RDD 时，每个节点的其它分区都可以使用 RDD 在内存中进行计算，在该数据上的其他 action 操作将直接使用内存中的数据。这样会让以后的 action 操作计算速度加快（通常运行速度会加速 10 倍）。缓存是迭代算法和快速的交互式使用的重要工具。 RDD 可以使用 persist() 方法或 cache() 方法进行持久化。数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。另外，每个持久化的 RDD 可以使用不同的存储级别进行缓存，例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。这些存储级别通过传递一个 StorageLevel 对象（Scala、Java、Python）给 persist() 方法进行设置。cache() 方法是使用默认存储级别的快捷设置方法，默认的存储级别是 StorageLevel.MEMORY_ONLY (将反序列化的对象存储到内存中）。详细的存储级别介绍如下： MEMORY_ONLY：将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。 MEMORY_AND_DISK：将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。 MEMORY_ONLY_SER：将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer 时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。 MEMORY_AND_DISK_SER：类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。 DISK_ONLY：只在磁盘上缓存 RDD。 MEMORY_ONLY_2,MEMORY_AND_DISK_2, 等等：与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。 OFF_HEAP (实验中)：类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。 注意，在 Python 中，缓存的对象总是使用 Pickle 进行序列化，所以在 Python 中不关心你选择的是哪一种序列化级别。python 中的存储级别包括 MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2 。 在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据。这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法。 如何选择存储级别Spark 的存储级别的选择，核心问题是在内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择： 如果使用默认的存储级别（MEMORY_ONLY），存储在内存中的 RDD 没有发生溢出，那么就选择默认的存储级别。默认存储级别可以最大程度的提高 CPU 的效率, 可以使在 RDD 上的操作以最快的速度运行。 如果内存不能全部存储 RDD, 那么使用 MEMORY_ONLY_SER，并挑选一个快速序列化库将对象序列化，以节省内存空间。使用这种存储级别，计算速度仍然很快。 除了在计算该数据集的代价特别高，或者在需要过滤大量数据的情况下，尽量不要将溢出的数据存储到磁盘。因为，重新计算这个数据分区的耗时与从磁盘读取这些数据的耗时差不多。 如果想快速还原故障，建议使用多副本存储级别（例如，使用 Spark 作为 web 应用的后台服务，在服务出故障时需要快速恢复的场景下）。所有的存储级别都通过重新计算丢失的数据的方式，提供了完全容错机制。但是多副本级别在发生数据丢失时，不需要重新计算对应的数据库，可以让任务继续运行。 删除数据Spark 自动监控各个节点上的缓存使用率，并以最近最少使用的方式（LRU）将旧数据块移除内存。如果想手动移除一个 RDD，而不是等待该 RDD 被 Spark 自动移除，可以使用 RDD.unpersist() 方法。 共享变量通常情况下，一个传递给 Spark 操作（例如 map 或 reduce）的函数 func 是在远程的集群节点上执行的。该函数 func 在多个节点执行过程中使用的变量，是同一个变量的多个副本。这些变量的以副本的方式拷贝到每个机器上，并且各个远程机器上变量的更新并不会传播回 driver program（驱动程序）。通用且支持 read-write（读 - 写） 的共享变量在任务间是不能胜任的。所以，Spark 提供了两种特定类型的共享变量 : broadcast variables（广播变量）和 accumulators（累加器）。 Broadcast Variables （广播变量）Broadcast variables（广播变量）允许程序员将一个 read-only（只读的）变量缓存到每台机器上，而不是给任务传递一个副本。它们是如何来使用呢，例如，广播变量可以用一种高效的方式给每个节点传递一份比较大的 input dataset（输入数据集）副本。在使用广播变量时，Spark 也尝试使用高效广播算法分发 broadcast variables（广播变量）以降低通信成本。 Spark 的 action（动作）操作是通过一系列的 stage（阶段）进行执行的，这些 stage（阶段）是通过分布式的 “shuffle” 操作进行拆分的。Spark 会自动广播出每个 stage（阶段）内任务所需要的公共数据。这种情况下广播的数据使用序列化的形式进行缓存，并在每个任务运行前进行反序列化。这也就意味着，只有在跨越多个 stage（阶段）的多个任务会使用相同的数据，或者在使用反序列化形式的数据特别重要的情况下，使用广播变量会有比较好的效果。 广播变量通过在一个变量 v 上调用 SparkContext.broadcast(v) 方法来进行创建。广播变量是 v 的一个 wrapper（包装器），可以通过调用 value 方法来访问它的值。代码示例如下 : Scala 1234scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)scala&gt; broadcastVar.valueres0: Array[Int] = Array(1, 2, 3) 在创建广播变量之后，在集群上执行的所有的函数中，应该使用该广播变量代替原来的 v 值，所以节点上的 v 最多分发一次。另外，对象 v 在广播后不应该再被修改，以保证分发到所有的节点上的广播变量具有同样的值（例如，如果以后该变量会被运到一个新的节点）。 Accumulators （累加器）Accumulators（累加器）是一个仅可以执行 “added”（添加）的变量来通过一个关联和交换操作，因此可以高效地执行支持并行。累加器可以用于实现 counter（ 计数，类似在 MapReduce 中那样）或者 sums（求和）。原生 Spark 支持数值型的累加器，并且程序员可以添加新的支持类型。 创建 accumulators（累加器）并命名之后，在 Spark 的 UI 界面上将会显示它。这样可以帮助理解正在运行的阶段的运行情况（注意 : 该特性在 Python 中还不支持）。 可以通过调用 SparkContext.longAccumulator() 或 SparkContext.doubleAccumulator() 方法创建数值类型的 accumulator（累加器）以分别累加 Long 或 Double 类型的值。集群上正在运行的任务就可以使用 add 方法来累计数值。然而，它们不能够读取它的值。只有 driver program（驱动程序）才可以使用 value 方法读取累加器的值。 下面的代码展示了一个 accumulator（累加器）被用于对一个数字中的元素求和。 Scala 123456789scala&gt; val accum = sc.longAccumulator("My Accumulator")accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))...10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 sscala&gt; accum.valueres2: Long = 10 上面的代码示例使用的是 Spark 内置的 Long 类型的累加器，程序员可以通过继承 AccumulatorV2 类创建新的累加器类型。AccumulatorV2 抽象类有几个需要 override（重写）的方法 : reset 方法可将累加器重置为 0，add 方法可将其它值添加到累加器中，merge 方法可将其他同样类型的累加器合并为一个。其他需要重写的方法可参考 scala API 文档。 例如，假设我们有一个表示数学上 vectors（向量）的 MyVector 类，我们可以写成 : Scala 123456789101112131415object VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] &#123; val vec_ : MyVector = MyVector.createZeroVector def reset(): MyVector = &#123; vec_.reset() &#125; def add(v1: MyVector, v2: MyVector): MyVector = &#123; vec_.add(v2) &#125; ...&#125;// Then, create an Accumulator of this type:val myVectorAcc = new VectorAccumulatorV2// Then, register it into spark context:sc.register(myVectorAcc, "MyVectorAcc1") 注意，在开发者定义自己的 AccumulatorV2 类型时， resulting type（返回值类型）可能与添加的元素的类型不一致。 累加器的更新只发生在 action 操作中，Spark 保证每个任务只更新累加器一次，例如，重启任务不会更新值。在 transformations（转换）中， 用户需要注意的是，如果 task（任务）或 job stages（阶段）重新执行，每个任务的更新操作可能会执行多次。 累加器不会改变 Spark lazy evaluation（懒加载）的模式。如果累加器在 RDD 中的一个操作中进行更新，它们的值仅被更新一次，RDD 被作为 action 的一部分来计算。因此，在一个像 map() 这样的 transformation（转换）时，累加器的更新并没有执行。下面的代码片段证明了这个特性 : Scala 123val accum = sc.accumulator(0)data.map &#123;x =&gt; accum += x; x &#125;// 在这里，accus 仍然为 0, 因为没有 actions（动作）来让 map 操作被计算。 部署应用到集群中应用提交指南 描述了如何将应用提交到集群中。简单的说，在您将应用打包成一个 JAR（针对 Java/Scala）或者一组 .py 或 .zip 文件（Python）后，就可以通过 bin/spark-submit 脚本将应用提交到任何支持的集群管理器中。 使用 Java / Scala 运行 spark Jobsorg.apache.spark.launcher 包提供了使用简单的 Java API 作为子进程启动 Spark Jobs 的类。 单元测试Spark 可以友好的使用流行的单元测试框架进行单元测试。在将 master URL 设置为 local 来测试时会简单的创建一个 SparkContext，运行您的操作，然后调用 SparkContext.stop() 将该作业停止。因为 Spark 不支持在同一个程序中并行的运行两个 contexts，所以需要确保使用 finally 块或者测试框架的 tearDown 方法将 context 停止。 下一步您可以在 Spark 官网上看一些 Spark 编程示例。另外，在 Spark 的 examples 目录中包含了许多例子（Scala，Java，Python，R）。您可以通过传递 class name 到 Spark 的 bin/run-example 脚本以运行 Java 和 Scala 示例。例如 : 1./bin/run-example SparkPi 针对 Python 示例，使用 spark-submit 来代替 : 1./bin/spark-submit examples/src/main/python/pi.py 针对 R 示例，使用 spark-submit 来代替 : 1./bin/spark-submit examples/src/main/r/dataframe.R 针对应用程序的优化，Spark 配置 和 优化指南 提供了一些最佳实践的信息。这些优化建议在确保你的数据以高效的格式存储在内存中尤其重要。针对部署参考，请阅读 集群模式概述，该文档描述了分布式操作和支持的集群管理器的组件。最后，所有的API文档可在 Scala，Java，Python，R 中获取。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark RDD API详解(一) Map和Reduce]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-rdd%2F</url>
      <content type="text"><![CDATA[RDD是什么？RDD是Spark中的抽象数据结构类型，任何数据在Spark中都被表示为RDD。从编程的角度来看，RDD可以简单看成是一个数组。和普通数组的区别是，RDD中的数据是分区存储的，这样不同分区的数据就可以分布在不同的机器上，同时可以被并行处理。因此，Spark应用程序所做的无非是把需要处理的数据转换为RDD，然后对RDD进行一系列的变换和操作从而得到结果。本文为第一部分，将介绍Spark RDD中与Map和Reduce相关的API中。 如何创建RDD？RDD可以从普通数组创建出来，也可以从文件系统或者HDFS中的文件创建出来。 举例：从普通数组创建RDD，里面包含了1到9这9个数字，它们分别在3个分区中。 12scala&gt; val a = sc.parallelize(1 to 9, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:12 举例：读取文件README.md来创建RDD，文件中的每一行就是RDD中的一个元素 12scala&gt; val b = sc.textFile("README.md")b: org.apache.spark.rdd.RDD[String] = MappedRDD[3] at textFile at &lt;console&gt;:12 虽然还有别的方式可以创建RDD，但在本文中我们主要使用上述两种方式来创建RDD以说明RDD的API。 mapmap是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。 举例： 123456scala&gt; val a = sc.parallelize(1 to 9, 3)scala&gt; val b = a.map(x =&gt; x*2)scala&gt; a.collectres10: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; b.collectres11: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18) 上述例子中把原RDD中每个元素都乘以2来产生一个新的RDD。 mapPartitionsmapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。 它的函数定义为： 1def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] f即为输入函数，它处理每个分区里面的内容。每个分区中的内容将以Iterator[T]传递给输入函数f，f的输出结果是Iterator[U]。最终的RDD由所有分区经过输入函数处理后的结果合并起来的。 举例： 1234567891011scala&gt; val a = sc.parallelize(1 to 9, 3)scala&gt; def myfunc[T](iter: Iterator[T]) : Iterator[(T, T)] = &#123; var res = List[(T, T)]() var pre = iter.next while (iter.hasNext) &#123; val cur = iter.next; res .::= (pre, cur) pre = cur; &#125; res.iterator&#125;scala&gt; a.mapPartitions(myfunc).collectres0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8)) 上述例子中的函数myfunc是把分区中一个元素和它的下一个元素组成一个Tuple。因为分区中最后一个元素没有下一个元素了，所以(3,4)和(6,7)不在结果中。 mapPartitions还有些变种，比如mapPartitionsWithContext，它能把处理过程中的一些状态信息传递给用户指定的输入函数。还有mapPartitionsWithIndex，它能把分区的index传递给用户指定的输入函数。 mapValuesmapValues顾名思义就是输入函数应用于RDD中Kev-Value的Value，原RDD中的Key保持不变，与新的Value一起组成新的RDD中的元素。因此，该函数只适用于元素为KV对的RDD。 举例： 1234scala&gt; val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", " eagle"), 2)scala&gt; val b = a.map(x =&gt; (x.length, x))scala&gt; b.mapValues("x" + _ + "x").collectres5: Array[(Int, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx),(3,xcatx), (7,xpantherx), (5,xeaglex)) mapWithmapWith是map的另外一个变种，map只需要一个输入函数，而mapWith有两个输入函数。它的定义如下： 1def mapWith[A: ClassTag, U: ](constructA: Int =&gt; A, preservesPartitioning: Boolean = false)(f: (T, A) =&gt; U): RDD[U] 第一个函数constructA是把RDD的partition index（index从0开始）作为输入，输出为新类型A； 第二个函数f是把二元组(T, A)作为输入（其中T为原RDD中的元素，A为第一个函数的输出），输出类型为U。 举例：把partition index 乘以10，然后加上2作为新的RDD的元素。 123val x = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)x.mapWith(a =&gt; a * 10)((a, b) =&gt; (b + 2)).collectres4: Array[Int] = Array(2, 2, 2, 12, 12, 12, 22, 22, 22, 22) flatMap与map类似，区别是原RDD中的元素经map处理后只能生成一个元素，而原RDD中的元素经flatmap处理后可生成多个元素来构建新RDD。 举例：对原RDD中的每个元素x产生y个元素（从1到y，y为元素x的值） 1234scala&gt; val a = sc.parallelize(1 to 4, 2)scala&gt; val b = a.flatMap(x =&gt; 1 to x)scala&gt; b.collectres12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4) flatMapWithflatMapWith与mapWith很类似，都是接收两个函数，一个函数把partitionIndex作为输入，输出是一个新类型A；另外一个函数是以二元组（T,A）作为输入，输出为一个序列，这些序列里面的元素组成了新的RDD。它的定义如下： 1def flatMapWith[A: ClassTag, U: ClassTag](constructA: Int =&gt; A, preservesPartitioning: Boolean = false)(f: (T, A) =&gt; Seq[U]): RDD[U] 举例： 1234scala&gt; val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 3)scala&gt; a.flatMapWith(x =&gt; x, true)((x, y) =&gt; List(y, x)).collectres58: Array[Int] = Array(0, 1, 0, 2, 0, 3, 1, 4, 1, 5, 1, 6, 2, 7, 2,8, 2, 9) flatMapValuesflatMapValues类似于mapValues，不同的在于flatMapValues应用于元素为KV对的RDD中Value。每个一元素的Value被输入函数映射为一系列的值，然后这些值再与原RDD中的Key组成一系列新的KV对。 举例 1234scala&gt; val a = sc.parallelize(List((1,2),(3,4),(3,6)))scala&gt; val b = a.flatMapValues(x=&gt;x.to(5))scala&gt; b.collectres3: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (3,4), (3,5)) 上述例子中原RDD中每个元素的值被转换为一个序列（从其当前值到5），比如第一个KV对(1,2), 其值2被转换为2，3，4，5。然后其再与原KV对中Key组成一系列新的KV对(1,2),(1,3),(1,4),(1,5)。 reducereduce将RDD中元素两两传递给输入函数，同时产生一个新的值，新产生的值与RDD中下一个元素再被传递给输入函数直到最后只有一个值为止。 举例 123scala&gt; val c = sc.parallelize(1 to 10)scala&gt; c.reduce((x, y) =&gt; x + y)res4: Int = 55 上述例子对RDD中的元素求和。 reduceByKey顾名思义，reduceByKey就是对元素为KV对的RDD中Key相同的元素的Value进行reduce，因此，Key相同的多个元素的值被reduce为一个值，然后与原RDD中的Key组成一个新的KV对。 举例: 123scala&gt; val a = sc.parallelize(List((1,2),(3,4),(3,6)))scala&gt; a.reduceByKey((x,y) =&gt; x + y).collectres7: Array[(Int, Int)] = Array((1,2), (3,10)) 上述例子中，对Key相同的元素的值求和，因此Key为3的两个元素被转为了(3,10)。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 概述]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-overview%2F</url>
      <content type="text"><![CDATA[Apache Spark 是一个快速的、多用途的集群计算系统。在 Java，Scala，Python 和 R 语言以及一个支持常见的图计算的经过优化的引擎中提供了高级 API。它还支持一组丰富的高级工具，包括用于 SQL 和结构化数据处理的 Spark SQL，用于机器学习的 MLlib，用于图形处理的 GraphX 以及 Spark Streaming。 下载从该项目官网的 下载页面 获取 Spark，该文档用于 Spark 2.0.2 版本。Spark 使用了用于 HDFS 和 YRAN 的 Hadoop client 的库。为了适用于主流的 Hadoop 版本可以下载先前的 package。用户还可以下载 “Hadoop free” binary 并且可以 通过增加 Spark 的 classpath 来与任何的 Hadoop 版本一起运行 Spark。 如果您希望从源码中构建 Spark，请访问 构建 Spark。 Spark 既可以在 Windows 上运行又可以在类似 UNIX 的系统（例如，Linux，Mac OS）上运行。它很容易在一台机器上本地运行 - 您只需要在您的系统 PATH 上安装 Java，或者将 JAVA_HOME 环境变量指向一个 Java 安装目录。 Spark 可运行在 Java 7+，Python 2.6+/3.4 和 R 3.1+ 的环境上。 针对 Scala API，Spark 2.0.1 使用了 Scala 2.11。 您将需要去使用一个可兼容的 Scala 版本（2.11.x）。 运行示例和 ShellSpark 自带了几个示例程序。 Scala，Java，Python 和 R 的示例在 examples/src/main 目录中。在最顶层的 Spark 目录中使用 bin/run-example [params] 该命令来运行 Java 或者 Scala 中的某个示例程序。（在该例子的底层，调用了 spark-submit 脚本以启动应用程序 ）。 例如， 1./bin/run-example SparkPi 10 您也可以通过一个改进版的 Scala shell 来运行交互式的 Spark。这是一个来学习该框架比较好的方式。 1./bin/spark-shell --master local[2] 这个 –master 选项可以指定为 分布式集群中的 master URL，或者指定为 local 以使用 1 个线程在本地运行，或者指定为 local[N] 以使用 N 个线程在本地运行 。您应该指定为 local 来启动以便测试。该选项的完整列表，请使用 –help 选项来运行 Spark shell。 Spark 同样支持 Python API。在 Python interpreter（解释器）中运行交互式的 Spark，请使用 bin/pyspark : 1./bin/pyspark --master local[2] Python 中也提供了应用示例。例如， 1./bin/spark-submit examples/src/main/python/pi.py 10 从 1.4 开始（仅包含了 DataFrames API）Spark 也提供了一个用于实验性的 R API。为了在 R interpreter（解释器）中运行交互式的 Spark，请执行 bin/sparkR : 1./bin/sparkR --master local[2] R 中也提供了应用示例。例如， 1./bin/spark-submit examples/src/main/r/dataframe.R 在集群上运行Spark 集群模式概述 说明了在集群上运行的主要的概念。Spark 既可以独立运行，也可以在几个已存在的 Cluster Manager（集群管理器）上运行。它当前提供了几种用于部署的选项 : Spark Standalone 模式 : 在私有集群上部署 Spark 最简单的方式。 Spark on Mesos Spark on YARN 快速跳转编程指南 : 快速入门 : 简单的介绍 Spark API，从这里开始！~ Spark 编程指南 : 在所有 Spark 支持的语言（Scala，Java，Python，R）中的详细概述。 构建在 Spark 之上的模块 : Spark Streaming : 实时数据流处理。 Spark SQL，Datasets，和 DataFrames : 支持结构化数据和关系查询。 MLlib : 内置的机器学习库。 GraphX : 新一代用于图形处理的 Spark API。 API 文档: Spark Scala API(Scaladoc) Spark Java API(Javadoc) Spark Python API(Sphinx) Spark R API(Roxygen2) 部署指南: 集群模式概述 : 在集群上运行时概念和组件的概述。 提交应用程序 : 打包和部署应用。 部署模式 : Amazon EC2 : 花费大约 5 分钟的时间让您在 EC2 上启动一个集群的介绍 Spark Standalone 模式 : 在不依赖第三方 Cluster Manager 的情况下快速的启动一个独立的集群 部署案例 Spark on Mesos : 使用 Apache Mesos 来部署一个私有的集群 Spark on YARN : 在 Hadoop NextGen（YARN）上部署 Spark 部署案例 其他文件: 配置: 通过它的配置系统定制 Spark 监控 : 监控应用程序的运行情况 优化指南 : 性能优化和内存调优的最佳实践 作业调度 : 资源调度和任务调度 安全性 : Spark 安全性支持 硬件配置 : 集群硬件挑选的建议 与其他存储系统的集成 : OpenStack Swift 构建 Spark : 使用 Maven 来构建 Spark Contributing to Spark Third Party Projects : 其它第三方 Spark 项目的支持 外部资源: Spark 主页 Spark Wiki Spark 社区 资源，包括当地的聚会 StackOverflow tag apache-spark 邮件列表 : 在这里询问关于 Spark 的问题 AMP 营地 在加州大学伯克利分校: 一系列的训练营, 特色和讨论 练习对 Spark，Spark Steaming，Mesos 以及更多。可以免费通过 视频 , 幻灯片 和 练习 学习。 代码示例 : 更多示例可以在 Spark 的子文件夹中（Scala , Java , Python , R ）获得。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 监控]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-monitoring%2F</url>
      <content type="text"><![CDATA[有几种方法可以监视 Spark 应用 : Web UI，metrics 和其他扩展工具。 Web 接口每一个 SparkContext 启动一个 web UI，默认情况下使用端口 4040，可以显示关于运行程序的有用信息。这包括 : 调度器阶段和任务的列表 RDD 大小和内存使用的概要信息 环境信息 正在运行的程序的信息 您只需打开 http://\:4040 的 web 浏览器就可以访问。如果在同一主机上运行多个 SparkContexts，他们将开始连续绑定到端口 4040（4041、4042、等）。 注意，默认情况下这些信息仅在有程序的执行时显示。你可以在启动 Spark 之前修改配置，设置 spark.eventLog.enabled 为 true。让 Spark 记录并持久化存储 Spark 事件使其可以在 UI 中显示。 历史信息如果 Spark 在 Mesos 或者 YARN 上运行，它仍有可能用已存在的程序日志通过 Spark history server（历史信息记录服务）来显示该程序运行时的详细信息。启动命令如下: 1./sbin/start-history-server.sh 这个会默认创建一个 web 接口 : http://\:18080，显示未完成、完成以及其他尝试的任务信息。 当指定使用一个文件系统提供 class 类（具体见下 spark.history.provider），那么基本的日志存储路径应该在 spark.history.fs.logDirectory 这个配置中指定，并且会有子目录，每个都表示某个程序信息的日志 log。 Spark 任务本身必须配置启用日志，并用相同的、共享的、可写的目录记录他们。例如，如果服务器配置的日志目录为 12spark.eventLog.enabled truespark.eventLog.dir hdfs://namenode/shared/spark-logs 那么 history server 的配置信息可以如下 : 环境变量 环境变量 含义 SPARK_DAEMON_MEMORY history server 分配内存（默认 : 1g）. SPARK_DAEMON_JAVA_OPTS history server JVM 配置（默认 : none）. SPARK_PUBLIC_DNS history server 的公用地址. 如果未配置， 可能会使用服务器的内部地址， 导致连接失效 （默认 : none）. SPARK_HISTORY_OPTS spark.history.* history server 的相关配置选项 （默认 : none）. 文件：conf/spark-env.sh Spark 配置选项 属性名称 默认 含义 spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider 实现了 history backend 的 class 类的名称。目前只有一个实现，Spark 本身提供，用于检测存在文件系统中的程序的日志文件。 spark.history.fs.logDirectory http://file/tmp/spark-events 提供历史日志文件存储路径，URL地址（包含可加载的程序日志的目录）。可以配置本地路径 file://，或者 HDFS 路径 hdfs://namenode/shared/spark-logs 或其他 Hadoop API 支持的文件系统。 spark.history.fs.update.interval 10s 以秒为单位，更新日志相关信息的时间间隔，更短的时间间隔帮助检测到新的程序更快，但是牺牲更多的服务器负载。一旦更新完成，完成和未完成的程序的都会发生变化。 spark.history.retainedApplications 50 保存在 UI 缓存中的程序数量。如果超过这个上限，那么时间最老的程序将从缓存中移除。如果一个程序未被缓存，它就必须从磁盘加载。 spark.history.ui.maxApplications Int.MaxValue 显示在总历史页面中的程序的数量。如果总历史页面未显示，程序 UI 仍可通过访问其 URL 来显示。 spark.history.ui.port 18080 UI 端口设置. spark.history.kerberos.enabled false 表明 history server 是否应该使用 kerberos 登录。如果历史服务器访问 HDFS 文件安全的 Hadoop 集群，这是必需的。如果设置为 true，将会使用 spark.history.kerberos.principal 和 spark.history.kerberos.keytab 两个配置。 spark.history.kerberos.principal （none） Kerberos 凭证名称。 spark.history.kerberos.keytab （none） kerberos keytab 文件位置。 spark.history.ui.acls.enable false 指定是否通过 acl 授权限制用户查看程序。如果启用，程序运行时会强制执行访问控制检查，检测程序是否设置 spark.ui.acls.enable，程序所有者总能够查看自己的应用程序和任何指定 spark.ui.view.acls 和 spark.ui.view.acls.groups 的程序。当程序运行也将要授权查看。如果禁用，就没有访问控制检查。 spark.history.fs.cleaner.enabled false 是否定期清理历史日志文件。 spark.history.fs.cleaner.interval 1d 清理历史日志文件的间隔，只会清理比 spark.history.fs.cleaner.maxAge 更老的日志文件。 spark.history.fs.cleaner.maxAge 7d 日志文件的最大年龄，超过就会被清理。 spark.history.fs.numReplayThreads 25% of available cores histroy sever 操作日志的最大线程数。 文件： conf/spark-defaults.conf 请注意，在所有这些 UI，表是可排序的点击他们的表头，让它容易更容易识别任务缓慢，数据倾斜的任务。 注意 : history server 会显示完成和未完成的 Spark 任务。如果一个程序多次尝试失败后，这时会显示失败的尝试，或任何正在进行的却未完成的尝试，或者最终成功的尝试。 未完成的程序只会间歇性地更新。更新之间的时间间隔定义在的检查文件变化间隔（spark.history.fs.update.interval）。在更大的集群上更新间隔可能被设置为更大值。可以通过 web UI 查看正在运行的程序。 程序如果退出时没有表明注册已经完成，将会被列入未完成的一栏，尽管他们不再运行。如果一个应用程序崩溃可能导致该情况发生，。 一种表明 Spark 工作完成方法是显式调用停止 Spack Context 的 sc.stop() 方法，或者在 Python 中使用 with SparkContext() as sc : 构造方法来加载和去除 Spark Context。 REST API除了 UI 中查看这些指标，也可以得到任务信息的 JSON ，这个能够是开发者更方便的创造新的 Spark 可视化和监控。 正在运行的程序和历史的程序都可以得到他们的 JSON 信息。挂载在 /api/v1，比如，对于 histroy server，他们通常会访问 http://\:18080/api/v1 ，对于运行的程序，则是 http://localhost:4040/api/v1。 API 中，程序拥有一个程序 ID， [app-id]。在 YARN 上执行时，每个程序可能进行有多次尝试，但尝试的 ID（attempt IDs）只有在集群模式下有，客户端模式的程序没有。在 YARN 集群模式上执行的程序会有 [attempt-id]。在下面列出的 API 中，是在 YARN 集群模式下运行时， [app-id] 就是 [base-app-id]/[attempt-id]，[base-app-id] 是 YARN 的程序 ID。 位置 含义 /applications 所有的应用程序的列表。?status=[completed\ running] 显示特定状态的程序。?minDate=[date] 显示的最早时间.。示例 :?minDate=2015-02-10；?minDate=2015-02-03T16:42:40.000GMT；?maxDate=[date] 显示的最新的时间; 格式和 minDate 相同；?limit=[limit] 程序显示数量的限制。 /applications/[app-id]/jobs 指定应用的所有 Job（作业）列表 :?status=[complete\ succeeded\ failed] 显示特定状态的信息。 /applications/[app-id]/jobs/[job-id] 某个 job 的详情。 /applications/[app-id]/stages 显示某个程序的 stages 列表。 /applications/[app-id]/stages/[stage-id] 显示给定的 stages-id 状态。?status=[active\ complete\ pending\ failed] 显示特定状态的信息。 /applications/[app-id]/stages/[stage-id]/[stage-attempt-id] 指定 stage attempt 的详情。 /applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskSummary stage attempt 的指标集合统计。?quantiles 统计指定的 quantiles。示例 : ?quantiles=0.01，0.5，0.99 /applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList 指定 stage attempt 的 task 列表。?offset=[offset]&amp;length=[len] 显示指定范围的task.。?sortBy=[runtime\ -runtime] task 排序.。示例 : ?offset=10&amp;length=50&amp;sortBy=runtime /applications/[app-id]/executors 程序的 executors。 /applications/[app-id]/storage/rdd 程序的 RDDs。 /applications/[app-id]/storage/rdd/[rdd-id] 指定 RDD 详情。 /applications/[base-app-id]/logs 以 zip 文件形式下载指定程序的 log。 /applications/[base-app-id]/[attempt-id]/logs 以 zip 文件形式下载指定程序 atempt 的 log。 可恢复的 jobs 和 stages 的数量被独立的 Spark UI 的相同保留机制所约束; “spark.ui.retainedJobs” 定义触发回收垃圾 jobs 阈值，和 spark.ui.retainedStages 限定 stages。注意，配置需要重启才能生效:通过增加这些值和重新启动服务器才可以保存获取更多的信息。 API 版本政策这些 endpoints 都已经版本化以便在其之上开发，Spark 保证 : endpoints 不会被移除 对于任何给定的 endpoint，不会删除个别 fields 可能新增 endpoints 可能新增已有 endpoints 的 fields 将来可能在单独的 endpoint（例如，api / v2）添加 api 的新版本。 新版本不需要向后兼容。 API 版本可能被删除，但之前至少有版本老的 API 与新的 API 版本共存。 注意，即使检查正在运行的程序的 UI，applications/[app-id] 部分仍然是必需的，尽管只有一个程序可用。如。查看正在运行的应用程序的工作列表，你会去 http://localhost:4040/api/v1/applications/[app-id]/jobs。这是保持在两种模式下的路径一致。 MetricsSpark 拥有可配置的 metrics system （度量系统） 其基于 Dropwizard Metrics Library。这使用户可以通过多种 sinks 比如 HTTP，JMX，CSV 文件报告 Spark 的各项指标。Metrics system 是通过一个配置文件配置， Spark 需要其在路径 $SPARK_HOME/conf/metrics.properties 下。 可以通过 spark.metrics.conf 配置属性 指定自定义文件的位置 。Spark 的指标拥有不同实例，对于不同的 Spark 组件是解耦的。在每个实例中，您可以配置一组需要的 Metrics 。目前支持以下 : master : Spark 独立的 master 进程。 applications : 一个 master 组件用于报告各种程序。 worker : Spark 独立的 worker 进程。 executor : Spark executor。 driver : Spark 驱动进程，（创建 SparkContext 的进程）。 每个例子都可以报告 0 项以上的 sinks，包含在 org.apache.spark.metrics.sink 之中 ConsoleSink : 记录指标信息到控制台。 CSVSink : 周期记录到 CSV 文件。 JmxSink : JMX 监控。 MetricsServlet : 启动 Servlet 向 Spark UI 提供 JSON 格式信息。 GraphiteSink : 传送到 Graphite 节点。 Slf4jSink : 存到 slf4j 。 Spark 同样支持 Ganglia sink ，因为版权原因无法默认安装 : GangliaSink : 发到 Ganglia 节点或者组播组（multicast group）。 安装 GangliaSink 需要构建一个定制的 Spark。注意，通过嵌入这个插件库 Spark 将包括 LGPL-licensed 授权的代码。对于 sbt 用户，构建前设置 SPARK_GANGLIA_LGPL 环境变量。对于 Maven 用户，要使 -Pspark-ganglia-lgpl 生效，除了修改 Spark 集群构建的环境配置，用户程序也需要加入 spark-ganglia-lgpl 组件。标准配置的语法在示例文件中，$SPARK_HOME/conf/metrics.properties.template。 高级工具一些扩展工具可以用来帮助监控 Spark 任务性能 : 集群监控工具， 比如 Ganglia， 可以对整个集群的利用率和性能瓶颈监控。比如， 一个 Ganglia 监控表盘可以很快地展示一个特定任务是否占用磁盘，网络，cpu。 操作系统分析工具如 dstat， iostat， 和 iotop 可以在单个节点上提供细粒度分析。 JVM 工具例如 jstack 生成 stack traces（线程堆栈信息）， jmap 生成 heap-dump（内存信息）， jstat 报告运行状态信息，并且 jconsole 提供对帮助理解 JVM 核心的各项属性可视化监视、管理。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 配置]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-configuration%2F</url>
      <content type="text"><![CDATA[Spark 提供了三个位置对系统进行配置： Spark 属性：通过 SparkConf 对象或者通过 Java 系统参数来控制大多数的参数， 环境变量：用于对每个节点进行设置， 例如通过 conf/spark-env.sh 脚本配置 IP 等。 日志：通过 log4j.properties 配置。 Spark 属性Spark 属性可以控制大多数的应用程序设置，并且每个应用的设置都是分开的。这些属性可以通过 SparkConf 对象直接设定。SparkConf 为一些常用的属性定制了专用方法（如，master URL 和 application name），其他属性都可以用键值对做参数，调用 set() 方法来设置。例如，我们可以初始化一个包含 2 个本地线程的 Spark 应用，代码如下： 注意，local[2] 代表 2 个本地线程 – 这是最小的并发方式，可以帮助我们发现一些只有在分布式上下文才能复现的 bug。 1234val conf = new SparkConf() .setMaster("local[2]") .setAppName("CountingSheep")val sc = new SparkContext(conf) 注意，本地模式下，我们可以使用多个线程。而且在像 Spark Streaming 这样的场景下，我们可能需要多个线程来防止类似线程饿死这样的问题。 配置时间段的属性应该写明时间单位，如下格式都是可接受的： 12345625ms (milliseconds)5s (seconds)10m or 10min (minutes)3h (hours)5d (days)1y (years) 配置大小的属性也应该写明单位，如下格式都是可接受的： 1234561b (bytes)1k or 1kb (kibibytes = 1024 bytes)1m or 1mb (mebibytes = 1024 kibibytes)1g or 1gb (gibibytes = 1024 mebibytes)1t or 1tb (tebibytes = 1024 gibibytes)1p or 1pb (pebibytes = 1024 tebibytes) 动态加载 Spark 属性在某些场景下，你可能想避免将属性值写死在 SparkConf 中。例如，你可能希望在同一个应用上使用不同的 master 或不同的内存总量。Spark 允许你简单地创建一个空的 SparkConf 对象： 1val sc = new SparkContext(new SparkConf()) 然后在运行时设置这些属性： 12./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar Spark shell 和 spark-submit 工具支持两种动态加载配置的方法。第一种，通过命令行选项，如：上面提到的–master。spark-submit 可以在启动 Spark 应用时，通过–conf 标志接受任何属性配置，同时有一些特殊配置参数同样可用。运行./bin/spark-submit –help 可以展示这些选项的完整列表。 同时，bin/spark-submit 也支持从 conf/spark-defaults.conf 中读取配置选项，在该文件中每行是一个键值对，并用空格分隔，如下： 1234spark.master spark://5.6.7.8:7077spark.executor.memory 4gspark.eventLog.enabled truespark.serializer org.apache.spark.serializer.KryoSerializer 这些通过参数或者属性配置文件传递的属性，最终都会在 SparkConf 中合并。其优先级是：首先是 SparkConf 代码中写的属性值，其次是 spark-submit 或 spark-shell 的标志参数，最后是 spark-defaults.conf 文件中的属性。有一些配置项被重命名过，这种情形下，老的名字仍然是可以接受的，只是优先级比新名字优先级低。 查看 Spark 属性Spark 应用程序 http://\:4040” 的 Environment“ tab 页可以查看 Spark 属性。如果你真的想确认一下属性设置是否正确的话，这个功能就非常有用了。注意，只有显式地通过 SparkConf 对象、在命令行参数、或者 spark-defaults.conf 设置的参数才会出现在页面上。其他属性，你可以认为都是默认值。 可用的属性绝大多数属性都有合理的默认值。这里是部分常用的选项： 应用程序属性 Property Name Default Meaning spark.app.name (none) Spark 应用的名字。会在 SparkUI 和日志中出现。 spark.dirver.cores 1 在 cluster 模式下，用几个 core 运行 driver 进程。 spark.driver.maxResultSize 1g Spark action 算子返回的结果集的最大数量。至少要 1M，可以设为 0 表示无限制。如果结果超过这一大小，Spark job 会直接中断退出。但是，设得过高有可能导致 driver 出现 out-of-memory 异常（取决于 spark.driver.memory 设置，以及驱动器 JVM 的内存限制）。设一个合理的值，以避免 driver 出现 out-of-memory 异常。 spark.driver.memory 1g driver 进程可以使用的内存总量（如：1g，2g）。注意，在 client 模式下，这个配置不能在 SparkConf 中直接设置，应为在那个时候 driver 进程的 JMR 已经启动了。因此需要在命令行里用 –-driver-memory 选项 或者在默认属性配置文件里设置。 spark.executor.memory 1g 每个 executor 进程使用的内存总量（如，2g，8g）。 spark.extraListeners (none) 逗号分隔的实现 SparkListener 接口的类名列表；初始化 SparkContext 时，这些类的实例会被创建出来，并且注册到 Spark 的监听器上。如果这些类有一个接受 SparkConf 作为唯一参数的构造函数，那么这个构造函数会被调用；否则，就调用无参构造函数。如果没有合适的构造函数，SparkContext 创建的时候会抛异常。 spark.local.dir /tmp Spark 的” 草稿 “目录，包括 map 输出的临时文件以及 RDD 存在磁盘上的数据。这个目录最好在本地文件系统中。这个配置可以接受一个以逗号分隔的多个挂载到不同磁盘上的目录列表。注意：Spark-1.0 及以后版本中，这个属性会被 cluster manager 设置的环境变量覆盖：SPARK_LOCAL_DIRS（Standalone,Mesos）或者 LOCAL_DIRS（YARN）。 spark.logConf false SparkContext 启动时是否把生效的 SparkConf 属性以 INFO 日志打印到日志里 spark.master (none) 要连接的 cluster manager。参考 Cluster Manager 类型 spark.submit.deployMode (none) Spark driver 程序的部署模式，可以是 “client” 或 “cluster”，意味着部署 dirver 程序本地 (“client”) 或者远程 (“cluster”) 在 Spark 集群的其中一个节点上。 运行时环境 Property Name Default Meaning spark.driver.extraClassPath (none) 额外的路径需预先考虑到驱动程序 classpath。注意: 在客户端模式下，这一套配置不能通过 SparkConf 直接在应用在应用程序中，因为 JVM 驱动已经启用了。相反，请在配置文件中通过设置 —-driver-class-path 选项或者选择默认属性。 spark.driver.extraJavaOptions (none) 一些额外的 JVM 属性传递给驱动。例如，GC 设置或其他日志方面设置。注意，设置最大堆大小 (-Xmx) 是不合法的。最大堆大小设置可以通过在集群模式下设置 spark.driver.memory 选项，并且可以通过 –driver-memory 在客户端模式设置。注意：在客户端模式下，这一套配置不能通过 SparkConf 直接应用在应用程序中，因为 JVM 驱动已经启用了。相反，请在配置文件中通过设置 –driver-java-options 选项或者选择默认属性。 spark.driver.extraLibraryPath (none) 当启动 JVM 驱动程序时设置一个额外的库路径。注意: 在客户端模式下，这一套配置不能通过 SparkConf 直接在应用在应用程序中，因为 JVM 驱动已经启用了。相反，请在配置文件中通过设置 –driver-library-path 选项或者选择默认属性。 spark.driver.userClassPathFirst (none) （实验）在驱动程序加载类库时，用户添加的 Jar 包是否优先于 Spark 自身的 Jar 包。这个特性可以用来缓解冲突引发的依赖性和用户依赖。目前只是实验功能。这是仅用于集群模式。 spark.executor.extraClassPath (none) 额外的类路径要预先考虑到 executor 的 classpath。这主要是为与旧版本的 Spark 向后兼容。用户通常不应该需要设置这个选项。 spark.executor.extraJavaOptions (none) 一些额外的 JVM 属性传递给 executor。例如，GC 设置或其他日志方面设置。注意，设置最大堆大小 (-Xmx) 是不合法的。Spark 应该使用 SparkConf 对象或 Spark 脚本中使用的 spark-defaults.conf 文件中设置。最大堆大小设置可以在 spark.executor.memory 进行设置。 spark.executor.extraLibraryPath (none) 当启动 JVM 的可执行程序时设置额外的类库路径。 spark.executor.logs.rolling.maxRetainedFiles (none) 最新回滚的日志文件将被系统保留。旧的日志文件将被删除。默认情况下禁用。 spark.executor.logs.rolling.maxSize (none) 设置最大文件的大小, 以字节为单位日志将被回滚。默认禁用。见 spark.executor.logs.rolling.maxRetainedFiles 旧日志的自动清洗。 spark.executor.logs.rolling.strategy (none) 设置 executor 日志的回滚策略。它可以被设置为 “时间”（基于时间的回滚）或 “大小”（基于大小的回滚）。对于 “时间”，使用 spark.executor.logs.rolling.time.interval 设置回滚间隔。用 spark.executor.logs.rolling.maxSize 设置最大文件大小回滚。 spark.executor.logs.rolling.time.interval daily 设定的时间间隔，executor 日志将回滚。默认情况下是禁用的。有效值是每天，每小时，每分钟或任何时间间隔在几秒钟内。见 spark.executor.logs.rolling.maxRetainedFiles 旧日志的自动清洗。 spark.executor.userClassPathFirst false （实验）与 spark.driver.userClassPathFirst 相同的功能，但适用于执行程序的实例。 spark.executorEnv.[EnvironmentVariableName] (none) 通过添加指定的环境变量 EnvironmentVariableName 给 executor 进程。用户可以设置多个环境变量。 spark.python.profile false 启用在 python 中的 profile。结果将由 sc.show_profiles() 显示, 或者它将会在驱动程序退出后显示。它还可以通过 sc.dump_profiles dump 到磁盘。如果一些 profile 文件的结果已经显示，那么它们将不会再驱动程序退出后再次显示。默认情况下，pyspark.profiler.BasicProfiler 将被使用，但这可以通过传递一个 profile 类作为一个参数到 SparkContext 中进行覆盖。 spark.python.profile.dump (none) 这个目录是在驱动程序退出后，proflie 文件 dump 到磁盘中的文件目录。结果将为每一个 RDD dump 为分片文件。它们可以通过 ptats.Stats() 加载。如果指定，profile 结果将不会自动显示。 spark.python.worker.memory 512m 在聚合过程中，每个 python 进程所用的内存大小，和 JVM 内存相同的格式。如果内存中超过设定值，就会溢出到磁盘。 spark.python.worker.reuse true 重用 python worker。如果为 true，它将使用固定数量的 worker 数量。不需要为每一个任务分配 python 进程。如果是大型的这将是非常有用。 Shuffle 行为 (Behavior) Property Name Default Meaning spark.reducer.maxSizeInFlight 48m 从每个 Reduce 任务中并行的 fetch 数据的最大大小。因为每个输出都要求我们创建一个缓冲区，这代表要为每一个 Reduce 任务分配一个固定大小的内存。除非内存足够大否则尽量设置小一点。 spark.reducer.maxReqsInFlight Int.MaxValue 在集群节点上，这个配置限制了远程 fetch 数据块的连接数目。当集群中的主机数量的增加时候，这可能导致大量的到一个或多个节点的主动连接，导致负载过多而失败。通过限制获取请求的数量，可以缓解这种情况。 spark.shuffle.compress true 是否要对 map 输出的文件进行压缩。默认为 true，使用 spark.io.compression.codec 。 spark.shuffle.file.buffer 32k 每个 shuffle 文件输出流的内存大小。这些缓冲区的数量减少了磁盘寻道和系统调用创建的 shuffle 文件。 spark.shuffle.io.maxRetries 3 (仅适用于 Netty) 如果设置了非 0 值，与 IO 异常相关失败的 fetch 将自动重试。在遇到长时间的 GC 问题或者瞬态网络连接问题时候，这种重试有助于大量 shuffle 的稳定性。 spark.shuffle.io.numConnectionsPerPeer 1 (仅适用于 Netty) 主机之间的连接被重用，以减少更多集群创建连接。对于硬盘和一些主机的集群，这可能导致磁盘并发不足，所以需要考虑到这一点。 spark.shuffle.io.preferDirectBufs true (仅适用于 Netty) 堆缓冲区用于减少在 shuffle 和缓存块传输中的垃圾回收。对于严格限制的堆内存环境中，用户可能希望把这个设置关闭，使得所有分配从 Netty 到堆。 spark.shuffle.io.retryWait 5s (仅适用于 Netty)fetch 重试的等待时长。默认 15s。计算公式是 maxRetries * retryWait。 spark.shuffle.service.enabled false 使用外部 shuffle。这个服务提供了由 executor 写出的 shuffle 文件所以 executor 可以被安全移除。 如果 spark.dynamicAllocation.enabled 设置为 true 那么这个选项一定是 true。外部 shuffle 服务必须通过设置来启用它。通过 动态分配配置和设置文档 了解更多信息。 spark.shuffle.service.port 7337 外部 shuffle 的运行端口。 spark.shuffle.sort.bypassMergeThreshold 200 在基于排序的 shuffle 管理中，如果没有在 map 端的数据聚集并且如果有这个数据量的 reduce partition，可以避免归并排序。 spark.shuffle.spill.compress true shuffle 过程中对溢出的文件是否压缩。使用 spark.io.compression.codec. spark.io.encryption.enabled false Enable IO encryption. Currently supported by all modes except Mesos. It’s recommended that RPC encryption be enabled when using this feature. spark.io.encryption.keySizeBits 128 IO encryption key size in bits. Supported values are 128, 192 and 256. spark.io.encryption.keygen.algorithm HmacSHA1 The algorithm to use when generating the IO encryption key. The supported algorithms are described in the KeyGenerator section of the Java Cryptography Architecture Standard Algorithm Name Documentation. Spark UI Property Name Default Meaning spark.eventLog.compress false 是否压缩日志文件。如果设置为 ture 即为压缩。 spark.eventLog.dir file:///tmp/spark-events Spark 事件日志的文件路径。如果 spark.eventLog.enabled 为 true。在这个基本目录下，Spark 为每个应用程序创建一个二级目录，日志事件特定于应用程序的目录。用户可能希望设置一个统一的文件目录像一个 HDFS 目录那样，所以历史文件可以从历史文件服务器中读取。 spark.eventLog.enabled false 是否对 Spark 事件记录日志。在应用程序启动后有助于重建 Web UI。 spark.ui.killEnabled true 允许从 Web UI 中结束相应的工作进程。 spark.ui.port 4040 应用 UI 的端口，用于显示内存和工作负载数据。 spark.ui.retainedJobs 1000 在垃圾回收前，Spark UI 和 API 有多少 Job 可以留存。 spark.ui.retainedStages 1000 在垃圾回收前，Spark UI 和 API 有多少 Stage 可以留存。 spark.ui.retainedTasks 100000 在垃圾回收前，Spark UI 和 API 有多少 Task 可以留存。 spark.worker.ui.retainedExecutors 1000 在垃圾回收前，Spark UI 和 API 有多少 executor 已经完成。 spark.worker.ui.retainedDrivers 1000 在垃圾回收前，Spark UI 和 API 有多少 driver 已经完成。 spark.sql.ui.retainedExecutions 1000 在垃圾回收前，Spark UI 和 API 有多少 execution 已经完成。 spark.streaming.ui.retainedBatches 1000 在垃圾回收前，Spark UI 和 API 有多少 batch 已经完成。 spark.ui.retainedDeadExecutors 1000 在垃圾回收前，Spark UI 和 API 有多少 dead executors。 压缩和序列化（Compression and Serialization） Property Name Default Meaning spark.broadcast.compress true 是否在发送广播变量前压缩。通常是个好主意。 spark.io.compression.codec lz4 内部数据使用的压缩编解码器，如 RDD 分区，广播变量和混洗输出。 默认情况下，Spark 提供三种编解码器：lz4, lzf 和 snappy。您还可以使用完全限定类名来指定编码解码器，例如：org.apache.spark.io.LZ4CompressionCodec，org.apache.spark.io.LZFCompressionCodec 和 org.apache.spark.io.SnappyCompressionCodec。 spark.io.compression.lz4.blockSize 32k 在采用 LZ4 压缩编解码器的情况下，LZ4 压缩使用的块大小。减少块大小还将降低采用 LZ4 时的混洗内存使用。 spark.io.compression.snappy.blockSize 32k 在采用 Snappy 压缩编解码器的情况下，Snappy 压缩使用的块大小。减少块大小还将降低采用 Snappy 时的混洗内存使用。 spark.kryo.classesToRegister (none) 如果你采用 Kryo 序列化，给一个以逗号分隔的自定义类名列以注册 Kryo。有关详细信息，请参阅 调优指南。 spark.kryo.referenceTracking true (当使用 Spark SQL Thrift Server 为 false) 是否在采用 Kryo 序列化数据时跟踪对同一对象的引用，如果你的对象图形包含循环则是有必要的，并且如果它们包含同一对象的多个副本则对效率有用。 如果你知道这不是这样，可以禁用提高性能。 spark.kryo.registrationRequired false 是否需要注册 Kryo。 如果设置为’true’，如果未注册的类被序列化，Kryo 将抛出异常。如果设置为 false（默认值），Kryo 将与每个对象一起写入未注册的类名。 编写类名可能会导致显著的性能开销，因此启用此选项可以严格强制用户没有从注册中省略类。 spark.kryo.registrator (none) 如果你采用 Kryo 序列化，则给一个逗号分隔的类列表，以使用 Kryo 注册你的自定义类。 如果你需要以自定义方式注册你的类，则此属性很有用，例如以指定自定义字段序列化程序。 否则，使用 spark.kryo.classesToRegisteris 更简单。 它应该设置为 KryoRegistrator 的子类。 详见：调优指南。 spark.kryoserializer.buffer.max 64m Kryo 序列化缓冲区的最大允许大小。 这必须大于你需要序列化的任何对象。 如果你在 Kryo 中得到一个 “buffer limit exceeded” 异常，你就需要增加这个值。 spark.kryoserializer.buffer 64k Kryo 序列化缓冲区的初始大小。 注意，每个 worker 上每个 core 会有一个缓冲区。 如果需要，此缓冲区将增长到 spark.kryoserializer.buffer.max。 spark.rdd.compress false 是否压缩序列化的 RDD 分区（例如，在 Java 和 Scala 中为 forStorageLevel.MEMORY_ONLY_SER 或在 Python 中为 StorageLevel.MEMORY_ONLY）。 能节省大量空间，但多消耗一些 CPU 时间。 spark.serializer org.apache.spark.serializer.JavaSerializer (当使用 Spark SQL Thrift Server 时为 org.apache.spark.serializer.KryoSerializer) 用于序列化将通过网络发送或需要以序列化形式缓存的对象的类。 Java 序列化的默认值适用于任何可序列化的 Java 对象，但是速度相当慢，因此我们建议使用 org.apache.spark.serializer.KryoSerializer 并在需要速度时配置 Kryo 序列化。当然你可以通过继承 org.apache.spark.Serializer 类自定义一个序列化器 。 spark.serializer.objectStreamReset 100 当正使用 org.apache.spark.serializer.JavaSerializer 序列化时, 序列化器缓存对象虽然可以防止写入冗余数据，但是却停止这些缓存对象的垃圾回收。通过调用’reset’你从序列化程序中清除该信息，并允许收集旧的对象。 要禁用此周期性重置，请将其设置为 - 1。 默认情况下，序列化器会每过 100 个对象被重置一次。 内存管理 Property Name Default Meaning spark.memory.fraction 0.6 用于执行和存储的（堆空间 - 300MB）的分数。这个值越低，溢出和缓存数据逐出越频繁。 此配置的目的是在稀疏、异常大的记录的情况下为内部元数据，用户数据结构和不精确的大小估计预留内存。推荐使用默认值。 有关更多详细信息，包括关于在增加此值时正确调整 JVM 垃圾回收的重要信息，请参阅 this description。 spark.memory.storageFraction 0.5 不会被逐出内存的总量，表示为 spark.memory.fraction 留出的区域大小的一小部分。 这个越高，工作内存可能越少，执行和任务可能更频繁地溢出到磁盘。 推荐使用默认值。有关更多详细信息，请参阅 this description。 spark.memory.offHeap.enabled false 如果为 true，Spark 会尝试对某些操作使用堆外内存。 如果启用了堆外内存使用，则 spark.memory.off Heap.size 必须为正值。 spark.memory.offHeap.size 0 可用于堆外分配的绝对内存量（以字节为单位）。 此设置对堆内存使用没有影响，因此如果您的执行器的总内存消耗必须满足一些硬限制，那么请确保相应地缩减 JVM 堆大小。 当 spark.memory.offHeap.enabled = true 时，必须将此值设置为正值。 spark.memory.useLegacyMode false 是否启用 Spark 1.5 及以前版本中使用的传统内存管理模式。 传统模式将堆空间严格划分为固定大小的区域，如果未调整应用程序，可能导致过多溢出。 必须启用本参数，以下选项才可用：spark.shuffle.memoryFraction spark.storage.memoryFraction spark.storage.unrollFraction spark.shuffle.memoryFraction 0.2 （废弃）只有在启用 spark.memory.useLegacyMode 时，此属性才是可用的。 混洗期间用于聚合和 cogroups 的 Java 堆的分数。 在任何给定时间，用于混洗的所有内存映射的集合大小不会超过这个上限，超过该限制的内容将开始溢出到磁盘。 如果溢出频繁，请考虑增加此值，但这以 spark.storage.memoryFraction 为代价。 spark.storage.memoryFraction 0.6 （废弃）只有在启用 spark.memory.useLegacyMode 时，此属性才是可用的。 Java 堆的分数，用于 Spark 的内存缓存。 这个值不应该大于 JVM 中老生代（old generation) 对象所占用的内存，默认情况下，它提供 0.6 的堆，但是如果配置你所用的老生代对象大小，你可以增加它。 spark.storage.unrollFraction 0.2 （废弃）只有在启用 spark.memory.useLegacyMode 时，此属性才是可用的。spark.storage.memoryFraction 用于在内存中展开块的分数。 当没有足够的空闲存储空间来完全展开新块时，通过删除现有块来动态分配。 执行行为 (Execution Behavior) Property Name Default Meaning spark.broadcast.blockSize 4m TorrentBroadcastFactory 的一个块的每个分片大小。 过大的值会降低广播期间的并行性（更慢了）; 但是，如果它过小，BlockManager 可能会受到性能影响。 spark.executor.cores 在 YARN 模式下默认为 1，独立和 Mesos 粗粒度模型中的 worker 节点的所有可用的 core。 单个执行器上使用的 core 数。 在独立和 Mesos 粗粒度模式下，设置此参数允许应用在同一 worker 上运行多个执行器，只要该 worker 上有足够的 core。 否则，每个应用在单个 worker 上只会启动一个执行器。 spark.default.parallelism 对于分布式混洗（shuffle）操作，如 reduceByKey 和 join，父 RDD 中分区的最大数量。 对于没有父 RDD 的并行操作，它取决于集群管理器：本地模式：本地机器上的 core 数；Mesos 细粒度模式：8；其他：所有执行器节点上的 core 总数或者 2，以较大者为准 如果用户没有指定参数值，则这个属性是 join，reduceByKey 和 parallelize 等转换返回的 RDD 中的默认分区数。 spark.executor.heartbeatInterval 10s 每个执行器的心跳与驱动程序之间的间隔。 心跳让驱动程序知道执行器仍然存活，并用正在进行的任务的指标更新它。 spark.files.fetchTimeout 60s 获取文件的通讯超时，所获取的文件是从驱动程序通过 SparkContext.addFile（）添加的。 spark.files.useFetchCache true 如果设置为 true（默认），文件提取将使用由属于同一应用程序的执行器共享的本地缓存，这可以提高在同一主机上运行许多执行器时的任务启动性能。 如果设置为 false，这些缓存优化将被禁用，所有执行器将获取它们自己的文件副本。 如果使用驻留在 NFS 文件系统上的 Spark 本地目录，可以禁用此优化（有关详细信息，请参阅 SPARK-6313 ）。 spark.files.overwrite false 当目标文件存在且其内容与源不匹配的情况下，是否覆盖通过 SparkContext.addFile（）添加的文件。 spark.hadoop.cloneConf false 如果设置为 true，则为每个任务克隆一个新的 Hadoop 配置对象。 应该启用此选项以解决配置线程安全问题（有关详细信息，请参阅 SPARK-2546 ）。 默认情况下禁用此功能，以避免不受这些问题影响的作业的意外性能回退。 spark.hadoop.validateOutputSpecs true 如果设置为 true，则验证 saveAsHadoopFile 和其他变体中使用的输出规范（例如，检查输出目录是否已存在）。 可以禁用此选项以静默由于预先存在的输出目录而导致的异常。 我们建议用户不要禁用此功能，除非需要实现与以前版本的 Spark 的兼容性。 可以简单地使用 Hadoop 的 FileSystem API 手动删除输出目录。 对于通过 Spark Streaming 的 StreamingContext 生成的作业会忽略此设置，因为在检查点恢复期间可能需要将数据重写到预先存在的输出目录。 spark.storage.memoryMapThreshold 2m 当从磁盘读取块时，Spark 内存映射的块大小。 这会阻止 Spark 从内存映射过小的块。 通常，存储器映射对于接近或小于操作系统的页大小的块具有高开销。 网络（Networking） Property Name Default Meaning spark.rpc.message.maxSize 128 在 “control plane” 通信中允许的最大消息大小（以 MB 为单位）; 一般只适用于在 executors 和 driver 之间发送的映射输出大小信息。 如果您正在运行带有数千个 map 和 reduce 任务的作业，并查看有关 RPC 消息大小的消息，请增加此值。 spark.blockManager.port (random) 所有块管理器监听的端口。 这些都存在于 driver 和 executors 上。 spark.driver.host (local hostname) 要监听的 driver 的主机名或 IP 地址。 这用于与 executors 和 standalone Master 进行通信。 spark.driver.port (random) 要监听的 driver 的端口。这用于与 executors 和 standalone Master 进行通信。 spark.network.timeout 120s 所有网络交互的默认超时。 如果未配置此项，将使用此配置替换 spark.core.connection.ack.wait.timeout，spark.storage.blockManagerSlaveTimeoutMs，spark.shuffle.io.connectionTimeout，spark.rpc.askTimeout 或 spark.rpc.lookupTimeout 。 spark.port.maxRetries 16 在绑定端口放弃之前的最大重试次数。 当端口被赋予特定值（非 0）时，每次后续重试将在重试之前将先前尝试中使用的端口增加 1。 这本质上允许它尝试从指定的开始端口到端口 + maxRetries 的一系列端口。 spark.rpc.numRetries 3 在 RPC 任务放弃之前重试的次数。 RPC 任务将在此数字的大多数时间运行。 spark.rpc.retry.wait 3s RPC 请求操作在重试之前等待的持续时间。 spark.rpc.askTimeout 120s RPC 请求操作在超时前等待的持续时间。 spark.rpc.lookupTimeout 120s RPC 远程端点查找操作在超时之前等待的持续时间。 调度（Scheduling） Property Name Default Meaning spark.cores.max (not set) 当以 “coarse-grained” 共享模式在 standalone deploy 集群 或 Mesos 集群上运行 时，从集群（而不是每台计算机）请求应用程序的最大 CPU 内核数量。 如果未设置，默认值将是 Spark 的 standalone deploy 管理器上的 spark.deploy.defaultCores，或者 Mesos 上的无限（所有可用核心）。 spark.locality.wait 3s 等待启动本地数据任务多长时间，然后在较少本地节点上放弃并启动它。 相同的等待将用于跨越多个地点级别（process-local, node-local, rack-local ，等所有）。 也可以通过设置 spark.locality.wait.node 等来自定义每个级别的等待时间。如果任务很长并且局部性较差，则应该增加此设置，但是默认值通常很好。 spark.locality.wait.node spark.locality.wait 自定义 node locality 等待时间。 例如，您可以将其设置为 0 以跳过 node locality，并立即搜索机架位置（如果群集具有机架信息）。 spark.locality.wait.process spark.locality.wait 自定义 process locality 等待时间。这会影响尝试访问特定执行程序进程中的缓存数据的任务。 spark.locality.wait.rack spark.locality.wait 自定义 rack locality 等待时间。 spark.scheduler.maxRegisteredResourcesWaitingTime 30s 在调度开始之前等待资源注册的最大时间量。 spark.scheduler.minRegisteredResourcesRatio 0.8 for YARN mode; 0.0 for standalone mode and Mesos coarse-grained mode 注册资源（注册资源 / 总预期资源）的最小比率（资源是 yarn 模式下的执行程序，standalone 模式下的 CPU 核心和 Mesos coarsed-grained 模式 [‘spark.cores.max’值是 Mesos coarse-grained 模式下的总体预期资源]）在调度开始之前等待。 指定为 0.0 和 1.0 之间的双精度。 无论是否已达到资源的最小比率，在调度开始之前将等待的最大时间量由 config spark.scheduler.maxRegisteredResourcesWaitingTime 控制。 spark.scheduler.mode FIFO 作业之间的 调度模式 提交到同一个 SparkContext。 可以设置为 FAIR 使用公平共享，而不是一个接一个排队作业。 对多用户服务有用。 spark.scheduler.revive.interval 1s 调度程序复活工作资源去运行任务的间隔长度。 spark.speculation false 如果设置为 “true”，则执行任务的推测执行。 这意味着如果一个或多个任务在一个阶段中运行缓慢，则将重新启动它们。 spark.speculation.interval 100ms Spark 检查要推测的任务的时间间隔。 spark.speculation.multiplier 1.5 一个任务的速度可以比推测的平均值慢多少倍。 spark.speculation.quantile 0.75 对特定阶段启用推测之前必须完成的任务的分数。 spark.task.cpus 1 要为每个任务分配的核心数。 spark.task.maxFailures 4 放弃作业之前任何特定任务的失败次数。 分散在不同任务中的故障总数不会导致作业失败; 一个特定的任务允许失败这个次数。 应大于或等于 1. 允许重试次数 = 此值 - 1。 动态分配（Dynamic Allocation） Property Name Default Meaning spark.dynamicAllocation.enabled false 是否使用动态资源分配，它根据工作负载调整为此应用程序注册的执行程序数量。 有关更多详细信息，请参阅 此处 的说明。这需要设置 spark.shuffle.service.enabled。 以下配置也相关：spark.dynamicAllocation.minExecutors，spark.dynamicAllocation.maxExecutors 和 spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.executorIdleTimeout 60s 如果启用动态分配，并且执行程序已空闲超过此持续时间，则将删除执行程序。 有关更多详细信息，请参阅此 描述。 spark.dynamicAllocation.cachedExecutorIdleTimeout infinity 如果启用动态分配，并且已缓存数据块的执行程序已空闲超过此持续时间，则将删除执行程序。 有关详细信息，请参阅此 描述。 spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.minExecutors 启用动态分配时要运行的执行程序的初始数。如果 --num-executors（或 spark.executor.instances）被设置并大于此值，它将被用作初始执行器数。 spark.dynamicAllocation.maxExecutors infinity 启用动态分配的执行程序数量的上限。 spark.dynamicAllocation.minExecutors 0 启用动态分配的执行程序数量的下限。 spark.dynamicAllocation.schedulerBacklogTimeout 1s 如果启用动态分配，并且有超过此持续时间的挂起任务积压，则将请求新的执行者。 有关更多详细信息，请参阅此 描述。 spark.dynamicAllocation.sustainedSchedulerBacklogTimeout schedulerBacklogTimeout 与 spark.dynamicAllocation.schedulerBacklogTimeout 相同，但仅用于后续执行者请求。 有关更多详细信息，请参阅此 描述。 安全性（Security） Property Name Default Meaning spark.acls.enable false 是否开启 Spark acls。如果开启了，它检查用户是否有权限去查看或修改 job。 Note this requires the user to be known, so if the user comes across as null no checks are done。UI 利用使用过滤器验证和设置用户 spark.admin.acls Empty 逗号分隔的用户或者管理员列表，列表中的用户或管理员有查看和修改所有 Spark job 的权限。如果你运行在一个共享集群，有一组管理员或开发者帮助 debug，这个选项有用 spark.admin.acls.groups Empty 逗号分隔的用户或者管理员列表，列表中的用户或管理员有查看和修改所有 Spark job 的权限。如果你运行在一个共享集群，有一组管理员或开发者帮助 debug，这个是针对组管理员组用户 设置 spark.user.groups.mapping 和 spark.user.groups.mapping spark.user.groups.mapping org.apache.spark.security.ShellBasedGroupsMappingProvider 用户组列表是由一组地图服务定义的特征 org.apache.spark.security。 GroupMappingServiceProvider 可以由这个属性配置。 提供一个默认的基于 unix shell 实现 org.apache.spark.security.ShellBasedGroupsMappingProvider 可以指定解决用户组列表。 注意: 这个实现只支持基于 Unix / Linux 环境。 Windows 环境 目前不支持。 然而, 一个新的平台 / 协议可以支持的实现 的特征 org.apache.spark.security.GroupMappingServiceProvider spark.authenticate false 是否 Spark 验证其内部连接。如果不是运行在 YARN 上，请看 spark.authenticate.secret spark.authenticate.secret None 设置密钥用于 spark 组件之间进行身份验证。 这需要设置 不启用运行在 yarn 和身份验证。 spark.authenticate.enableSaslEncryption false 身份验证时启用加密通信。 这是块传输服务和支持 RPC 的端点。 spark.network.sasl.serverAlwaysEncrypt false 禁用未加密的连接服务, 支持 SASL 验证。 这是目前支持的外部转移服务。 spark.core.connection.ack.wait.timeout 60s 连接等待回答的时间。单位为秒。为了避免不希望的超时，你可以设置更大的值 spark.core.connection.auth.wait.timeout 30s 连接时等待验证的实际。单位为秒 spark.modify.acls Empty 逗号分隔的用户列表，列表中的用户有查看 (view)Spark web UI 的权限。默认情况下，只有启动 Spark job 的用户有查看权限 spark.modify.acls.groups Empty 针对 spark.modify.acls 设置组权限 spark.ui.filters None 应用到 Spark web UI 的用于过滤类名的逗号分隔的列表。过滤器必须是标准的 javax servlet Filter。通过设置 java 系统属性也可以指定每个过滤器的参数。spark..params=’param1=value1,param2=value2’。例如 - Dspark.ui.filters=com.test.filter1、-Dspark.com.test.filter1.params=’param1=foo,param2=testing’ spark.ui.view.acls Empty 逗号分隔的用户列表，列表中的用户有查看 (view)Spark web UI 的权限。默认情况下，只有启动 Spark job 的用户有查看权限 spark.ui.view.acls.groups Empty 针对 spark.ui.view.acls 设置组权限 加密（Encryption） Property Name Default Meaning spark.ssl.enabled | false | 所有支持的协议是否启用 SSL 连接；所有的 SSL 设置 spark.ssl.xxx 在哪里 xxx 是一个 特定的配置属性, 表示全局所有支持的配置协议。 为了覆盖全局配置特定的协议, 属性必须被覆盖特定于协议的名称空间；使用 spark.ssl.YYY.XXX 覆盖全局配置设置特定的协议用。 示例值多包括 fs , 用户界面 , standlone , historyServer。查看 SSL 配置 为服务细节层次 SSL 配置 || spark.ssl.enabledAlgorithms | Empty | 密码的逗号分隔列表。 指定的密码必须由 JVM 支持。 协议的参考列表可以查看这个 页面。 注意: 如果没有设置, 它将使用 JVM 的缺省密码 || spark.ssl.keyPassword | None | 钥的密钥存储库的密码。 || spark.ssl.keyStore | None | 密钥存储库文件的路径。 可以绝对或相对路径的目录 组件开始。 || spark.ssl.keyStorePassword | None | 密钥存储密码 || spark.ssl.keyStoreType | JKS | 密钥存储库的类型 || spark.ssl.protocol | None | 协议名称。 协议必须由 JVM 支持。 协议的参考列表 一个可以找到这 页面 || spark.ssl.needClientAuth | false | 设置真实如果 SSL 客户机身份验证需求。 || spark.ssl.trustStore | None | 信任存储文件的路径。 可以绝对或相对路径的目录 组件是开始的地方 || spark.ssl.trustStorePassword | None | 信任存储密码 || spark.ssl.trustStoreType | JKS | 信任存储库的类型 | Spark SQL运行 SET -v 命令将显示 SQL 配置的完整列表。 12// spark is an existing SparkSession # spark 是一个现有的 SparkSession，可以用如下语法进行设置spark.sql("SET -v").show(numRows = 200, truncate = false) Spark Streaming Property Name Default Meaning spark.streaming.backpressure.enabled false 开启或关闭 Spark Streaming 内部的 backpressure mecheanism（自 1.5 开始）。基于当前批次调度延迟和处理时间，这使得 Spark Streaming 能够控制数据的接收率，因此，系统接收数据的速度会和系统处理的速度一样快。从内部来说，这动态地设置了系统的最大接收率。这个速率上限通过 spark.streaming.receiver.maxRate 和 spark.streaming.kafka.maxRatePerPartition 两个参数设定（如下）。 spark.streaming.backpressure.initialRat not set 当 backpressure mecheanism 开启时，每个 receiver 接受数据的初始最大值。 spark.streaming.blockInterval 200ms 在这个时间间隔（ms）内，通过 Spark Streaming receivers 接收的数据在保存到 Spark 之前，chunk 为数据块。推荐的最小值为 50ms。具体细节见 Spark Streaming 指南的 performance tuning 一节。 spark.streaming.receiver.maxRate not set 每秒钟每个 receiver 将接收的数据的最大速率（每秒钟的记录数目）。有效的情况下，每个流每秒将最多消耗这个数目的记录。设置这个配置为 0 或者 - 1 将会不作限制。细节参见 Spark Streaming 编程指南的 deployment guide 一节。 spark.streaming.receiver.writeAheadLog.enable false 为 receiver 启用 write ahead logs。所有通过接收器接收输入的数据将被保存到 write ahead logs，以便它在驱动程序故障后进行恢复。见星火流编程指南部署指南了解更多详情。细节参见 Spark Streaming 编程指南的 deployment guide 一节。 spark.streaming.unpersist true 强制通过 Spark Streaming 生成并持久化的 RDD 自动从 Spark 内存中非持久化。通过 Spark Streaming 接收的原始输入数据也将清除。设置这个属性为 false 允许流应用程序访问原始数据和持久化 RDD，因为它们没有被自动清除。但是它会造成更高的内存花费 spark.streaming.stopGracefullyOnShutdown false 如果为 true，Spark 将缓慢地 (gracefully) 关闭在 JVM 运行的 StreamingContext ，而非立即执行。 spark.streaming.kafka.maxRatePerPartition not set 在使用新的 Kafka direct stream API 时，从每个 kafka 分区读到的最大速率（每秒的记录数目）。详见 Kafka Integration guide。 spark.streaming.kafka.maxRetries 1 driver 连续重试的最大次数，以此找到每个分区 leader 的上次 (latest) 的偏移量（默认为 1 以意味着 driver 将尝试最多两次）。仅应用于新的 kafka direct stream API。 spark.streaming.ui.retainedBatches 1000 在垃圾回收之前，Spark Streaming UI 和状态 API 所能记得的 批处理 (batches) 数量。 spark.streaming.driver.writeAheadLog.closeFileAfterWrite false 在写入一条 driver 中的 write ahead log 记录 之后，是否关闭文件。如果你想为 driver 中的元数据 WAL 使用 S3（或者任何文件系统而不支持 flushing），设定为 true。 spark.streaming.receiver.writeAheadLog.closeFileAfterWrite false 在写入一条 reveivers 中的 write ahead log 记录 之后，是否关闭文件。如果你想为 reveivers 中的元数据 WAL 使用 S3（或者任何文件系统而不支持 flushing），设定为 true。 SparkR Property Name Default Meaning spark.r.numRBackendThreads 2 使用 RBackend 处理来自 SparkR 包中的 RPC 调用的线程数。 spark.r.command Rscript 在 driver 和 worker 两种集群模式下可执行的 R 脚本。 spark.r.driver.command spark.r.command 在 driver 的 client 模式下可执行的 R 脚本。在集群模式下被忽略。 部署 Property Name Default Meaning spark.deploy.recoveryMode NONE 集群模式下，Spark jobs 执行失败或者重启时，恢复提交 Spark jobs 的恢复模式设定。 spark.deploy.zookeeper.url None 当 spark.deploy.recoveryMode 被设定为 ZOOKEEPER，这一配置被用来连接 zookeeper URL。 spark.deploy.zookeeper.dir None 当 spark.deploy.recoveryMode 被设定为 ZOOKEEPER，这一配置被用来设定 zookeeper 目录为 store recovery state Cluster Manager（集群管理器）Spark 中的每个集群管理器都有额外的配置选项，这些配置可以在每个模式的页面中找到。 YARN Mesos Standalone Mode 环境变量通过环境变量配置特定的 Spark 设置。环境变量从 Spark 安装目录下的 conf/spark-env.sh 脚本读取（或者是 window 环境下的 conf/spark-env.cmd）。在 Standalone 或 Mesos 模式下，这个文件可以指定机器的特定信息，比如主机名。它也可以为正在运行的 Spark Application 或者提交脚本提供来源。 注意，当 Spark 被安装，默认情况下 conf/spark-env.sh 是不存在的。但是，你可以通过拷贝 conf/spark-env.sh.template 来创建它。确保你的拷贝文件时可执行的。 spark-env.sh: 中有有以下变量可以被设置： Environment Variable Meaning JAVA_HOME Java 的安装路径（如果不在你的默认路径下） PYSPARK_PYTHON 在 driver 和 worker 中 PySpark 用到的 Python 二进制可执行文件（如何有默认为 Python2.7，否则为 python） PYSPARK_DRIVER_PYTHON 只在 driver 中 PySpark 用到的 Python 二进制可执行文件（默认为 PySpark_Python） PYSPARK_DRIVER_R SparkR shell 用到的 R 二进制可执行文件（默认为 R） SPARK_DRIVER_IP 机器绑定的 IP 地址 SPARK_PUBLIC_DNS 你的 Spark 程序通知其他机器的主机名 除了以上参数，standalone cluster scripts 也可以设置其他选项，比如每个机器使用的 CPU 核数和最大内存。因为 spark-env.sh 是 shell 脚本，一些可以通过程序的方式来设置，比如你可以通过特定的网络接口来计算 SPARK_LOCAL_IP。 注意：当以集群模式运行 Spark on YARN 时，环境变量需要通过 spark.yarn.appMasterEnv 来设定。在你的 conf/spark-defaults.conf 文件中的 [EnvironmentVariableName] 属性。集群模式下，spark-env.sh 中设定的环境变量将不会在 YARN Application Master 过程中反应出来。详见 YARN-related Spark PropertiesProperties。 配置 Logging（日志）Spark 用 log4j 生成日志，你可以通过在 conf 目录下添加 log4j.properties 文件来配置。一种方法时拷贝 log4j.properties.template 文件。 覆盖配置目录如果你想指定不同的配置目录，而不是默认的 SPARK_HOME/conf，你可以设置 SPARK_CONF_DIR。Spark 将从这一目录下读取（spark-defaults.conf, spark-env.sh, log4j.properties, 等） 继承 Hadoop 集群配置如果你想用 Spark 来读写 HDFS，在 Spark 的 classpath 就需要包括两个 Hadoop 配置文件。 hdfs-site.xml，为 HDFS client 提供默认的行为。 core-site.xml，设定默认的文件系统名称。 这两个配置文件的位置视 CDH 和 HDP 两个版本而不同，不过一般来说在 /etc/hadoop/conf 下。一些工具，如 Cloudera Manager，创建即时 (one-the-fly) 的配置文件，但提供一个机制来下载它们的副本。 为了使这些文件对 Spark 可见，需要设定 $SPARK_HOME/spark-env.sh 中的 HADOOP_CONF_DIR 到一个包含配置文件的位置。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 作业调度]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-job-scheduling%2F</url>
      <content type="text"><![CDATA[概述Spark 有好几种计算资源调度的方式。首先，回忆一下 集群模式概述 中每个 Spark 应用（包含一个 SparkContext 实例）中运行了一些其独占的执行器（executor）进程。集群管理器提供了 Spark 应用之间的资源调度（scheduling across applications）。其次，在各个Spark 应用内部，各个线程可能并发地通过 action 算子提交多个 Spark 作业（job）。如果你的应用服务于网络请求，那这种情况是很常见的。在 Spark 应用内部（对应同一个 SparkContext）各个作业之间，Spark 默认 FIFO 调度，同时也可以支持公平调度（fair scheduler）。 跨应用调度如果在集群上运行，每个 Spark 应用都会 SparkContext 获得一批独占的执行器 JVM，来运行其任务并存储数据。如果有多个用户共享集群，那么会有很多资源分配相关的选项，如何设置还取决于具体的集群管理器。 对 Spark 所支持的各个集群管理器而言，最简单的资源分配，就是对资源静态划分。这种方式就意味着，每个 Spark 应用都是设定一个最大可用资源总量，并且该应用在整个生命周期内都会占住这些资源。这种方式在 Spark 独立部署 和 YARN 调度，以及 Mesos 粗粒度模式（coarse-grained Mesos mode）下都可用。 Standalone mode: 默认情况下，Spark 应用在独立部署的集群中都会以 FIFO（first-in-first-out）模式顺序提交运行，并且每个 Spark 应用都会占用集群中所有可用节点。不过你可以通过设置 spark.cores.max 或者 spark.deploy.defaultCores 来限制单个应用所占用的节点个数。最后，除了可以控制对 CPU 的使用数量之外，还可以通过 spark.executor.memory 来控制各个应用的内存占用量。 Mesos: 在 Mesos 中要使用静态划分的话，需要将 spark.mesos.coarse 设为 true，同样，你也需要设置 spark.cores.max 来控制各个应用的 CPU 总数，以及 spark.executor.memory 来控制各个应用的内存占用。 YARN: 在 YARN 中需要使用 –num-executors 选项来控制 Spark 应用在集群中分配的执行器的个数，对于单个执行器（executor）所占用的资源，可以使用 –executor-memory 和 –executor-cores 来控制，详见YARN Spark Properties。 Mesos 上还有一种动态共享 CPU 的方式。在这种模式下，每个 Spark 应用的内存占用仍然是固定且独占的（仍由 spark.executor.memory 决定），但是如果该 Spark 应用没有在某个机器上执行任务的话，那么其他应用可以占用该机器上的 CPU。这种模式对集群中有大量不是很活跃应用的场景非常有效，例如: 集群中有很多不同用户的 Spark shell session。但这种模式不适用于低延迟的场景，因为当 Spark 应用需要使用 CPU 的时候，可能需要等待一段时间才能取得 CPU 的使用权。要使用这种模式，只需要在 mesos:// URL 上设置 spark.mesos.coarse 属性为 false 即可。 值得注意的是，目前还没有任何一种资源分配模式能支持跨 Spark 应用的内存共享。如果你想通过这种方式共享数据，我们建议你可以单独使用一个服务（例如 alluxio），这样就能实现多应用访问同一个 RDD 的数据。 动态资源分配Spark 提供了一种基于负载来动态调节 Spark 应用资源占用的机制。这意味着，你的应用会在资源空闲的时候将其释放给集群，需要时再重新申请。这一特性在多个应用共享 Spark 集群资源的情况下特别有用。 这个特性默认是禁用的，但是在所有的粗粒度集群管理器上都是可用的，如: 独立部署模式，YARN 模式以及 Mesos 粗粒度模式（Mesos coarse-grained mode）。 配置和部署要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动 external shuffle service，并在你的应用中将 spark.shuffle.service.enabled 设为 true。external shuffle service 的目的是在移除 executor 的时候，能够保留 executor 输出的 shuffle 文件（本文 后续有更详细的描述）。启用 external shuffle service 的方式在各个集群管理器上各不相同 : 在 Spark 独立部署的集群中，你只需要在 worker 启动前设置 spark.shuffle.server.enabled 为 true 即可。 在 Mesos 粗粒度模式下，你需要在各个节点上运行 ${SPARK_HOME}/sbin/start-mesos-shuffle-service.sh 并设置 spark.shuffle.service.enabled 为 true 即可。例如，你可以用 Marathon 来启用这一功能。 在 YARN 模式下，需要按以下 步骤 在各个 NodeManager 上启动 : 首先按照YARN profile 构建 Spark。如果你已经有打好包的 Spark，可以忽略这一步。 找到 spark-\-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-\，否则应该可以在 lib 目录下找到这个 jar 包。 将该 jar 包添加到 NodeManager 的 classpath 路径中。 配置各个节点上的 yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。 最后重启各节点上的 NodeManager。 所有相关的配置都是可选的，并且都在 spark.dynamicAllocation. 和 spark.shuffle.service. 命名空间下。更详细请参考: 配置页面。 资源分配策略总体上来说，Spark 应该在执行器空闲时将其关闭，而在后续要用时再次申请。因为没有一个固定的方法，可以预测一个执行器在后续是否马上回被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。 请求策略一个启用了动态分配的 Spark 应用会在有等待任务需要调度的时候，申请额外的执行器。这种情况下，必定意味着已有的执行器已经不足以同时执行所有未完成的任务。 Spark 会分轮次来申请执行器。实际的资源申请，会在任务挂起 spark.dynamicAllocation.schedulerBacklogTimeout 秒后首次触发，其后如果等待队列中仍有挂起的任务，则每过 spark.dynamicAlloction.sustainedSchedulerBacklogTimeout 秒后触发一次资源申请。另外，每一轮所申请的执行器个数以指数形式增长。例如，一个 Spark 应用可能在首轮申请1个执行器，后续的轮次申请个数可能是 2个、4个、8个… … 。 采用指数级增长策略的原因有两个: 第一，对于任何一个 Spark 应用如果只是需要多申请少数几个执行器的话，那么必须非常谨慎地启动资源申请，这和 TCP 慢启动有些类似；第二，如果一旦 Spark 应用确实需要申请很多个执行器的话，那么可以确保其所需的计算资源及时地增长。 移除策略移除执行器的策略就简单多了。Spark 应用会在某个执行器空闲超过 spark.dynamicAllocation.executorIdleTimeout 秒后将其删除。在绝大多数情况下，执行器的移除条件和申请条件都是互斥的，也就是说，执行器在有待执行任务挂起时，不应该空闲。 优雅地关闭 Executor（执行器）非动态分配模式下，执行器可能的退出原因有执行失败或者相关 Spark 应用已经退出。不管是那种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在 Spark 应用运行期间被移除。这时候，如果 Spark 应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark 需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。 这种需求对于 Shuffle（混洗） 操作尤其重要。在 Shuffle 过程中，Spark 执行器首先将 map 输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的 map 结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。 要解决这一问题，就需要用到 external shuffle service ，该服务在 Spark 1.2 引入。该服务在每个节点上都会启动一个不依赖于任何 Spark 应用或执行器的独立进程。一旦该服务启用，Spark 执行器不再从各个执行器上获取 shuffle 文件，转而从这个 service 获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。 应用内调度在指定的 Spark 应用内部（对应同一个 SparkContext 实例），多个线程可能并发地提交 Spark 作业（job）。在本节中，作业（job）是指，由 Spark action 算子（如: collect）触发的一系列计算任务的集合。Spark 调度器是完全线程安全的，并且能够支持 Spark 应用同时处理多个请求（比如: 来自不同用户的查询）。 默认，Spark 应用内部使用 FIFO 调度策略。每个作业被划分为多个阶段（stage）（例如: map 阶段和 reduce 阶段），第一个作业在其启动后会优先获取所有的可用资源，然后是第二个作业再申请，再第三个……。如果前面的作业没有把集群资源占满，则后续的作业可以立即启动运行，否则，后提交的作业会有明显的延迟等待。 不过从 Spark 0.8 开始，Spark 也能支持各个作业间的公平（Fair）调度。公平调度时，Spark 以轮询的方式给每个作业分配资源，因此所有的作业获得的资源大体上是平均分配。这意味着，即使有大作业在运行，小的作业再提交也能立即获得计算资源而不是等待前面的作业结束，大大减少了延迟时间。这种模式特别适合于多用户配置。 要启用公平调度器，只需设置一下 SparkContext 中 spark.scheduler.mode 属性为 FAIR 即可 : 123val conf = new SparkConf().setMaster(...).setAppName(...)conf.set("spark.scheduler.mode", "FAIR")val sc = new SparkContext(conf) 公平调度资源池公平调度器还可以支持将作业分组放入资源池（pool），然后给每个资源池配置不同的选项（如: 权重）。这样你就可以给一些比较重要的作业创建一个“高优先级”资源池，或者你也可以把每个用户的作业分到一组，这样一来就是各个用户平均分享集群资源，而不是各个作业平分集群资源。Spark 公平调度的实现方式基本都是模仿 Hadoop Fair Scheduler 来实现的。 默认情况下，新提交的作业都会进入到默认资源池中，不过作业对应于哪个资源池，可以在提交作业的线程中用 SparkContext.setLocalProperty 设定 spark.scheduler.pool 属性。示例代码如下 : 12// 假设 sc 是您的 SparkContext 变量sc.setLocalProperty("spark.scheduler.pool", "pool1") 一旦设好了局部属性，所有该线程所提交的作业（即: 在该线程中调用 action 算子，如: RDD.save, count, collect 等）都会使用这个资源池。这个设置是以线程为单位保存的，你很容易实现用同一线程来提交同一用户的所有作业到同一个资源池中。同样，如果需要清除资源池设置，只需在对应线程中调用如下代码 : 1sc.setLocalProperty("spark.scheduler.pool", null) 资源池默认行为默认地，各个资源池之间平分整个集群的资源（包括 default 资源池），但在资源池内部，默认情况下，作业是 FIFO 顺序执行的。举例来说，如果你为每个用户创建了一个资源池，那么久意味着各个用户之间共享整个集群的资源，但每个用户自己提交的作业是按顺序执行的，而不会出现后提交的作业抢占前面作业的资源。 配置资源池属性 资源池的属性需要通过配置文件来指定。每个资源池都支持以下3个属性 : schedulingMode: 可以是 FIFO 或 FAIR，控制资源池内部的作业是如何调度的。 weight: 控制资源池相对其他资源池，可以分配到资源的比例。默认所有资源池的 weight 都是 1。如果你将某个资源池的 weight 设为 2，那么该资源池中的资源将是其他池子的2倍。如果将 weight 设得很高，如 1000，可以实现资源池之间的调度优先级 – 也就是说，weight=1000 的资源池总能立即启动其对应的作业。 minShare: 除了整体 weight 之外，每个资源池还能指定一个最小资源分配值（CPU 个数），管理员可能会需要这个设置。公平调度器总是会尝试优先满足所有活跃（active）资源池的最小资源分配值，然后再根据各个池子的 weight 来分配剩下的资源。因此，minShare 属性能够确保每个资源池都能至少获得一定量的集群资源。minShare 的默认值是 0。 资源池属性是一个 XML 文件，可以基于 conf/fairscheduler.xml.template 修改，然后在 SparkConf 的 spark.scheduler.allocation.file 属性指定文件路径： 1conf.set("spark.scheduler.allocation.file", "/path/to/file") 资源池 XML 配置文件格式如下，其中每个池子对应一个 元素，每个资源池可以有其独立的配置 : 12345678910111213&lt;?xml version="1.0"?&gt;&lt;allocations&gt; &lt;pool name="production"&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;2&lt;/minShare&gt; &lt;/pool&gt; &lt;pool name="test"&gt; &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt;&lt;/allocations&gt; 完整的例子可以参考 conf/fairscheduler.xml.template。注意，没有在配置文件中配置的资源池都会使用默认配置（schedulingMode : FIFO，weight : 1，minShare : 0）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 硬件配置]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-hardware-provisioning%2F</url>
      <content type="text"><![CDATA[Spark 开发者都会遇到一个常见问题，那就是如何为 Spark 配置硬件。然而正确的硬件配置取决于使用的场景，我们提出以下建议。 存储系统因为大多数 Spark 作业都很可能必须从外部存储系统（例如 Hadoop 文件系统或者 HBase）读取输入的数据，所以部署 Spark 时尽可能靠近这些系统是很重要的。我们建议如下 : 如果可以，在 HDFS 相同的节点上运行 Spark 。最简单的方法是在相同节点上设置 Spark standalone 集群模式，并且配置 Spark 和 Hadoop 的内存和 CPU 的使用以避免干扰（Hadoop 的相关选项为 : 设置每个任务内存大小的选项 mapred.child.java.opts 以及设置任务数量的选项 mapred.tasktracker.map.tasks.maximum 和 mapred.tasktracker.reduce.tasks.maximum）。当然您也可以在常用的集群管理器（比如 Mesos 或者 YARN）上运行 Hadoop 和 Spark。 如果不可以在相同的节点上运行，建议在与 HDFS 相同的局域网中的不同节点上运行 Spark 。 对于像 HBase 这样的低延时数据存储系统，在与这些存储系统不同的节点上运行计算作业来可能更有利于避免干扰。 本地磁盘虽然 Spark 可以在内存中执行大量计算，但是他仍然使用本地磁盘来存储不适合内存存储的数据以及各个阶段的中间结果。我们建议每个节点配置 4-8 个磁盘，并且不使用 RAID 配置 (只作为单独挂载点)。在 Linux 中, 使用 noatime 选项 挂载磁盘以减少不必要的写入。在 Spark 中，配置 spark.local.dir 变量为逗号分隔的本地磁盘列表。如果您正在运行 HDFS，可以使用与 HDFS 相同的磁盘。 内存一般来说，Spark 可以在每台机器 8GB 到数百 GB 内存的任何地方正常运行。在所有情况下，我们建议只为 Spark 分配最多 75% 的内存；其余部分供操作系统和缓存区高速缓存存储器使用。 您需要多少内存取决于您的应用程序。如果您需要确定的应用程序中某个特定数据集占用内存的大小，您可以把这个数据集加载到一个 Spark RDD 中，然后在 Spark 监控 UI 页面（http://\:4040）中的 Storage 选项卡下查看它在内存中的大小。需要注意的是，存储级别和序列化格式对内存使用量有很大的影响 - 如何减少内存使用量的建议，请参阅 优化指南。 最后, 需要注意的是 Java 虚拟机在超过 200GB 的 RAM 时表现得并不好。如果您购置的机器有比这更多的 RAM ，您可以在每个节点上运行多个 Worker 的 JVM 实例。在 Spark standalone 模式 下, 您可以通过 conf/spark-env.sh 中的 SPARK_WORKER_INSTANCES 和 SPARK_WORKER_CORES 两个参数来分别设置每个节点的 Worker 数量和每个 Worker 使用的 Core 数量。 网络根据我们的经验，当数据在内存中时，很多 Spark 应用程序跟网络有密切的关系。使用 10 千兆位以太网或者更快的网络是让这些应用程序变快的最佳方式。这对于 “distributed reduce” 类的应用程序来说尤其如此，例如 group-by 、reduce-by 和 SQL join。任何程序都可以在应用程序监控 UI 页面（http://\:4040）中查看 Spark 通过网络传输的数据量。 CPU Core 数量因为 Spark 实行线程之间的最小共享，所以 Spark 可以很好地在每台机器上扩展数十个 CPU Core。您应该为每台机器至少配置 8-16 个 Core。根据您工作负载的 CPU 成本，您可能还需要更多 : 当数据都在内存中时，大多数应用程序就只跟 CPU 或者网络有关了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark SQL]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-sql-programming-guide%2F</url>
      <content type="text"><![CDATA[Spark SQL 概述 (DataFrames, Datasets 和 SQL)Spark SQL 是一个用户结构化数据处理的 Spark 模块。不像基础的 Spark RDD API 一样，Spark SQL 提供的接口提供了 Spark 与构造数据和执行计算之间的更多信息。在内部，Spark SQL 使用这个额外的信息去执行额外的优化。有几种方式可以跟 Spark SQL 进行交互，包括 SQL 和 Dataset API。当使用相同执行引擎进行计算时，无论使用哪种 API / 语言，都可以快速的计算。这种统一意味着开发人员能够在基于提供最自然的方式来表达一个给定的 transformation API 之间实现轻松的来回切换不同的 。 该页面所有例子使用的示例数据都包含在 Spark 的发布中，并且可以使用 spark-shell，pyspark，或者 sparkR 来运行。 SQLSpark SQL 的功能之一是执行 SQL 查询。Spark SQL 也能够被用于从已存在的 Hive 环境中读取数据。更多关于如何配置这个特性的信息，请参考 Hive 表 这部分。当以另外的编程语言运行 SQL 时，查询结果将以 Dataset/DataFrame 的形式返回。您也可以使用 命令行 或者通过 JDBC/ODBC 与 SQL 接口交互。 Datasets 和 DataFrames一个 Dataset 是一个分布式的数据集合。Dataset 是在 Spark 1.6 中被添加的新接口，它提供了 RDD 的好处（强类型化, 能够使用强大的 lambda 函数）与 Spark SQL 优化的执行引擎的好处。一个 Dataset 可以从 JVM 对象来 构造 并且使用转换功能（map，flatMap，filter，等等）。Dataset API 在 Scala 和 Java 中是可用的。Python 不支持 Dataset API。但是由于 Python 的动态特性，许多 Dataset API 的有点已经可用了（也就是说，你可能通过 name 天生的 row.columnName 属性访问一行中的字段）。这种情况和 R 相似。 一个 DataFrame 是一个 Dataset 组织成的指定列。它的概念与一个在关系型数据库或者在 R/Python 中的表是相等的，但是有更多的优化。DataFrame 可以从大量的 Source 中构造出来，像 : 结构化的数据文件，Hive 中的表，外部的数据库，或者已存在的 RDD。DataFrame API 在 Scala，Java，Python 和 R 中是可用的。在 Scala 和 Java 中，一个 DataFrame 所代表的是一个 Dataset 的 Row（行）。在 Scala API 中，DataFrame 仅仅是一个 Dataset[Row] 类型的别名 。然而，在 Java API 中，用户需要去使用 Dataset 来表示 DataFrame。 在这个文档中，我们将常常会引用 Scala / Java 的 Dataset 的 Row（行）作为 DataFrame。 Spark SQL 入门指南起始点 : SparkSessionSpark 中所有功能的入口点是 SparkSession 类。去创建一个基本的 SparkSession，仅使用 SparkSession.builder() : Scala 123456789101112import org.apache.spark.sql.SparkSessionval spark = SparkSession .builder() .appName("Spark SQL Example") .config("spark.some.config.option", "some-value") .getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._// 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 Java 123456789import org.apache.spark.sql.SparkSession;SparkSession spark = SparkSession .builder() .appName("Java Spark SQL Example") .config("spark.some.config.option", "some-value") .getOrCreate();// 所有的示例代码可以在 Spark repo 的 "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" 中找到。 Python 123456789from pyspark.sql import SparkSessionspark = SparkSession\ .builder\ .appName("PythonSQL")\ .config("spark.some.config.option", "some-value")\ .getOrCreate()# 所有的示例代码可以在 Spark repo 的 "examples/src/main/python/sql.py" 中找到。 R 12345sparkR.session(appName = "MyApp", sparkConfig = list(spark.executor.memory = "1g"))// 所有的示例代码可以在 Spark repo 的 "examples/src/main/r/RSparkSQLExample.R" 中找到。// 注意 : 当第一次调用时，sparkR.session() 初始化了一个全局的 SparkSession 单例实例，并且为了连续的调用总是返回一个指向这个实力的引用。// 用这种方式，用户只需要去初始化 SparkSession 一次，然后 SparkR 函数可以像 read.df 一样可以隐式的访问这个全局实例，并且用户不需要通过 SparkSession 实例访问。 在 Spark 2.0 中 SparkSession 为 Hive 特性提供了内嵌的支持，包括使用 HiveQL 编写查询的能力，访问 Hive UDF，以及从 Hive 表中读取数据的能力。为了使用这些特性，你不需要去有一个已存在的 Hive 设置。 创建 DataFrame与一个 SparkSession 一起，应用程序可以从一个 已存在的 RDD，或者一个 Hive 表中，或者从 Spark 数据源 中创建 DataFrame。举个例子，下面基于一个 JSON 文件的内容创建一个 DataFrame : Scala 12345678910111213val df = spark.read.json("examples/src/main/resources/people.json")// Displays the content of the DataFrame to stdoutdf.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 Java 12345678910111213141516import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;Dataset&lt;Row&gt; df = spark.read().json("examples/src/main/resources/people.json");// Displays the content of the DataFrame to stdoutdf.show();// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" 中找到。 Python 12345# spark is an existing SparkSessiondf = spark.read.json("examples/src/main/resources/people.json")# Displays the content of the DataFrame to stdoutdf.show() R 123456789df &lt;- read.json("examples/src/main/resources/people.json")# Displays the content of the DataFramehead(df)# Another method to print the first few rows and optionally truncate the printing of long valuesshowDF(df)// 所有的示例代码可以在 Spark repo 的 "examples/src/main/r/RSparkSQLExample.R" 中找到。 无类型 Dataset 操作（aka DataFrame 操作）DataFrame 提供了一个 DSL（domain-specific language）用于在 Scala，Java，Python 或者 R 中的结构化数据操作。 正如上面提到的一样，Spark 2.0 中 DataFrame 在 Scala 和 JavaAPI 中仅仅 Dataset 的 RowS（行）。这些操作也参考了与强类型的 Scala/Java Datasets 的 “类型转换” 相对应的 “无类型转换”。 这里包括一些使用 Dataset 进行结构化数据处理的示例 : Scala 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// This import is needed to use the $-notationimport spark.implicits._// Print the schema in a tree formatdf.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Select only the "name" columndf.select("name").show()// +-------+// | name|// +-------+// |Michael|// | Andy|// | Justin|// +-------+// Select everybody, but increment the age by 1df.select($"name", $"age" + 1).show()// +-------+---------+// | name|(age + 1)|// +-------+---------+// |Michael| null|// | Andy| 31|// | Justin| 20|// +-------+---------+// Select people older than 21df.filter($"age"&gt; 21).show()// +---+----+// |age|name|// +---+----+// | 30|Andy|// +---+----+// Count people by agedf.groupBy("age").count().show()// +----+-----+// | age|count|// +----+-----+// | 19| 1|// |null| 1|// | 30| 1|// +----+-----+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// col("...") is preferable to df.col("...")import static org.apache.spark.sql.functions.col;// Print the schema in a tree formatdf.printSchema();// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Select only the "name" columndf.select("name").show();// +-------+// | name|// +-------+// |Michael|// | Andy|// | Justin|// +-------+// Select everybody, but increment the age by 1df.select(col("name"), col("age").plus(1)).show();// +-------+---------+// | name|(age + 1)|// +-------+---------+// |Michael| null|// | Andy| 31|// | Justin| 20|// +-------+---------+// Select people older than 21df.filter(col("age").gt(21)).show();// +---+----+// |age|name|// +---+----+// | 30|Andy|// +---+----+// Count people by agedf.groupBy("age").count().show();// +----+-----+// | age|count|// +----+-----+// | 19| 1|// |null| 1|// | 30| 1|// +----+-----+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" 中找到。 在 Python 中它既可以通过（df.age）属性又可以通过（df[‘age’]）下标去访问一个 DataFrame 的列。前者是方便交互数据探索，用户使用后者的形式，它在以后并且不会破坏 DataFrame class 上属性的列名。 12345678910111213141516171819202122232425262728293031323334353637383940414243# spark is an existing SparkSession# Create the DataFramedf = spark.read.json("examples/src/main/resources/people.json")# Show the content of the DataFramedf.show()## age name## null Michael## 30 Andy## 19 Justin# Print the schema in a tree formatdf.printSchema()## root## |-- age: long (nullable = true)## |-- name: string (nullable = true)# Select only the "name" columndf.select("name").show()## name## Michael## Andy## Justin# Select everybody, but increment the age by 1df.select(df['name'], df['age'] + 1).show()## name (age + 1)## Michael null## Andy 31## Justin 20# Select people older than 21df.filter(df['age'] &gt; 21).show()## age name## 30 Andy# Count people by agedf.groupBy("age").count().show()## age count## null 1## 19 1## 30 1 R 12345678910111213141516171819202122232425262728293031323334353637383940414243 # Create the DataFramedf &lt;- read.json("examples/src/main/resources/people.json")# Show the content of the DataFramehead(df)## age name## null Michael## 30 Andy## 19 Justin# Print the schema in a tree formatprintSchema(df)## root## |-- age: long (nullable = true)## |-- name: string (nullable = true)# Select only the "name" columnhead(select(df, "name"))## name## Michael## Andy## Justin# Select everybody, but increment the age by 1head(select(df, df$name, df$age + 1))## name (age + 1)## Michael null## Andy 31## Justin 20# Select people older than 21head(where(df, df$age&gt; 21))## age name## 30 Andy# Count people by agehead(count(groupBy(df, "age")))## age count## null 1## 19 1## 30 1# 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 能够在 DataFrame 上被执行的操作类型的完整列表请参考 API 文档。 除了简单的列引用和表达式之外，DataFrame 也有丰富的函数库，包括 string 操作，date 算术，常见的 math 操作以及更多。可用的完整列表请参考 DataFrame 函数参考。 以编程的方式运行 SQL 查询Scala 12345678910111213141516// SparkSession 使应用程序的 SQL 函数能够以编程的方式运行 SQL 查询并且将查询结果以一个 DataFrame。// Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView("people")val sqlDF = spark.sql("SELECT * FROM people")sqlDF.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 Java 12345678910111213141516171819// SparkSession 使应用程序的 SQL 函数能够以编程的方式运行 SQL 查询并且将查询结果以一个 Dataset&lt;Row&gt;。import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;// Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView("people");Dataset&lt;Row&gt; sqlDF = spark.sql("SELECT * FROM people");sqlDF.show();// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" 中找到。 Python 1234# SparkSession 使应用程序的 SQL 函数能够以编程的方式运行 SQL 查询并且将查询结果以一个 DataFrame。# spark is an existing SparkSessiondf = spark.sql("SELECT * FROM table") R 123df &lt;- sql("SELECT * FROM table")# 所有的示例代码可以在 Spark repo 的 "examples/src/main/r/RSparkSQLExample.R" 中找到。 创建 DatasetDataset 与 RDD 相似，然而，并不是使用 Java 序列化或者 Kryo，他们使用一个指定的 Encoder（编码器） 来序列化用于处理或者通过网络进行传输的对象。虽然编码器和标准的序列化都负责将一个对象序列化成字节，编码器是动态生成的代码，并且使用了一种允许 Spark 去执行许多像 filtering，sorting 以及 hashing 这样的操作，不需要将字节反序列化成对象的格式。 Scala 123456789101112131415161718192021222324252627282930// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,// you can use custom classes that implement the Product interfacecase class Person(name: String, age: Long)// Encoders are created for case classesval caseClassDS = Seq(Person("Andy", 32)).toDS()caseClassDS.show()// +----+---+// |name|age|// +----+---+// |Andy| 32|// +----+---+// Encoders for most common types are automatically provided by importing spark.implicits._val primitiveDS = Seq(1, 2, 3).toDS()primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by nameval path = "examples/src/main/resources/people.json"val peopleDS = spark.read.json(path).as[Person]peopleDS.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.util.Arrays;import java.util.Collections;import java.io.Serializable;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.Encoder;import org.apache.spark.sql.Encoders;public static class Person implements Serializable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125;// Create an instance of a Bean classPerson person = new Person();person.setName("Andy");person.setAge(32);// Encoders are created for Java beansEncoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);Dataset&lt;Person&gt; javaBeanDS = spark.createDataset( Collections.singletonList(person), personEncoder);javaBeanDS.show();// +---+----+// |age|name|// +---+----+// | 32|Andy|// +---+----+// Encoders for most common types are provided in class EncodersEncoder&lt;Integer&gt; integerEncoder = Encoders.INT();Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer call(Integer value) throws Exception &#123; return value + 1; &#125;&#125;, integerEncoder);transformedDS.collect(); // Returns [2, 3, 4]// DataFrames can be converted to a Dataset by providing a class. Mapping based on nameString path = "examples/src/main/resources/people.json";Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);peopleDS.show();// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" 中找到。 RDD 的互操作性Spark SQL 支持两种不同的方法用于转换已存在的 RDD 成为 Dataset。 第一种方法是使用反射去推断一个包含指定的对象类型的 RDD 的 Schema。在你的 Spark 应用程序中当你已知 Schema 时这个基于方法的反射可以让你的代码更简洁。 第二种用于创建 Dataset 的方法是通过一个允许你构造一个 Schema 然后把它应用到一个已存在的 RDD 的编程接口。然而这种方法更繁琐，当列和它们的类型知道运行时都是未知时它允许你去构造 Dataset。 使用反射推断 SchemaScala 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// Spark SQL 的 Scala 接口支持自动转换一个包含 case classes 的 RDD 为 DataFrame。 Case class 定义了表的 Schema。Case class 的参数名使用反射读取并且成为了列名。// Case class 也可以是嵌套的或者包含像 SeqS 或者 ArrayS 这样的复杂类型。这个 RDD 能够被隐式转换成一个 DataFrame 然后被注册为一个表。表可以用于后续的 SQL 语句。import org.apache.spark.sql.catalyst.encoders.ExpressionEncoderimport org.apache.spark.sql.Encoder// For implicit conversions from RDDs to DataFramesimport spark.implicits._// Create an RDD of Person objects from a text file, convert it to a Dataframeval peopleDF = spark.sparkContext .textFile("examples/src/main/resources/people.txt") .map(_.split(",")) .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)) .toDF()// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by Sparkval teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")// The columns of a row in the result can be accessed by field indexteenagersDF.map(teenager =&gt; "Name:" + teenager(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// or by field nameteenagersDF.map(teenager =&gt; "Name:" + teenager.getAs[String]("name")).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// No pre-defined encoders for Dataset[Map[K,V]], define explicitlyimplicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]// Primitive types and case classes can be also defined asimplicit val stringIntMapEncoder: Encoder[Map[String, Int]] = ExpressionEncoder()// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List("name", "age"))).collect()// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))// 所有的示例代码可以在 Spark repo 的 "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" 中找到。 Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// Spark SQL 支持自动转换一个 JavaBean 的 RDD 为一个 DataFrame。这个 BeanInfo（Bean 的信息），可以使用反射获取，定义表的 Schema。目前，Spark SQL 不支持包含 Map 字段的 JavaBean。嵌套的 JavaBean 和 List 或者 Array 字段已经支持。// 您可以通过创建一个实现了序列化的和拥有它的所有字段的 getter 以及 setter 方法的 class 来创建一个 JavaBean。import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.Encoder;import org.apache.spark.sql.Encoders;// Create an RDD of Person objects from a text fileJavaRDD&lt;Person&gt; peopleRDD = spark.read() .textFile("examples/src/main/resources/people.txt") .javaRDD() .map(new Function&lt;String, Person&gt;() &#123; @Override public Person call(String line) throws Exception &#123; String[] parts = line.split(","); Person person = new Person(); person.setName(parts[0]); person.setAge(Integer.parseInt(parts[1].trim())); return person; &#125; &#125;);// Apply a schema to an RDD of JavaBeans to get a DataFrameDataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView("people");// SQL statements can be run by using the sql methods provided by sparkDataset&lt;Row&gt; teenagersDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19");// The columns of a row in the result can be accessed by field indexEncoder&lt;String&gt; stringEncoder = Encoders.STRING();Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(new MapFunction&lt;Row, String&gt;() &#123; @Override public String call(Row row) throws Exception &#123; return "Name:" + row.getString(0); &#125;&#125;, stringEncoder);teenagerNamesByIndexDF.show();// +------------+// | value|// +------------+// |Name: Justin|// +------------+// or by field nameDataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(new MapFunction&lt;Row, String&gt;() &#123; @Override public String call(Row row) throws Exception &#123; return "Name:" + row.&lt;String&gt;getAs("name"); &#125;&#125;, stringEncoder);teenagerNamesByFieldDF.show();// +------------+// | value|// +------------+// |Name: Justin|// +------------+// 所有的示例代码可以在 Spark repo 的 "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" 中找到。 Python 1234567891011121314151617181920212223# Spark SQL 可以转换一个 Row 的 RDD 对象为一个 DataFrame，然后推断数据类型。Row 通过传递一系列 key/value（键 / 值）对作为 kwargs 到 Row class 从而被构造出来。# 列出的 key 定义了表的列名，通过抽样整个数据库推断类型，与 JSON 文件上的执行的推断是相似的。# spark is an existing SparkSession.from pyspark.sql import Rowsc = spark.sparkContext# Load a text file and convert each line to a Row.lines = sc.textFile("examples/src/main/resources/people.txt")parts = lines.map(lambda l: l.split(","))people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))# Infer the schema, and register the DataFrame as a table.schemaPeople = spark.createDataFrame(people)schemaPeople.createOrReplaceTempView("people")# SQL can be run over DataFrames that have been registered as a table.teenagers = spark.sql("SELECT name FROM people WHERE age&gt;= 13 AND age &lt;= 19")# The results of SQL queries are RDDs and support all the normal RDD operations.teenNames = teenagers.map(lambda p: "Name:" + p.name)for teenName in teenNames.collect(): print(teenName) 以编程的方式指定 Schema当 case class 不能够在执行之前被定义（例如，records 记录的结构在一个 string 字符串中被编码了，或者一个 text 文本 datase 将被解析并且不同的用户投影的字段是不一样的）。一个 DataFrame 可以使用下面的三步以编程的方式来创建。 从原始的 RDD 创建 RDD 的 RowS（行）。 Step 1 被创建后，创建 Schema 表示一个 StructType 匹配 RDD 中的 Rows（行）的结构。 通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD 的 RowS（行）。 例如 : Scala 123456789101112131415161718192021222324252627282930313233343536373839import org.apache.spark.sql.types._import org.apache.spark.sql.Row// Create an RDDval peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")// The schema is encoded in a stringval schemaString = "name age"// Generate the schema based on the string of schemaval fields = schemaString.split(" ") .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))val schema = StructType(fields)// Convert records of the RDD (people) to Rowsval rowRDD = peopleRDD .map(_.split(",")) .map(attributes =&gt; Row(attributes(0), attributes(1).trim))// Apply the schema to the RDDval peopleDF = spark.createDataFrame(rowRDD, schema)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL can be run over a temporary view created using DataFramesval results = spark.sql("SELECT name FROM people")// The results of SQL queries are DataFrames and support all the normal RDD operations// The columns of a row in the result can be accessed by field index or by field nameresults.map(attributes =&gt; "Name:" + attributes(0)).show()// +-------------+// | value|// +-------------+// |Name: Michael|// | Name: Andy|// | Name: Justin|// +-------------+ 数据源Spark SQL 支持通过 DataFrame 接口操作多种数据源。一个 DataFrame 可以通过关联转换来操作，也可以被创建为一个临时的 view。注册一个 DataFrame 作为一个临时的 view 就可以允许你在数据集上运行 SQL 查询。本节介绍了一些通用的方法通过使用 Spark Data Sources 来加载和保存数据以及一些可用的内置数据源的特定选项。 通用的 Load/Save 函数在最简单的方式下，默认的数据源 (parquet 除非另外配置通过 spark.sql.sources.default) 将会用于所有的操作。 Scala 12val usersDF = spark.read.load("examples/src/main/resources/users.parquet")usersDF.select("name", "favorite_color").write.save("namesAndFavColors.parquet") 手动指定选项你也可以手动的指定数据源，并且将与你想要传递给数据源的任何额外选项一起使用。数据源由其完全限定名指定 (例如：org.apache.spark.sql.parquet)，不过对于内置数据源你也可以使用它们的缩写名 (json,parquet,jdbc)。使用下面这个语法可以将从任意类型数据源加载的 DataFrames 转换为其他类型。 Scala 12val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")peopleDF.select("name", "age").write.format("parquet").save("namesAndAges.parquet") 直接在文件上运行 SQL你也可以直接在文件上运行 SQL 查询来替代使用 API 将文件加载到 DataFrame 再进行查询 Scala 1val sqlDF = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`") 保存模式Save 操作可以使用 SaveMode，可以指定如何处理已经存在的数据。这是很重要的要意识到这些保存模式没有利用任何锁并且也不是原子操作。另外，当执行 Overwrite，新数据写入之前会先将旧数据删除。 Scala/Java Any Language Meaning SaveMode.ErrorIfExists(default) “error”(default) 当保存 DataFrame 到一个数据源，如果数据已经存在，将会抛出异常。 SaveMode.Append “append” 当保存 DataFrame 到一个数据源，如果数据 / 表已经存在, DataFrame 的内容将会追加到已存在的数据后面。 SaveMode.Overwrite “overwrite” Overwrite 模式意味着当保存 DataFrame 到一个数据源，如果数据 / 表已经存在，那么已经存在的数据将会被 DataFrame 的内容覆盖。 SaveMode.Ignore “ignore” Ignore 模式意味着当保存 DataFrame 到一个数据源，如果数据已经存在，save 操作不会将 DataFrame 的内容保存，也不会修改已经存在的数据。这个和 SQL 中的’CREATE TABLE IF NOT EXISTS’类似 。 保存为持久化的表DataFrames 也可以通过 saveAsTable 命令来保存为一张持久表到 Hive metastore 中。值得注意的是对于这个功能来说已经存在的 Hive 部署不是必须的。Spark 将会为你创造一个默认的本地 Hive metastore（使用 Derby)。不像 createOrReplaceTempView 命令，saveAsTable 将会持久化 DataFrame 的内容并在 Hive metastore 中创建一个指向数据的指针。持久化的表将会一直存在甚至当你的 Spark 应用已经重启，只要保持你的连接是和一个相同的 metastore。一个相对于持久化表的 DataFrame 可以通过在 SparkSession 中调用 table 方法创建。 默认的话 saveAsTable 操作将会创建一个 “managed table”，意味着数据的位置将会被 metastore 控制。Managed tables 在表 drop 后也数据也会自动删除。 Parquet 文件Parquet 是一个列式存储格式的文件，被许多其他数据处理系统所支持。Spark SQL 支持对 Parquet 文件的读写还可以自动的保存源数据的模式 以编程的方式加载数据Scala 123456789101112import spark.implicits._val peopleDF = spark.read.json("examples/src/main/resources/people.json")peopleDF.write.parquet("people.parquet")val parquetFileDF = spark.read.parquet("people.parquet")parquetFileDF.createOrReplaceTempView("parquetFile")val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")namesDF.map(attributes =&gt; "Name:" + attributes(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+ 分区发现在系统中，比如 Hive，表分区是一个很常见的优化途径。在一个分区表中 ，数据通常存储在不同的文件目录中，对每一个分区目录中的途径按照分区列的值进行编码。Parquet 数据源现在可以自动地发现并且推断出分区的信息。例如，我们可以将之前使用的人口数据存储成下列目录结构的分区表，两个额外的列，gender 和 country 作为分区列： 12345678910111213141516171819path└── to └── table ├── gender=male │ ├── ... │ │ │ ├── country=US │ │ └── data.parquet │ ├── country=CN │ │ └── data.parquet │ └── ... └── gender=female ├── ... │ ├── country=US │ └── data.parquet ├── country=CN │ └── data.parquet └── ... 通过向 SparkSession.read.parquet 或 SparkSession.read.load 中传入 path/to/table,，Spark SQL 将会自动地从路径中提取分区信息。现在返回的 DataFrame schema 变成： 123456root\|-- name: string (nullable = true)\|-- age: long (nullable = true)\|-- gender: string (nullable = true)\|-- country: string (nullable = true) 需要注意的是分区列的数据类型是自动推导出的。当前，支持数值数据类型以及 string 类型。有些时候用户可能不希望自动推导出分区列的数据类型。对于这些使用场景，自动类型推导功能可以通过 spark.sql.sources.partitionColumnTypeInference.enabled 来配置，默认值是 true。当自动类型推导功能禁止，分区列的数据类型是 string。 从 Spark 1.6.0 开始，分区发现只能发现在默认给定的路径下的分区。对于上面那个例子，如果用户向 SparkSession.read.parquet 或 SparkSession.read.load, gender 传递 path/to/table/gender=male 将不会被当做分区列。如果用户需要指定发现的根目录，可以在数据源设置 basePath 选项。比如，将 path/to/table/gender=male 作为数据的路径并且设置 basePath 为 path/to/table/，gender 将会作为一个分区列。 Schema 合并类似 ProtocolBuffer，Avro，以及 Thrift，Parquet 也支持 schema 演变。用户可以从一个简单的 schema 开始，并且根据需要逐渐地向 schema 中添加更多的列。这样，用户最终可能会有多个不同但是具有相互兼容 schema 的 Parquet 文件。Parquet 数据源现在可以自动地发现这种情况，并且将所有这些文件的 schema 进行合并。 由于 schema 合并是一个性格开销比较高的操作，并且在大部分场景下不是必须的，从 Spark 1.5.0 开始默认关闭了这项功能。你可以通过以下方式开启： 设置数据源选项 mergeSchema 为 true 当读取 Parquet 文件时（如下面展示的例子），或者 这是全局 SQL 选项 spark.sql.parquet.mergeSchema 为 true。 Scala 12345678910111213141516171819202122import spark.implicits._// Create a simple DataFrame, store into a partition directoryval squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * i)).toDF("value", "square")squaresDF.write.parquet("data/test_table/key=1")// Create another DataFrame in a new partition directory,// adding a new column and dropping an existing columnval cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * i * i)).toDF("value", "cube")cubesDF.write.parquet("data/test_table/key=2")// Read the partitioned tableval mergedDF = spark.read.option("mergeSchema", "true").parquet("data/test_table")mergedDF.printSchema()// The final schema consists of all 3 columns in the Parquet files together// with the partitioning column appeared in the partition directory paths// root// |-- value: int (nullable = true)// |-- square: int (nullable = true)// |-- cube: int (nullable = true)// |-- key: int (nullable = true) Hive metastore Parquet 表转换当从 Hive metastore 里读写 Parquet 表时，为了更好地提升新能 Spark SQL 会尝试用自己支持的 Parquet 替代 Hive SerDe。这个功能通过 spark.sql.hive.convertMetastoreParquet 选项来控制，默认是开启的。 Hive/Parquet Schema Reconciliation从 Hive 和 Parquet 处理表 schema 过程的角度来看有两处关键的不同。 Hive 对大小写不敏感，而 Parquet 不是 Hive 认为所有列都是 nullable 可为空的，在再 Parquet 中为空性 nullability 是需要显示声明的。 由于这些原因，当我们将 Hive metastore Parquet table 转换为 Spark SQLtable 时必须使 Hive metastore schema 与 Parquet schema 相兼容。兼容规则如下： 相同 schema 的字段的数据类型必须相同除了 nullability。要兼容的字段应该具有 Parquet 的数据类型，因此 nullability 是被推崇的。 reconciled schema 包含了这些 Hive metastore schema 里定义的字段。 任何字段只出现在 Parquet schema 中会被 reconciled schema 排除。 任何字段只出现在 Hive metastore schema 中会被当做 nullable 字段来添加到 reconciled schema 中。 Metadata 刷新为了提高性能 Spark SQL 缓存了 Parquet metadata。当 Hive metastore Parquet table 转换功能开启，这些转换后的元数据信息也会被缓存。如果这些表被 Hive 或者其他外部的工具更新，你需要手动刷新以确保元数据信息保持一致。 Scala 1spark.catalog.refreshTable("my_table") Parquet 配置Parquet 的配置可以使用 SparkSession 的 setConf 来设置或者通过使用 SQL 运行 SET key=value 命令 Property Name Default Meaning spark.sql.parquet.binaryAsString false 其他的一些产生 Parquet 的系统，特别是 Impala 和 SparkSQL 的老版本，当将 Parquet 模式写出时不会区分二进制数据和字符串。这个标志告诉 Spark SQL 将二进制数据解析成字符串，以提供对这些系统的兼容。 spark.sql.parquet.int96AsTimestamp true 其他的一些产生 Parquet 的系统，特别是 Impala，将时间戳存储为 INT96 的形式。Spark 也将时间戳存储为 INT96，因为我们要避免纳秒级字段的精度的损失。这个标志告诉 Spark SQL 将 INT96 数据解析为一个时间戳，以提供对这些系统的兼容。 spark.sql.parquet.cacheMetadata true 打开 Parquet 模式的元数据的缓存。能够加快对静态数据的查询。 spark.sql.parquet.compression.codec gzip 设置压缩编码解码器，当写入一个 Parquet 文件时。可接收的值包括：uncompressed, snappy, gzip, lzo spark.sql.parquet.filterPushdown false 打开 Parquet 过滤器的后进先出存储的优化。这个功能默认是被关闭的，因为一个 Parquet 中的一个已知的 bug 1.6.0rc3 (PARQUET-136)。然而，如果你的表中不包含任何的可为空的 (nullable) 字符串或者二进制列，那么打开这个功能是安全的。 spark.sql.hive.convertMetastoreParquet true 当设置成 false，Spark SQL 会为 parquet 表使用 Hive SerDe(Serialize/Deserilize spark.sql.parquet.mergeSchema false 当设置 true，Parquet 数据源从所有的数据文件中合并 schemas，否则 schema 来自 summary file 或随机的数据文件当 summary file 不可得时. JSON DatasetsSpark SQL 可以自动的推断出 JSON 数据集的 schema 并且将它作为 DataFrame 进行加载。这个转换可以通过使用 SparkSession.read.json() 在字符串类型的 RDD 中或者 JSON 文件。注意作为 json file 提供的文件不是一个典型的 JSON 文件。每一行必须包含一个分开的独立的有效 JSON 对象。因此，常规的多行 JSON 文件通常会失败。 Scala 12345678910111213141516171819202122232425262728293031323334 // A JSON dataset is pointed to by path.// The path can be either a single text file or a directory storing text filesval path = "examples/src/main/resources/people.json"val peopleDF = spark.read.json(path)// The inferred schema can be visualized using the printSchema() methodpeopleDF.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by sparkval teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")teenagerNamesDF.show()// +------+// | name|// +------+// |Justin|// +------+// Alternatively, a DataFrame can be created for a JSON dataset represented by// an RDD[String] storing one JSON object per stringval otherPeopleRDD = spark.sparkContext.makeRDD( """&#123;"name":"Yin","address":&#123;"city":"Columbus","state":"Ohio"&#125;&#125;""" :: Nil)val otherPeople = spark.read.json(otherPeopleRDD)otherPeople.show()// +---------------+----+// | address|name|// +---------------+----+// |[Columbus,Ohio]| Yin|// +---------------+----+ Hive 表Spark SQL 还支持在 Apache Hive 中读写数据。然而，由于 Hive 依赖项太多，这些依赖没有包含在默认的 Spark 发行版本中。如果在 classpath 上配置了 Hive 依赖，那么 Spark 会自动加载它们。注意，Hive 依赖也必须放到所有的 worker 节点上，因为如果要访问 Hive 中的数据它们需要访问 Hive 序列化和反序列化库（SerDes)。 Hive 配置是通过将 hive-site.xml，core-site.xml（用于安全配置）以及 hdfs-site.xml（用于 HDFS 配置）文件放置在 conf/ 目录下来完成的。 下面给出示例： Scala 版： 如果要使用 Hive, 你必须要实例化一个支持 Hive 的 SparkSession，包括连接到一个持久化的 Hive metastore, 支持 Hive 序列化反序列化库以及 Hive 用户自定义函数。即使用户没有安装部署 Hive 也仍然可以启用 Hive 支持。如果没有在 hive-site.xml 文件中配置，Spark 应用程序启动之后，上下文会自动在当前目录下创建一个 metastore_db 目录并创建一个由 spark.sql.warehouse.dir 配置的、默认值是当前目录下的 spark-warehouse 目录的目录。请注意：从 Spark 2.0.0 版本开始, hive-site.xml 中的 hive.metastore.warehouse.dir 属性就已经过时了，你可以使用 spark.sql.warehouse.dir 来指定仓库中数据库的默认存储位置。你可能还需要给启动 Spark 应用程序的用户赋予写权限。 Scala 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessioncase class Record(key: Int, value: String)// warehouseLocation points to the default location for managed databases and tablesval warehouseLocation = "file:$&#123;system:user.dir&#125;/spark-warehouse"val spark = SparkSession .builder() .appName("Spark Hive Example") .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .getOrCreate()import spark.implicits._import spark.sqlsql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")sql("LOAD DATA LOCAL INPATH'examples/src/main/resources/kv1.txt'INTO TABLE src")// Queries are expressed in HiveQLsql("SELECT * FROM src").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.sql("SELECT COUNT(*) FROM src").show()// +--------+// |count(1)|// +--------+// | 500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.val sqlDF = sql("SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key")// The items in DaraFrames are of type Row, which allows you to access each column by ordinal.val stringsDS = sqlDF.map &#123; case Row(key: Int, value: String) =&gt; s"Key: $key, Value: $value"&#125;stringsDS.show()// +--------------------+// | value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a HiveContext.val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s"val_$i")))recordsDF.createOrReplaceTempView("records")// Queries can then join DataFrame data with data stored in Hive.sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// | 2| val_2| 2| val_2|// | 2| val_2| 2| val_2|// | 4| val_4| 4| val_4|// ... Java 版： 如果要使用 Hive, 你必须要实例化一个支持 Hive 的 SparkSession，包括连接到一个持久化的 Hive metastore, 支持 Hive 序列化反序列化库以及 Hive 用户自定义函数。即使用户没有安装部署 Hive 也仍然可以启用 Hive 支持。如果没有在 hive-site.xml 文件中配置，Spark 应用程序启动之后，上下文会自动在当前目录下创建一个 metastore_db 目录并创建一个由 spark.sql.warehouse.dir 配置的、默认值是当前目录下的 spark-warehouse 目录的目录。请注意：从 Spark 2.0.0 版本开始, hive-site.xml 中的 hive.metastore.warehouse.dir 属性就已经过时了，你可以使用 spark.sql.warehouse.dir 来指定仓库中数据库的默认存储位置。你可能还需要给启动 Spark 应用程序的用户赋予写权限。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114import java.io.Serializable;import java.util.ArrayList;import java.util.List;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Encoders;import org.apache.spark.sql.Row;import org.apache.spark.sql.SparkSession;public static class Record implements Serializable &#123; private int key; private String value; public int getKey() &#123; return key; &#125; public void setKey(int key) &#123; this.key = key; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125;&#125;// warehouseLocation points to the default location for managed databases and tablesString warehouseLocation = "file:" + System.getProperty("user.dir") + "spark-warehouse";SparkSession spark = SparkSession .builder() .appName("Java Spark Hive Example") .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .getOrCreate();spark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)");spark.sql("LOAD DATA LOCAL INPATH'examples/src/main/resources/kv1.txt'INTO TABLE src");// Queries are expressed in HiveQLspark.sql("SELECT * FROM src").show();// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.spark.sql("SELECT COUNT(*) FROM src").show();// +--------+// |count(1)|// +--------+// | 500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.Dataset&lt;Row&gt; sqlDF = spark.sql("SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key");// The items in DaraFrames are of type Row, which lets you to access each column by ordinal.Dataset&lt;String&gt; stringsDS = sqlDF.map(new MapFunction&lt;Row, String&gt;() &#123; @Override public String call(Row row) throws Exception &#123; return "Key:" + row.get(0) + ", Value:" + row.get(1); &#125;&#125;, Encoders.STRING());stringsDS.show();// +--------------------+// | value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a HiveContext.List&lt;Record&gt; records = new ArrayList&lt;&gt;();for (int key = 1; key &lt; 100; key++) &#123; Record record = new Record(); record.setKey(key); record.setValue("val_" + key); records.add(record);&#125;Dataset&lt;Row&gt; recordsDF = spark.createDataFrame(records, Record.class);recordsDF.createOrReplaceTempView("records");// Queries can then join DataFrames data with data stored in Hive.spark.sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show();// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// | 2| val_2| 2| val_2|// | 2| val_2| 2| val_2|// | 4| val_4| 4| val_4|// ... Python 版： 如果要使用 Hive, 你必须要实例化一个支持 Hive 的 SparkSession，包括连接到一个持久化的 Hive metastore, 支持 Hive 序列化反序列化库以及 Hive 用户自定义函数。即使用户没有安装部署 Hive 也仍然可以启用 Hive 支持。如果没有在 hive-site.xml 文件中配置，Spark 应用程序启动之后，上下文会自动在当前目录下创建一个 metastore_db 目录并创建一个由 spark.sql.warehouse.dir 配置的、默认值是当前目录下的 spark-warehouse 目录的目录。请注意：从 Spark 2.0.0 版本开始, hive-site.xml 中的 hive.metastore.warehouse.dir 属性就已经过时了，你可以使用 spark.sql.warehouse.dir 来指定仓库中数据库的默认存储位置。你可能还需要给启动 Spark 应用程序的用户赋予写权限。 Python 123456789# spark is an existing SparkSessionspark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")spark.sql("LOAD DATA LOCAL INPATH'examples/src/main/resources/kv1.txt'INTO TABLE src")# Queries can be expressed in HiveQL.results = spark.sql("FROM src SELECT key, value").collect() R 版： 如果要使用 Hive, 你必须要实例化一个支持 Hive 的 SparkSession。这添加了在 MetaStore 中查找表和使用 HiveQL 写查询的支持。 R 12345678# enableHiveSupport defaults to TRUEsparkR.session(enableHiveSupport = TRUE)sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")sql("LOAD DATA LOCAL INPATH'examples/src/main/resources/kv1.txt'INTO TABLE src")# Queries can be expressed in HiveQL.results &lt;- collect(sql("FROM src SELECT key, value")) 完整示例代码参见 Spark 仓库中的 “examples/src/main/r/RSparkSQLExample.R”。 与不同版本的 Hive Metastore 交互Spark SQL 对 Hive 最重要的一个支持就是可以和 Hive metastore 进行交互，这使得 Spark SQL 可以访问 Hive 表的元数据。从 Spark 1.4.0 版本开始，通过使用下面描述的配置, Spark SQL 一个简单的二进制编译版本可以用来查询不同版本的 Hive metastore。注意，不管用于访问 metastore 的 Hive 是什么版本，Spark SQL 内部都使用 Hive 1.2.1 版本进行编译, 并且使用这个版本的一些类用于内部执行（serdes，UDFs，UDAFs 等）。 下面的选项可用来配置用于检索元数据的 Hive 版本： 属性名 默认值 含义 spark.sql.hive.metastore.version 1.2.1 Hive metastore 版本。可用选项从 0.12.0 到 1.2.1 。 spark.sql.hive.metastore.jars builtin 存放用于实例化 HiveMetastoreClient 的 jar 包位置。这个属性可以是下面三个选项之一：1. builtin：使用 Hive 1.2.1 版本，当启用 -Phive 时会和 Spark 一起打包。如果使用了这个选项, 那么 spark.sql.hive.metastore.version 要么是 1.2.1，要么就不定义。2. maven：使用从 Maven 仓库下载的指定版本的 Hive jar 包。生产环境部署通常不建议使用这个选项。3. 标准格式的 JVM classpath。这个 classpath 必须包含所有 Hive 及其依赖的 jar 包，并且包含正确版本的 hadoop。这些 jar 包只需要部署在 driver 节点上，但是如果你使用 yarn cluster 模式运行，那么你必须要确保这些 jar 包是和应用程序一起打包的。 spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc, org.postgresql, com.microsoft.sqlserver, oracle.jdbc 一个逗号分隔的类名前缀列表，这些类使用 classloader 加载，且可以在 Spark SQL 和特定版本的 Hive 间共享。一个共享类的示例就是用来访问 Hive metastore 的 JDBC driver。其它需要共享的类，是需要与已经共享的类进行交互的。例如，log4j 使用的自定义 appender 。 spark.sql.hive.metastore.barrierPrefixes (empty) 一个逗号分隔的类名前缀列表，这些类需要在 Spark SQL 访问的每个 Hive 版本中显式地重新加载。例如，在一个共享前缀列表（org.apache.spark.*）中声明的 Hive UDF 通常需要被共享。 JDBC 连接其它数据库Spark SQL 还有一个能够使用 JDBC 从其他数据库读取数据的数据源。当使用 JDBC 访问其它数据库时，应该首选 JdbcRDD。这是因为结果是以数据框（DataFrame）返回的，且这样 Spark SQL 操作轻松或便于连接其它数据源。因为这种 JDBC 数据源不需要用户提供 ClassTag，所以它也更适合使用 Java 或 Python 操作。（注意，这与允许其它应用使用 Spark SQL 执行查询操作的 Spark SQL JDBC 服务器是不同的）。 使用 JDBC 访问特定数据库时，需要在 spark classpath 上添加对应的 JDBC 驱动配置。例如，为了从 Spark Shell 连接 postgres，你需要运行如下命令： 通过调用数据源 API，远程数据库的表可以被加载为 DataFrame 或 Spark SQL 临时表。支持的参数有： 1bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar 属性名 含义 url 要连接的 JDBC URL。 dbtable 要读取的 JDBC 表。 注意，一个 SQL 查询的 From 分语句中的任何有效表都能被使用。例如，既可以是完整表名，也可以是括号括起来的子查询语句。 driver 用于连接 URL 的 JDBC 驱动的类名。 partitionColumn, lowerBound, upperBound, numPartitions 这 几个选项，若有一个被配置，则必须全部配置。它们描述了当从多个 worker 中并行的读取表时，如何对它分区。partitionColumn 必须时所查询表的一个数值字段。注意，lowerBound 和 upperBound 都只是用于决定分区跨度的，而不是过滤表中的行。因此，表中的所有行将被分区并返回。 fetchSize JDBC fetch size, 决定每次读取多少行数据。 默认将它设为较小值（如，Oracle 上设为 10）有助于 JDBC 驱动上的性能优化。 代码示例如下 : Scala 1234567val jdbcDF = spark.read .format("jdbc") .option("url", "jdbc:postgresql:dbserver") .option("dbtable", "schema.tablename") .option("user", "username") .option("password", "password") .load() Java 1234567Dataset&lt;Row&gt; jdbcDF = spark.read() .format("jdbc") .option("url", "jdbc:postgresql:dbserver") .option("dbtable", "schema.tablename") .option("user", "username") .option("password", "password") .load(); Python 1234567jdbcDF = spark.read \ .format("jdbc") \ .option("url", "jdbc:postgresql:dbserver") \ .option("dbtable", "schema.tablename") \ .option("user", "username") \ .option("password", "password") \ .load() R 1df &lt;- read.jdbc("jdbc:postgresql:dbserver", "schema.tablename", user = "username", password = "password") SQL 123456CREATE TEMPORARY VIEW jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url "jdbc:postgresql:dbserver", dbtable "schema.tablename") 故障排除 在客户端会话（client session) 中或者所有 executor 上，JDBC 驱动类必须可见于原生的类加载器。这是因为 Java 的驱动管理（DriverManager）类在打开一个连接之前会做一个安全检查，这就导致它忽略了所有对原生类加载器不可见的驱动。一个方便的方法，就是修改所有 worker 节点上的 compute_classpath.sh 以包含你的驱动 Jar 包。 一些数据库，如 H2，会把所有的名称转为大写。在 Spark SQL 中你也需要使用大写来引用这些名称。 性能调优对一些工作负载，可能的性能改进的方式，不是把数据缓存在内存里，就是调整一些试验选项。 缓存数据到内存Spark SQL 可以通过调用 spark.cacheTable(“tableName”) 或者 dataFrame.cache() 以列存储格式缓存表到内存中。随后，Spark SQL 将会扫描必要的列，并自动调整压缩比例，以减少内存占用和 GC 压力。你可以调用 spark.uncacheTable(“tableName”) 来删除内存中的表。 你可以在 SparkSession 上使用 setConf 方法或在 SQL 语句中运行 SET key=value 命令，来配置内存中的缓存。 属性名 默认值 含义 spark.sql.inMemoryColumnarStorage.compressed true 当设置为 true 时，Spark SQL 将会基于数据的统计信息自动地为每一列选择单独的压缩编码方式。 spark.sql.inMemoryColumnarStorage.batchSize 10000 控制列式缓存批量的大小。当缓存数据时，增大批量大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险。 其它配置选项下面的选项也可以用来提升查询执行的性能。随着 Spark 自动地执行越来越多的优化操作，这些选项在未来的发布版本中可能会过时。 属性名 默认值 含义 spark.sql.files.maxPartitionBytes 134217728 (128 MB) 读取文件时单个分区可容纳的最大字节数。 spark.sql.files.openCostInBytes 4194304 (4 MB) 打开文件的估算成本，按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)。 spark.sql.autoBroadcastJoinThreshold 10485760 (10 MB) 配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1，可以禁用广播。注意，目前的数据统计仅支持已经运行了 ANALYZE TABLE COMPUTE STATISTICS noscan 命令的 Hive metastore 表。 spark.sql.shuffle.partitions 200 配置为连接或聚合操作混洗（shuffle）数据时使用的分区数。 分布式 SQL 引擎通过使用 Spark SQL 的 JDBC/ODBC 或者命令行接口，它还可以作为一个分布式查询引擎。在这种模式下，终端用户或应用程序可以运行 SQL 查询来直接与 Spark SQL 交互，而不需要编写任何代码。 运行 Thrift JDBC/ODBC server这里实现的 Thrift JDBC/ODBC server 对应于 Hive 1.2.1 版本中的 HiveServer2。你可以使用 Spark 或者 Hive 1.2.1 自带的 beeline 脚本来测试这个 JDBC server。 要启动 JDBC/ODBC server， 需要在 Spark 安装目录下运行如下命令： 1./sbin/start-thriftserver.sh 这个脚本能接受所有 bin/spark-submit 命令行选项，外加一个用于指定 Hive 属性的 –hiveconf 选项。你可以运行 ./sbin/start-thriftserver.sh –help 来查看所有可用选项的完整列表。默认情况下，这启动的 server 将会在 localhost:10000 上进行监听。你可以覆盖该行为，比如使用以下环境变量： 12345export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;./sbin/start-thriftserver.sh \ --master &lt;master-uri&gt; \ ... 或者系统属性： 12345./sbin/start-thriftserver.sh \ --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \ --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \ --master &lt;master-uri&gt; ... 现在你可以使用 beeline 来测试这个 Thrift JDBC/ODBC server： 1./bin/beeline 在 beeline 中使用以下命令连接到 JDBC/ODBC server : 1beeline&gt; !connect jdbc:hive2://localhost:10000 Beeline 会要求你输入用户名和密码。在非安全模式下，只需要输入你本机的用户名和一个空密码即可。对于安全模式，请参考 beeline 文档 中的指示。 将 hive-site.xml，core-site.xml 以及 hdfs-site.xml 文件放置在 conf 目录下可以完成 Hive 配置。 你也可以使用 Hive 自带的 beeline 的脚本。 Thrift JDBC server 还支持通过 HTTP 传输来发送 Thrift RPC 消息。使用下面的设置作为系统属性或者对 conf 目录中的 hive-site.xml 文件配置来启用 HTTP 模式： 123hive.server2.transport.mode - Set this to value: httphive.server2.thrift.http.port - HTTP port number fo listen on; default is 10001hive.server2.http.endpoint - HTTP endpoint; default is cliservice 为了测试，在 HTTP 模式中使用 beeline 连接到 JDBC/ODBC server： 1beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt; 运行 Spark SQL CLISpark SQL CLI 是一个很方便的工具，它可以在本地模式下运行 Hive metastore 服务，并且执行从命令行中输入的查询语句。注意：Spark SQL CLI 无法与 Thrift JDBC server 通信。 要启动 Spark SQL CLI, 可以在 Spark 安装目录运行下面的命令: 1./bin/spark-sql 将 hive-site.xml，core-site.xml 以及 hdfs-site.xml 文件放置在 conf 目录下可以完成 Hive 配置。你可以运行 ./bin/spark-sql –help 来获取所有可用选项的完整列表。 迁移指南从 Spark SQL 1.6 升级到 2.0 SparkSession 现在是 Spark 新的切入点, 它替代了老的 SQLContext 和 HiveContext。注意：为了向下兼容, 老的 SQLContext 和 HiveContext 仍然保留。可以从 SparkSession 获取一个新的 catalog 接口——现有的访问数据库和表的 API, 如 listTables, createExternalTable, dropTempView, cacheTable 都被移到该接口。 Dataset API 和 DataFrame API 进行了统一。在 Scala 中，DataFrame 变成了 Dataset[Row] 的一个类型别名, 而 Java API 使用者必须将 DataFrame 替换成 Dataset。Dataset 类既提供了强类型转换操作 (如 map, filter 以及 groupByKey) 也提供了非强类型转换操作 (如 select 和 groupBy) 。由于编译期的类型安全不是 Python 和 R 语言的一个特性, Dataset 的概念并不适用于这些语言的 API。相反，DataFrame 仍然是最基本的编程抽象, 就类似于这些语言中单节点数据帧的概念。 Dataset 和 DataFrame API 中 unionAll 已经过时并且由 union 替代。 Dataset 和 DataFrame API 中 explode 已经过时，作为选择，可以结合 select 或 flatMap 使用 functions.explode() 。 Dataset 和 DataFrame API 中 registerTempTable 已经过时并且由 createOrReplaceTempView 替代。 从 Spark SQL 1.5 升级到 1.6 Spark 1.6 中，默认情况下服务器在多会话模式下运行。这意味着每个 JDBC / ODBC 连接拥有一份自己的 SQL 配置和临时注册表。缓存表仍在并共享。如果你想在单会话模式服务器运行，请设置选项 spark.sql.hive.thriftServer.singleSession 为 true。您既可以将此选项添加到 spark-defaults.conf，或者通过 –conf 将它传递给 start-thriftserver.sh。 1./sbin/start-thriftserver.sh \ --conf spark.sql.hive.thriftServer.singleSession=true \ ... 从 1.6.1 开始，在 sparkR 中 withColumn 方法支持添加一个新列或更换数据框同名的现有列。 从 Spark 1.6 开始，LongType 强制转换为 TimestampType 秒，而不是微秒。这一变化是为了匹配 Hive 1.2 ，保证从数值类型转换到 TimestampType 的一致性。见 SPARK-11724 了解详情。 从 Spark SQL 1.4 升级到 1.5 使用手动管理的内存优化执行，现在是默认启用的，以及代码生成表达式求值。这些功能既可以通过设置 spark.sql.tungsten.enabled 到 false 来禁止使用。 Parquet 的模式合并默认情况下不再启用。它可以通过设置重新启用 spark.sql.parquet.mergeSchema 到 true 。 字符串在 Python 列的分辨率现在支持使用点（.）来限定列或访问嵌套值。例如 df[‘table.column.nestedField’]。但是，这意味着如果你的列名中包含任何圆点，你现在必须避免使用反引号（如 table.column.with.dots.nested）。 在内存中的列存储分区修剪默认是开启的。它可以通过设置 spark.sql.inMemoryColumnarStorage.partitionPruning 到 false 来禁用。 无限精度的小数列不再支持，而不是 Spark SQL 最大精度为 38 。当从 BigDecimal 对象推断模式时，现在使用（38，18）。当 DDL 没有指定精度，则默认保留 Decimal(10, 0)。 时间戳现在存储在 1 微秒的精度，而不是 1 纳秒的。 在 sql 语句中，浮点数现在解析为十进制。HiveQL 解析保持不变。 SQL/DateFrame 数据帧功能的规范名称现在是小写（e.g. sum vs SUM）。 JSON 数据源不会自动加载由其他应用程序（未通过 Spark SQL 插入到数据集的文件）创建的新文件。对于 JSON 持久表（即表的元数据存储在 Hive Metastore），用户可以使用 REFRESH TABLE SQL 命令或 HiveContext 的 refreshTable 方法，把那些新文件列入到表中。对于代表一个 JSON 数据集的数据帧，用户需要重新创建数据框，同时数据框中将包括新的文件。 PySpark DataFrame 的 withColumn 方法支持添加新的列或替换现有的同名列。 从 Spark SQL 1.3 升级到 1.4数据帧的数据读 / 写器接口根据用户的反馈，我们创建了一个新的更快速的 API 中读取数据 ( SQLContext.read）和写入数据（DataFrame.write）。同时废弃的过时的 API（例如 SQLContext.parquetFile，SQLContext.jsonFile）。 请参阅 API 文档 SQLContext.read（Scala ，Java “http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SQLContext.html#read()&quot;)， Python ）和 DataFrame.write（ Scala ，Java， Python ）的更多信息。 DataFrame.groupBy 保留分组列根据用户反馈，我们改变的默认行为 DataFrame.groupBy().agg() 保留在 DataFrame 的分组列。为了维持 1.3 的行为特征，设置 spark.sql.retainGroupColumns 为 false。 Scala 示例： 123456789// 在 1.3.x 中, 为了让 "department" 列得到展示，// 必须包括明确作为 gg 函数调用的一部分。df.groupBy("department").agg($"department", max("age"), sum("expense"))// 在 1.4 以上版本, "department" 列自动包含了.df.groupBy("department").agg(max("age"), sum("expense"))// 恢复到 1.3 版本（不保留分组列）sqlContext.setConf("spark.sql.retainGroupColumns", "false") 在 DataFrame.withColumn 中的改变之前 1.4 版本中，DataFrame.withColumn（）只支持添加列。该列将始终在 DateFrame 结果中被加入作为新的列，即使现有的列可能存在相同的名称。从 1.4 版本开始，DataFrame.withColumn（）支持添加与所有现有列的名称不同的列或替换现有的同名列。 请注意，这一变化仅适用于 Scala API , 并不适用于 PySpark 和 SparkR。 从 Spark SQL 1.0~1.2 升级到 1.3在 Spark 1.3 中，我们从 Spark SQL 中删除了 “Alpha” 的标签，作为一部分已经清理过的可用的 API 。从 Spark 1.3 版本以上，Spark SQL 将提供在 1.X 系列的其他版本的二进制兼容性。这种兼容性保证不包括被明确标记为不稳定的（即 DeveloperApi 类或 Experimental）的 API。 重命名 SchemaRDD 到 DateFrame升级到 Spark SQL 1.3 版本时，用户会发现最大的变化是，SchemaRDD 已更名为 DataFrame。这主要是因为 DataFrames 不再从 RDD 直接继承，而是由 RDDS 自己来实现这些功能。DataFrames 仍然可以通过调用 .rdd 方法转换为 RDDS 。 在 Scala 中，有一个从 SchemaRDD 到 DataFrame 类型别名，可以为一些情况提供源代码兼容性。它仍然建议用户更新他们的代码以使用 DataFrame 来代替。Java 和 Python 用户需要更新他们的代码。 在 Java 和 Scala API 的统一此前 Spark 1.3 有单独的 Java 兼容类（JavaSQLContext 和 JavaSchemaRDD），借鉴于 Scala API。在 Spark 1.3 中，Java API 和 Scala API 已经统一。两种语言的用户可以使用 SQLContext 和 DataFrame 。一般来说论文类尝试使用两种语言的共有类型（如 Array 替代了一些特定集合）。在某些情况下不通用的类型情况下，（例如，passing in closures 或 Maps）使用函数重载代替。 此外，该 Java 的特定类型的 API 已被删除。Scala 和 Java 的用户可以使用存在于 org.apache.spark.sql.types 类来描述编程模式。 隐式转换和 DSL 包的移除（仅限于 Scala）许多 Spark 1.3 版本以前的代码示例都以 import sqlContext._ 开始，这提供了从 sqlContext 到 cope 的所有功能。在 Spark 1.3 中，我们移除了从 RDDs 到 DateFrame 再到 SQLContext 内部对象的隐式转换。 此外，隐式转换现在只是通过 toDF 方法增加 RDDs 所组成的一些类型（例如 classes 或 tuples），而不是自动应用。 当使用 DSL 的内部函数（现在由 DataFrame API 代替）的时候，用于一般会导入 org.apache.spark.sql.catalyst.dsl 来代替一些公有的 DataFrame 的 API 函数 ：import org.apache.spark.sql.functions._。 删除在 org.apache.spark.sql 包中的一些 DataType 别名（仅限于 Scala）Spark 1.3 移除存在于基本 SQL 包的 DataType 类型别名。开发人员应改为导入类 org.apache.spark.sql.types。 UDF 注册迁移到 sqlContext.udf 中 (针对 Java 和 Scala)用于注册 UDF 的函数，不管是 DataFrame DSL 还是 SQL 中用到的，都被迁移到 SQLContext 中的 udf 对象中。 Scala 1sqlContext.udf.register("strLen", (s: String) =&gt; s.length()) Python UDF 注册保持不变。 Python 的 DataType 不再是单例的在 Python 中使用 DataTypes 时，你需要先构造它们（如：StringType（）），而不是引用一个单例对象。 兼容 Apache HiveSpark SQL 在设计时就考虑到了和 Hive metastore，SerDes 以及 UDF 之间的兼容性。目前 Hive SerDes 和 UDF 都是基于 Hive 1.2.1 版本，并且 Spark SQL 可以连接到不同版本的 Hive metastore（从 0.12.0 到 1.2.1，可以参考［与不同版本的 Hive Metastore 交互］） 在现有的 Hive 仓库中部署Spark SQL Thrift JDBC server 采用了开箱即用的设计以兼容已有的 Hive 安装版本。你不需要修改现有的 Hive Metastore , 或者改变数据的位置和表的分区。 支持 Hive 的特性Spark SQL 支持绝大部分的 Hive 功能，如： Hive 查询语句, 包括： SELECT GROUP BY ORDER BY CLUSTER BY SORT BY 所有的 Hive 运算符， 包括： 关系运算符 (=, ⇔, ==, &lt;&gt;, &lt;, &gt;, &gt;=, &lt;=, etc) 算术运算符 (+, -, *, /, %, etc) 逻辑运算符 (AND, &amp;&amp;, OR, ||, etc) 复杂类型构造器 - 数学函数 (sign, ln, cos 等) String 函数 (instr, length, printf 等) 用户自定义函数（UDF） 用户自定义聚合函数（UDAF） 用户自定义序列化格式（SerDes） 窗口函数 Joins JOIN {LEFT|RIGHT|FULL} OUTER JOIN LEFT SEMI JOIN - CROSS JOIN Unions 子查询 SELECT col FROM (SELECT a + b AS col from t1) t2 采样 Explain 分区表，包括动态分区插入 视图 所有 Hive DDL 功能, 包括： CREATE TABLE CREATE TABLE AS SELECT ALTER TABLE 绝大多数 Hive 数据类型，包括 TINYINT SMALLINT INT BIGINT BOOLEAN FLOAT DOUBLE STRING BINARY TIMESTAMP DATE ARRAY&lt;&gt; MAP&lt;&gt; STRUCT&lt;&gt; 不支持的 Hive 功能以下是目前还不支持的 Hive 功能列表。在 Hive 部署中这些功能大部分都用不到。 Hive 核心功能bucket：bucket 是 Hive 表分区内的一个哈希分区，Spark SQL 目前还不支持 bucket。 Hive 高级功能 UNION 类型 Unique join 列统计数据收集：Spark SQL 目前不依赖扫描来收集列统计数据并且仅支持填充 Hive metastore 的 sizeInBytes 字段。 Hive 输入输出格式 CLI 文件格式：对于回显到 CLI 中的结果，Spark SQL 仅支持 TextOutputFormat。 Hadoop archive Hive 优化有少数 Hive 优化还没有包含在 Spark 中。其中一些（比如索引）由于 Spark SQL 的这种内存计算模型而显得不那么重要。另外一些在 Spark SQL 未来的版本中会持续跟踪。 块级别位图索引和虚拟列（用来建索引） 自动为 join 和 groupBy 计算 reducer 个数：目前在 Spark SQL 中，你需要使用 “SET spark.sql.shuffle.partitions=[num_tasks];” 来控制后置混洗的并行程度。 仅查询元数据：对于只需要使用元数据的查询请求，Spark SQL 仍需要启动任务来计算结果。 数据倾斜标志：Spark SQL 不遵循 Hive 中的数据倾斜标志 STREAMTABLE join 操作提示：Spark SQL 不遵循 STREAMTABLE 提示。 对于查询结果合并多个小文件：如果返回的结果有很多小文件，Hive 有个选项设置，来合并小文件，以避免超过HDFS的文件数额度限制。Spark SQL 不支持这个。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark Streaming]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-streaming-programming-guide%2F</url>
      <content type="text"><![CDATA[Spark Streaming 概述Spark Streaming 是 Spark 核心 API 的扩展，它支持弹性的，高吞吐的，容错的实时数据流的处理。数据可以通过多种数据源获取，例如 Kafka，Flume，Kinesis 以及 TCP sockets，也可以通过高阶函数例如 map，reduce，join，window 等组成的复杂算法处理。最终，处理后的数据可以输出到文件系统，数据库以及实时仪表盘中。事实上，你还可以在数据流上使用 Spark 机器学习 以及 图形处理 算法 。 在内部，它工作原理如下，Spark Streaming 接收实时输入数据流并将数据切分成多个批数据，然后交由 Spark 引擎处理并分批的生成结果数据流。 Spark Streaming 提供了一个高层次的抽象叫做离散流 (discretized stream) 或者 DStream，它代表一个连续的数据流。DStream 可以通过来自数据源的输入数据流创建，例如 Kafka，Flume 以及 Kinesis，或者在其他 DStream 上进行高层次的操作创建。在内部，一个 DStream 是通过一个 RDDs 的序列来表示。 本指南告诉你如何使用 DStream 来编写一个 Spark Streaming 程序。你可以使用 Scala，Java 或者 Python (Spark 1.2 版本后引进) 来编写程序。 快速开始示例在我们详细介绍如何你自己的 Spark Streaming 程序的细节之前，让我们先来看一看一个简单的 Spark Streaming 程序的样子。比方说，我们想要计算从一个监听 TCP socket 的数据服务器接收到的文本数据（text data）中的字数。所有你需要做的就是照着下面的步骤做。 首先，我们导入了 Spark Streaming 类和部分从 StreamingContext 隐式转换到我们的环境的名称，目的是添加有用的方法到我们需要的其他类（如 DStream）。 StreamingContext 是所有流功能的主要入口点。我们创建了一个带有 2 个执行线程和间歇时间为 1 秒的本地 StreamingContext 。 Scala 示例： 12345678import org.apache.spark._import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._ // 自从 Spark 1.3 开始，不再是必要的了// 创建一个具有两个工作线程 (working thread) 和批次间隔为 1 秒的本地 StreamingContext//master 需要 2 个核，以防止饥饿情况 (starvation scenario)。val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")val ssc = new StreamingContext(conf, Seconds(1)) 在使用这种背景下，我们可以创建一个代表从 TCP 源流数据的离散流（DStream），指定主机名 (hostname)（例如 localhost）和端口（例如 9999）。 Scala 示例： 12// 创建一个将要连接到 hostname:port 的离散流，如 localhost:9999val lines = ssc.socketTextStream("localhost", 9999) 上一步的这个 lines 离散流（DStream）表示将要从数据服务器接收到的数据流。在这个 离散流（DStream）中的每一条记录都是一行文本（text）。接下来，我们想要通过空格字符（space characters）拆分这些数据行（lines）成单词（words）。 Scala 12// 将每一行拆分成单词val words = lines.flatMap(_.split(" ")) flatMap 是一种一对多的离散流（DStream）操作，它会通过在源离散流（source DStream）中根据每个记录（record）生成多个新纪录的形式创建一个新的离散流（DStream）。在这种情况下，在这种情况下，每一行（each line）都将被拆分成多个单词（words）和代表单词离散流（words DStream）的单词流。接下来，我们想要计算这些单词。 Scala 示例： 12345678import org.apache.spark.streaming.StreamingContext._ // 自从 Spark 1.3 不再是必要的// 计算每一个批次中的每一个单词（ Count each word in each batch）val pairs = words.map(word =&gt; (word, 1))val wordCounts = pairs.reduceByKey(_ + _)// 在控制台打印出在这个离散流（DStream）中生成的每个 RDD 的前十个元素（Print the first ten elements of each RDD generated in this DStream to the console）// 必需要触发 action（很多初学者会忘记触发 action 操作，导致报错：No output operations registered, so nothing to execute）wordCounts.print() 上一步的 words 离散流进行了进一步的映射（一对一的转变）为一个 (word, 1) 对 的离散流（DStream），这个离散流然后被规约（reduce）来获得数据中每个批次（batch）的单词频率。最后， wordCounts.print() 将会打印一些每秒生成的计数。 请注意，当这些行（lines）被执行的时候， Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理。为了在所有的转换都已经完成之后开始处理，我们在最后运行： Scala 示例： 12ssc.start() // 启动计算ssc.awaitTermination() // 等待计算的终止 完整的代码可以在 Spark Streaming 的例子 NetworkWordCount 中找到。 如果你已经 下载 并且 建立 了 Spark，你可以运行下面的例子。你首先需要运行 Netcat(一个在大多数类 Unix 系统中的小工具) 作为我们使用的数据服务器。 1$ nc -lk 9999 然后，在另一个不同的终端，你可以运行这个例子通过执行： Scala 示例： 1$ ./bin/run-example streaming.NetworkWordCount localhost 9999 然后，在运行在 netcat 服务器上的终端输入的任何行（lines），都将被计算，并且每一秒都显示在屏幕上，它看起来就像下面这样： 123456# TERMINAL 1:# Running Netcat$ nc -lk 9999hello world Scala 示例： 12345678910# TERMINAL 2: RUNNING NetworkWordCount$ ./bin/run-example streaming.NetworkWordCount localhost 9999...-------------------------------------------Time: 1357008430000 ms-------------------------------------------(hello,1)(world,1)... 基本概念接下来，我们了解完了简单的例子，开始阐述 Spark Streaming 的基本知识。 Spark Streaming 依赖与 Spark 类似， Spark Streaming 可以通过 Maven 来管理依赖。为了写你自己的 Spark Streaming 程序，你必须添加以下的依赖到你的 SBT 或者 Maven 项目中。 Maven 示例： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 对于从现在没有在 Spark Streaming 核心 API 中的数据源获取数据，如 Kafka，Flume，Kinesis，你必须添加相应的组件 spark-streaming-xyz_2.11 到依赖中。例如，有一些常见的如下。 Source Artifact Kafka spark-streaming-kafka-0-8_2.11 Flume spark-streaming-flume_2.11 Kinesis spark-streaming-kinesis-asl_2.11 [Amazon Software License] 想要查看一个实时更新的列表，请参阅 Maven repository 来了解支持的源文件和组件的完整列表。 初始化 StreamingContext为了初始化一个 Spark Streaming 程序，一个 StreamingContext 对象必须要被创建出来，它是所有的 Spark Streaming 功能的主入口点。 Scala 示例： 一个 StreamingContext 对象可以从一个 SparkConf 对象中创建出来。 Scala 12345import org.apache.spark._import org.apache.spark.streaming._val conf = new SparkConf().setAppName(appName).setMaster(master)val ssc = new StreamingContext(conf, Seconds(1)) 这个 appName 参数是展示在集群用户界面上的你的应用程序的名称。 master 是一个 Spark, Mesos or YARN cluster URL，或者一个特殊的 “local[*]” 字符串运行在本地模式下。在实践中，在一个集群上运行时，你不会想 在程序中硬编码 master，而是使用 spark-submit 启动应用程序，并且接收这个参数。然而，对于本地测试和单元测试，你可以传递 “local[*]” 去运行 Spark Streaming 过程（检测本地系统中内核的个数）。请注意，这内部创建了一个 SparkContext （所有 Spark 功能的出发点），它可以像这样被访问 ssc.sparkContext 。 这个批处理间隔（batch interval）必须根据您的应用程序和可用的集群资源的等待时间要求进行设置。详情请参阅 性能调优（Performance Tuning） 部分。 一个 StreamingContext 对象也可以从一个现有的 SparkContext 对象中创建出。 Scala 示例： 1234import org.apache.spark.streaming._val sc = ... // 现有已存在的 SparkContextval ssc = new StreamingContext(sc, Seconds(1)) 一个 context 定义之后，你必须做以下几个方面。 通过创建输入 DStreams 定义输入源。 通过应用转换和输出操作 DStreams 定义流计算（streaming computations）。 开始接收数据，并用 streamingContext.start() 处理它。 等待处理被停止（手动停止或者因为任何错误停止）使用 StreamingContext.awaitTermination() 。 该处理可以使用 streamingContext.stop() 手动停止。 要记住的要点： 一旦一个 context 已经启动，将不会有新的数据流的计算可以被创建或者添加到它。 一旦一个 context 已经停止，它不会被重新启动。 同一时间内在 JVM 中只有一个 StreamingContext 可以被激活。 在 StreamingContext 上的 stop() 同样也停止了 SparkContext 。为了只停止 StreamingContext，设置 stop() 的可选参数，名叫 stopSparkContext 为 false 。 一个 SparkContext 可以被重新用于创建多个 StreamingContexts，只要是当前的 StreamingContext 被停止（不停止 SparkContext）之前创建下一个 StreamingContext 就可以。 Discretized Streams (DStreams)（离散化流）离散化流 或者 离散流 是 Spark Streaming 提供的基本抽象。它代表了一个连续的数据流，无论是从源接收到的输入数据流，还是通过变换输入流所产生的处理过的数据流。在内部，一个离散流（DStream）被表示为一系列连续的 RDDs，RDD 是 Spark 的一个不可改变的，分布式的数据集的抽象（查看 Spark 编程指南 了解更多）。在一个 DStream 中的每个 RDD 包含来自一定的时间间隔的数据，如下图所示。 应用于 DStream 的任何操作转化为对于底层的 RDDs 的操作。例如，在 之前的例子，转换一个行（lines）流成为单词（words）中，flatMap 操作被应用于在行离散流（lines DStream）中的每个 RDD 来生成单词离散流（words DStream）的 RDDs 。如下图所示。 这些底层的 RDD 变换由 Spark 引擎（engine）计算。 DStream 操作隐藏了大多数这些细节并为了方便起见，提供给了开发者一个更高级别的 API 。这些操作细节会在后边的章节中讨论。 输入 DStreams 以及接收输入 DStreams 是代表输入数据是从流的源数据（streaming sources）接收到的流的 DStream 。在快速简单的例子 中，行（lines）是一个输入 DStream，因为它代表着从 netcat 服务器接收到的数据的流。每个输入离散流（input DStream）（除了文件流（file stream），在后面的章节进行讨论）都会与一个接收器（Scala doc，Java doc）对象联系，这个接收器对象从一个源头接收数据并且存储到 Sparks 的内存中用于处理。 Spark Streaming 提供了两种内置的流来源（streaming source）。 基本来源（Basic sources）：在 StreamingContext API 中直接可用的源（source）。例如，文件系统（file systems），和 socket 连接（socket connections）。 高级来源（Advanced sources）：就像 Kafka，Flume，Kinesis 之类的通过额外的实体类来使用的来源。这些都需要连接额外的依赖，就像在 连接 部分的讨论。 在本节的后边，我们将讨论每种类别中的现有的一些来源。 需要注意的是，如果你想要在你的流处理程序中并行的接收多个数据流，你可以创建多个输入离散流（input DStreams）（在 性能优化 部分进一步讨论）。这将创建同时接收多个数据流的多个接收器（receivers）。但需要注意，一个 Spark 的 worker/executor 是一个长期运行的任务（task），因此它将占用分配给 Spark Streaming 的应用程序的所有核中的一个核（core）。因此，要记住，一个 Spark Streaming 应用需要分配足够的核（core）（或 线程（threads），如果本地运行的话）来处理所接收的数据，以及来运行接收器（receiver(s)）。 需要记住的要点 当在本地运行一个 Spark Streaming 程序的时候，不要使用 “local” 或者 “local[1]” 作为 master 的 URL 。这两种方法中的任何一个都意味着只有一个线程将用于运行本地任务。如果你正在使用一个基于接收器（receiver）的输入离散流（input DStream）（例如， sockets，Kafka，Flume 等），则该单独的线程将用于运行接收器（receiver），而没有留下任何的线程用于处理接收到的数据。因此，在本地运行时，总是用 “local[n]” 作为 master URL，其中的 n &gt; 运行接收器的数量（查看 更多Spark 属性 来了解怎样去设置 master 的信息）。 将逻辑扩展到集群上去运行，分配给 Spark Streaming 应用程序的内核（core）的内核数必须大于接收器（receiver）的数量。否则系统将接收数据，但是无法处理它。 基本来源（Basic Sources）我们已经简单地了解过了 ssc.socketTextStream(…) 在 快速开始的例子中，例子中是从通过一个 TCP socket 连接接收到的文本数据中创建了一个离散流（DStream）。除了 sockets，StreamingContext API 也提供了根据文件作为输入来源创建离散流（DStreams）的方法。 1. 文件流（File Streams）：用于从文件中读取数据，在任何与 HDFS API 兼容的文件系统中（即 HDFS，S3，NFS 等），一个离散流（DStream）可以像下面这样创建： Scala 示例： 1streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) Spark Streaming 将监控 dataDirectory 目录，并处理任何在该目录下创建的文件（写在嵌套目录中的文件是不支持的）。注意： 文件必须具有相同的数据格式。 文件必须在 dataDirectory 目录中通过原子移动或者重命名它们到这个 dataDirectory 目录下来创建。 一旦移动，这些文件必须不能再更改，因此如果文件被连续地追加，新的数据将不会被读取。 对于简单的文本文件，还有一个更加简单的方法 streamingContext.textFileStream(dataDirectory)。并且文件流（file streams）不需要运行一个接收器（receiver），因此，不需要分配内核（core）。 Python API fileStream 在 Python API 中是不可用的，只有 textFileStream 是可用的。 2. 基于自定义的接收器（custom receivers）的流（Stream）：离散流（DStreams）可以使用通过自定义的接收器接收到的数据来创建。查看 自定义接收器指南 来了解更多细节。 3. RDDs 队列作为一个流：为了使用测试数据测试 Spark Streaming 应用程序，还可以使用 streamingContext.queueStream(queueOfRDDs) 创建一个基于 RDDs 队列的离散流（DStream），每个进入队列的 RDD 都将被视为 DStream 中的一个批次数据，并且就像一个流进行处理。想要了解更多的关于从 sockets 和文件（files）创建的流的细节，查看相关功能的 API 文档，Scala 中的 StreamingContext，Java 中的 JavaStreamingContext 和 Python 中的 StreamingContext。 高级来源（Advanced Sources）Python API 在 Spark 2.0.0 中，这些来源中， Kafka， Kinesis 和 Flume 在 Python API 中都是可用的。 这一类别的来源需要使用非 Spark 库中的外部接口，它们中的其中一些还需要比较复杂的依赖关系（例如， Kafka 和 Flume）。因此，为了最小化有关的依赖关系的版本冲突的问题，这些资源本身不能创建 DStream 的功能，它是通过 连接单独的类库实现创建 DStream 的功能。 需要注意的是这些高级来源在 Spark Shell 中是不可用的。因此，基于这些高级来源的应用程序不能在 shell 中被测试。如果你真的想要在 Spark shell 中使用它们，你必须下载带有它的依赖的相应的 Maven 组件的 JAR，并且将其添加到 classpath 。 一些高级来源如下。 Kafka ： Spark Streaming 2.0.0 与 Kafka 0.8.2.1 以及更高版本兼容。查看 Kafka 集成指南 来了解更多细节。 Flume ： Spark Streaming 2.0.0 与 Flume 1.6.0 兼容。查看 Flume 集成指南 来了解更多细节。 Kinesis ： Spark Streaming 2.0.0 与 Kinesis 客户端库 1.2.1 兼容。查看 Kinesis 集成指南 来了解更多细节。 自定义来源（Custom Sources）：Python API 在 Python 中还不支持自定义来源。 输入离散流（Input DStreams）也可以从创建自定义数据源。所有你需要做的就是实现一个用户定义（user-defined）的接收器（receiver）（查看下一章节去了解那是什么），这个接收器可以从自定义的数据源接收数据并将它推送到 Spark 。查看 自定义接收器指南（Custom Receiver Guide） 来了解更多。 接收器的可靠性（Reveiver Reliability）可以有两种基于他们的可靠性的数据源。数据源（如 Kafka 和 Flume）允许传输的数据被确认。如果系统从这些可靠的数据来源接收数据，并且被确认（acknowledges）正确地接收数据，它可以确保数据不会因为任何类型的失败而导致数据丢失。这样就出现了 2 种接收器（receivers）： 可靠的接收器（Reliable Receiver） - 当数据被接收并存储在 Spark 中并带有备份副本时，一个可靠的接收器（reliable receiver）正确地发送确认（acknowledgment）给一个可靠的数据源（reliable source）。 不可靠的接收器（Unreliable Receiver） - 一个不可靠的接收器（ unreliable receiver ）不发送确认（acknowledgment）到数据源。这可以用于不支持确认的数据源，或者甚至是可靠的数据源当你不想或者不需要进行复杂的确认的时候。 如何去编写一个可靠的接收器的细节在 自定义接收器指南（Custom Receiver Guide） 中。 DStreams 中的 Transformations和 RDD 类似，transformation 允许从输入 DStream 来的数据被修改。DStreams 支持很多在 RDD 中可用的 transformation 算子。一些常用的算子如下所示: Transformation Meaning map(func) 利用函数 func 处理原 DStream 的每个元素，返回一个新的 DStream flatMap(func) 与 map 相似，但是每个输入项可用被映射为 0 个或者多个输出项 filter(func) 返回一个新的 DStream，它仅仅包含源 DStream 中满足函数 func 的项 repartition(numPartitions) 通过创建更多或者更少的 partition 改变这个 DStream 的并行级别 (level of parallelism) union(otherStream) 返回一个新的 DStream，它包含源 DStream 和 otherStream 的联合元素 count() 通过计算源 DStream 中每个 RDD 的元素数量，返回一个包含单元素 (single-element) RDDs 的新 DStream reduce(func) 利用函数 func 聚集源 DStream 中每个 RDD 的元素，返回一个包含单元素 (single-element) RDDs 的新 DStream。函数应该是相关联的，以使计算可以并行化 countByValue() 这个算子应用于元素类型为 K 的 DStream 上，返回一个（K,long）对的新 DStream，每个键的值是在原 DStream 的每个 RDD 中的频率。 reduceByKey(func, [numTasks]) 当在一个由 (K,V) 对组成的 DStream 上调用这个算子，返回一个新的由 (K,V) 对组成的 DStream，每一个 key 的值均由给定的 reduce 函数聚集起来。注意：在默认情况下，这个算子利用了 Spark 默认的并发任务数去分组。你可以用 numTasks 参数设置不同的任务数 join(otherStream, [numTasks]) 当应用于两个 DStream（一个包含（K,V）对， 一个包含 (K,W) 对），返回一个包含 (K, (V, W)) 对的新 DStream cogroup(otherStream, [numTasks]) 当应用于两个 DStream（一个包含（K,V）对， 一个包含 (K,W) 对），返回一个包含 (K, Seq[V], Seq[W]) 的元组 transform(func) 通过对源 DStream 的每个 RDD 应用 RDD-to-RDD 函数，创建一个新的 DStream。这个可以在 DStream 中的任何 RDD 操作中使用 updateStateByKey(func) 利用给定的函数更新 DStream 的状态，返回一个新 “state” 的 DStream。 最后两 个 transformation 算子需要重点介绍一下： UpdateStateByKey 操作updateStateByKey 操作允许不断用新信息更新它的同时保持任意状态。你需要通过两步来使用它： 定义状态 - 状态可以是任何的数据类型 定义状态更新函数 - 怎样利用更新前的状态和从输入流里面获取的新值更新状态 在每个 batch 中，Spark 会使用更新状态函数为所有的关键字更新状态，不管在 batch 中是否含有新的数据。如果这个更新函数返回一个 none，这个键值对也会被消除。 让我们举个例子说明。在例子中，你想保持一个文本数据流中每个单词的运行次数，运行次数用一个 state 表示，它的类型是整数 Scala： 1234def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = &#123; val newCount = ... // add the new values with the previous running count to get the new count Some(newCount)&#125; Java： 12345678Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunction = new Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123; @Override public Optional&lt;Integer&gt; call(List&lt;Integer&gt; values, Optional&lt;Integer&gt; state) &#123; Integer newSum = ... // add the new values with the previous running count to get the new count return Optional.of(newSum); &#125; &#125;; Python： 1234def updateFunction(newValues, runningCount): if runningCount is None: runningCount = 0 return sum(newValues, runningCount) # add the new values with the previous running count to get the new count 这个 DStream 应用在单词计数上（参考之前的案例） 1val runningCounts = pairs.updateStateByKey[Int](updateFunction _) 更新函数将会被每个单词调用，newValues 拥有一系列的 1（从 (word, 1) 组对应），runningCount 拥有之前的次数。要看完整的代码，见案例 注意， 使用 updateStateByKey 需要配置的检查点的目录，这是 详细的讨论 CheckPointing 部分。 Transform 操作transform 操作（以及它的变化形式如 transformWith）允许在 DStream 运行任何 RDD-to-RDD 函数。它能够被用来应用任何没在 DStream API 中提供的 RDD 操作（It can be used to apply any RDD operation that is not exposed in the DStream API）。 例如，连接数据流中的每个批（batch）和另外一个数据集的功能并没有在 DStream API 中提供，然而你可以简单的利用 transform 方法做到。如果你想通过连接带有预先计算的垃圾邮件信息的输入数据流 来清理实时数据，然后过了它们，你可以按如下方法来做： Scala: 123456val spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) // RDD containing spam informationval cleanedDStream = wordCounts.transform(rdd =&gt; &#123; rdd.join(spamInfoRDD).filter(...) // join data stream with spam information to do data cleaning ...&#125;) Java: 1234567891011import org.apache.spark.streaming.api.java.*;// RDD containing spam informationfinal JavaPairRDD&lt;String, Double&gt; spamInfoRDD = jssc.sparkContext().newAPIHadoopRDD(...);JavaPairDStream&lt;String, Integer&gt; cleanedDStream = wordCounts.transform( new Function&lt;JavaPairRDD&lt;String, Integer&gt;, JavaPairRDD&lt;String, Integer&gt;&gt;() &#123; @Override public JavaPairRDD&lt;String, Integer&gt; call(JavaPairRDD&lt;String, Integer&gt; rdd) throws Exception &#123; rdd.join(spamInfoRDD).filter(...); // join data stream with spam information to do data cleaning ... &#125; &#125;); Python: 1234spamInfoRDD = sc.pickleFile(...) # RDD containing spam information# join data stream with spam information to do data cleaningcleanedDStream = wordCounts.transform(lambda rdd: rdd.join(spamInfoRDD).filter(...)) 请注意， 每批间隔提供的函数被调用。 这允许你做 时变抽样操作， 即抽样操作， 数量的分区， 广播变量， 批次之间等可以改变。 窗口 (window) 操作Spark Streaming 也支持窗口计算，它允许你在一个滑动窗口数据上应用 transformation 算子。下图阐明了这个滑动窗口 如上图显示，窗口在源 DStream 上滑动，合并和操作落入窗内的源 RDDs，产生窗口化的 DStream 的 RDDs。在这个具体的例子中，程序在三个时间单元的数据上进行窗口操作，并且每两个时间单元滑动一次。 这说明，任何一个窗口操作都需要指定两个参数： 窗口长度：窗口的持续时间 滑动的时间间隔：窗口操作执行的时间间隔 这两个参数必须是源 DStream 的批时间间隔的倍数。 下面举例说明窗口操作。例如，你想扩展 前面的例子 用来计算过去 30 秒的词频，间隔时间是 10 秒。为了达到这个目的，我们必须在过去 30 秒的 pairs DStream 上应用 reduceByKey 操作。用方法 reduceByKeyAndWindow 实现。 Scala: 12// Reduce last 30 seconds of data, every 10 secondsval windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b), Seconds(30), Seconds(10)) Java: 123456789// Reduce function adding two integers, defined separately for clarityFunction2&lt;Integer, Integer, Integer&gt; reduceFunc = new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125;&#125;;// Reduce last 30 seconds of data, every 10 secondsJavaPairDStream&lt;String, Integer&gt; windowedWordCounts = pairs.reduceByKeyAndWindow(reduceFunc, Durations.seconds(30), Durations.seconds(10)); Python: 12# Reduce last 30 seconds of data, every 10 secondswindowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10) 一些常用的窗口操作如下所示，这些操作都需要用到上文提到的两个参数：窗口长度和滑动的时间间隔 转换 意义 window (windowLength , slideInterval) 返回一个新的 DStream 计算基于窗口的批 DStream 来源。 countByWindow (windowLength , slideInterval) 返回一个滑动窗口中的元素计算流。 reduceByWindow (func, windowLength , slideInterval) 返回一个新创建的单个元素流， 通过聚合元素流了 滑动时间间隔使用 函数 。 函数应该关联和交换， 以便它可以计算 正确地并行执行。 reduceByKeyAndWindow (func, windowLength , slideInterval , ( numTasks]) 当呼吁 DStream(K、V) 对， 返回一个新的 DStream(K、V) 对每个键的值在哪里聚合使用给定的 reduce 函数 函数 在一个滑动窗口批次。 注意: 默认情况下， 它使用引发的默认数量 并行任务 (2 为本地模式， 在集群模式是由配置数量 财产 spark.default.parallelism 分组)。 你可以通过一个可选的 numTasks 参数设置不同数量的任务。 reduceByKeyAndWindow (func, invFunc , windowLength , slideInterval ,( numTasks]) 上面的 reduceByKeyAndWindow() 的一个更有效的版本，其中每个窗口的 reduce 值是使用上一个窗口的 reduce 值递增计算的。 这是通过减少进入滑动窗口的新数据和 “反向减少” 离开窗口的旧数据来完成的。 一个例子是在窗口滑动时 “添加” 和 “减去” 键的计数。 然而，它仅适用于 “可逆缩减函数”，即，具有对应的 “逆缩减” 函数（作为参数 invFunc）的那些缩减函数。 像 reduceByKeyAndWindow 中一样，reduce 任务的数量可通过可选参数进行配置。 请注意，必须启用 检查点 设置才能使用此操作。 countByValueAndWindow (windowLength , slideInterval ,[ numTasks]) 当呼吁 DStream(K、V) 对， 返回一个新的 DStream(K, 长) 对的 每个键的值是它的频率在一个滑动窗口。 就像在 reduceByKeyAndWindow，通过一个减少任务的数量是可配置的 可选参数。 连接操作最后， Spark streaming 可以连接其它的数据源 Stream-stream 连接stream 可以很容易与其他 stream 连接 Scala: 123val stream1: DStream[String, String] = ...val stream2: DStream[String, String] = ...val joinedStream = stream1.join(stream2) Java: 123JavaPairDStream&lt;String, String&gt; windowedStream1 = stream1.window(Durations.seconds(20));JavaPairDStream&lt;String, String&gt; windowedStream2 = stream2.window(Durations.minutes(1));JavaPairDStream&lt;String, Tuple2&lt;String, String&gt;&gt; joinedStream = windowedStream1.join(windowedStream2); Python: 123stream1 = ...stream2 = ...joinedStream = stream1.join(stream2) 在每批间隔， 生成的抽样 stream1 将与生成的抽样 stream2。 也可以做 leftOuterJoin，rightOuterJoin，fullOuterJoin。 此外， 它通常是非常有用的做连接的窗口 (window) stream。 这是非常容易的。 也就是说： 时间间隔一致，统计不同时间窗口的 2 个 Dstream 情况。 随便举例，例如： 当前时间点，统计前 10s 内的订单 关联 20s 内的点击情况。 计算最近的 3 个点击。 Scala: 123val windowedStream1 = stream1.window(Seconds(20))val windowedStream2 = stream2.window(Minutes(1))val joinedStream = windowedStream1.join(windowedStream2) Java: 123JavaPairDStream&lt;String, String&gt; windowedStream1 = stream1.window(Durations.seconds(20));JavaPairDStream&lt;String, String&gt; windowedStream2 = stream2.window(Durations.minutes(1));JavaPairDStream&lt;String, Tuple2&lt;String, String&gt;&gt; joinedStream = windowedStream1.join(windowedStream2); Python: 123windowedStream1 = stream1.window(20)windowedStream2 = stream2.window(60)joinedStream = windowedStream1.join(windowedStream2) Stream-dataset 连接这已经被证明在早些时候解释 DStream.transform 操作。 这是另一个例子， 加入一个有窗口的流数据集。 Scala: 123val dataset: RDD[String, String] = ...val windowedStream = stream.window(Seconds(20))...val joinedStream = windowedStream.transform &#123;rdd =&gt; rdd.join(dataset) &#125; Java: 12345678910JavaPairRDD&lt;String, String&gt; dataset = ...JavaPairDStream&lt;String, String&gt; windowedStream = stream.window(Durations.seconds(20));JavaPairDStream&lt;String, String&gt; joinedStream = windowedStream.transform( new Function&lt;JavaRDD&lt;Tuple2&lt;String, String&gt;&gt;, JavaRDD&lt;Tuple2&lt;String, String&gt;&gt;&gt;() &#123; @Override public JavaRDD&lt;Tuple2&lt;String, String&gt;&gt; call(JavaRDD&lt;Tuple2&lt;String, String&gt;&gt; rdd) &#123; return rdd.join(dataset); &#125; &#125;); Python: 123dataset = ... # some RDDwindowedStream = stream.window(20)joinedStream = windowedStream.transform(lambda rdd: rdd.join(dataset)) 事实上， 你也可以动态地改变你想加入的数据集。 提供的函数 变换 评估每批间隔， 因此将使用当前数据集 作为参考点。DStream 转换的完整列表可以在 API 文档。 Scala API， 看到 DStream 和 PairDStreamFunctions 。 Java API， 明白了 JavaDStream 和 JavaPairDStream 。 Python API， 看到 DStream 。 DStreams 上的输出操作输出操作允许 DStream 的操作推到如数据库、文件系统等外部系统中。因为输出操作实际上是允许外部系统消费转换后的数据，它们触发的实际操作是 DStream 转换。目前，定义了下面几种输出操作： Output Operation Meaning print() 在 DStream 的每个批数据中打印前 10 条元素，这个操作在开发和调试中都非常有用。在 Python API 中调用 pprint()。 saveAsObjectFiles(prefix, [suffix]) 保存 DStream 的内容为一个序列化的文件 SequenceFile。每一个批间隔的文件的文件名基于 prefix 和 suffix 生成。”prefix-TIME_IN_MS[.suffix]”，在 Python API 中不可用。 saveAsTextFiles(prefix, [suffix]) 保存 DStream 的内容为一个文本文件。每一个批间隔的文件的文件名基于 prefix 和 suffix 生成。”prefix-TIME_IN_MS[.suffix]” saveAsHadoopFiles(prefix, [suffix]) 保存 DStream 的内容为一个 hadoop 文件。每一个批间隔的文件的文件名基于 prefix 和 suffix 生成。”prefix-TIME_IN_MS[.suffix]”，在 Python API 中不可用。 foreachRDD(func) 在从流中生成的每个 RDD 上应用函数 func 的最通用的输出操作。这个函数应该推送每个 RDD 的数据到外部系统，例如保存 RDD 到文件或者通过网络写到数据库中。需要注意的是，func 函数在驱动程序中执行，并且通常都有 RDD action 在里面推动 RDD 流的计算。 提示注意：在 foreachRDD(func) 的 func 中可以输出到 hdfs， 即使用 rdd.saveAsHadoopFileF &lt;: OutputFormat[K, V] 操作 foreachRDD 设计模式的使用dstream.foreachRDD 是一个强大的原生语法，发送数据到外部系统中。然而，明白怎样正确地、有效地用这个原生语法是非常重要的。下面几点介绍了如何避免一般错误。经常写数据到外部系统需要建一个连接对象（例如到远程服务器的 TCP 连接），用它发送数据到远程系统。为了达到这个目的，开发人员可能不经意的在 Spark 驱动中创建一个连接对象，但是在 Spark worker 中 尝试调用这个连接对象保存记录到 RDD 中，如下: Scala: 1234567dstream.foreachRDD &#123;rdd =&gt; rdd.foreach &#123;record =&gt; val connection = createNewConnection() connection.send(record) connection.close() &#125;&#125; Python: 123456def sendRecord(record): connection = createNewConnection() connection.send(record) connection.close()dstream.foreachRDD(lambda rdd: rdd.foreach(sendRecord)) 这是不正确的，因为这需要先序列化连接对象，然后将它从 driver 发送到 worker 中。这样的连接对象在机器之间不能传送。它可能表现为序列化错误（连接对象不可序列化）或者初始化错误（连接对象应该 在 worker 中初始化）等等。正确的解决办法是在 worker 中创建连接对象。 最后，这可以进一步通过再利用在多个 RDDS / 批次的连接对象进行了优化。可以保持连接对象的静态池比可以为多个批次 RDDS 被推到外部系统，从而进一步降低了开销被重用。 然而，这会造成另外一个常见的错误 - 为每一个记录创建了一个连接对象。例如： Scala: 12345678dstream.foreachRDD &#123;rdd =&gt; rdd.foreachPartition &#123;partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) ConnectionPool.returnConnection(connection) // return to the pool for future reuse &#125;&#125; Python: 123456789def sendPartition(iter): # ConnectionPool is a static, lazily initialized pool of connections connection = ConnectionPool.getConnection() for record in iter: connection.send(record) # return to the pool for future reuse ConnectionPool.returnConnection(connection)dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition)) 需要注意的是，池中的连接对象应该根据需要延迟创建，并且在空闲一段时间后自动超时。这样就获取了最有效的方式发生数据到外部系统。其它需要注意的地方： 输出操作通过懒执行的方式操作 DStream，正如 RDD action 通过懒执行的方式操作 RDD。具体地看，RDD action 和 DStream 输出操作接收数据的处理。因此，如果你的应用程序没有任何输出操作或者 用于输出操作 dstream.foreachRDD()，但是没有任何 RDD action 操作在 dstream.foreachRDD() 里面，那么什么也不会执行。系统仅仅会接收输入，然后丢弃它们。 默认情况下，DStreams 输出操作是分时执行的，它们按照应用程序的定义顺序按序执行 DataFrame 和 SQL 操作你可以很容易地使用 DataFrames 和 SQL Streaming 操作数据。 需要使用 SparkContext 或者正在使用的 StreamingContext 创建一个 SparkSession。 这样做的目的就是为了使得驱动程序可以在失败之后进行重启。 使用懒加载模式创建单例的 SparkSession 对象。 下面的示例所示。 在原先的 单词统计 程序的基础上进行修改，使用 DataFrames 和 SQL 生成 单词统计。 每个 RDD 转换为 DataFrame， 注册为临时表， 然后使用 SQL 查询。 Scala(源码): 123456789101112131415161718192021/** 流程序中的 DataFrame 操作 */val words: DStream[String] = ...words.foreachRDD &#123;rdd =&gt; // 获取单例的 SQLContext val sqlContext = SQLContext.getOrCreate(rdd.sparkContext) import sqlContext.implicits._ // 将 RDD [String] 转换为 DataFrame val wordsDataFrame = rdd.toDF("word") // 注册临时表 wordsDataFrame.registerTempTable("words") // 在 DataFrame 上使用 SQL 进行字计数并打印它 val wordCountsDataFrame = sqlContext.sql("select word, count(*) as total from words group by word") wordCountsDataFrame.show()&#125; Java(源码): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** Java Bean class for converting RDD to DataFrame */public class JavaRow implements java.io.Serializable &#123; private String word; public String getWord() &#123; return word; &#125; public void setWord(String word) &#123; this.word = word; &#125;&#125;.../** DataFrame operations inside your streaming program */JavaDStream&lt;String&gt; words = ...words.foreachRDD( new Function2&lt;JavaRDD&lt;String&gt;, Time, Void&gt;() &#123; @Override public Void call(JavaRDD&lt;String&gt; rdd, Time time) &#123; // Get the singleton instance of SQLContext SQLContext sqlContext = SQLContext.getOrCreate(rdd.context()); // Convert RDD[String] to RDD[case class] to DataFrame JavaRDD&lt;JavaRow&gt; rowRDD = rdd.map(new Function&lt;String, JavaRow&gt;() &#123; public JavaRow call(String word) &#123; JavaRow record = new JavaRow(); record.setWord(word); return record; &#125; &#125;); DataFrame wordsDataFrame = sqlContext.createDataFrame(rowRDD, JavaRow.class); // Register as table wordsDataFrame.registerTempTable("words"); // Do word count on table using SQL and print it DataFrame wordCountsDataFrame = sqlContext.sql("select word, count(*) as total from words group by word"); wordCountsDataFrame.show(); return null; &#125; &#125;); Python(源码): 1234567891011121314151617181920212223242526272829303132# Lazily instantiated global instance of SQLContextdef getSqlContextInstance(sparkContext): if ('sqlContextSingletonInstance' not in globals()): globals()['sqlContextSingletonInstance'] = SQLContext(sparkContext) return globals()['sqlContextSingletonInstance']...# DataFrame operations inside your streaming programwords = ... # DStream of stringsdef process(time, rdd): print("========= %s =========" % str(time)) try: # Get the singleton instance of SQLContext sqlContext = getSqlContextInstance(rdd.context) # Convert RDD[String] to RDD[Row] to DataFrame rowRdd = rdd.map(lambda w: Row(word=w)) wordsDataFrame = sqlContext.createDataFrame(rowRdd) # Register as table wordsDataFrame.registerTempTable("words") # Do word count on table using SQL and print it wordCountsDataFrame = sqlContext.sql("select word, count(*) as total from words group by word") wordCountsDataFrame.show() except: passwords.foreachRDD(process) 你可以在使用其他线程读取的流数据上进行 SQL 查询（就是说，可以异步运行 StreamingContext）。 只要确保 StreamingContext 可以缓存一定量的数据来满足查询的需求。 否则 StreamingContext， 检测不到任何异步 SQL 查询， 在完成查询之前将删除旧的数据。 例如， 如果您想查询最后一批， 但您的查询可以运行需要 5 分钟， 然后调用 streamingContext.remember(Minutes(5)) (在 Scala 中， 或其他语言)。访问 DataFrames 和 SQL 了解更多关于 DataFrames 指南。 MLlib 操作你还可以轻松地使用所提供的机器学习算法 MLlib 。 首先这些 streaming，(如机器学习算法。 Streaming 线性回归，StreamingKMeans 等) 可以同时学习 Streaming 数据的应用模型。 除了这些， 对于一个大得多的机器学习算法， 可以学习模型离线 (即使用历史数据)， 然后应用该模型在线流媒体数据。 访问 MLlib 指南更多细节。 缓存 / 持久性类似于抽样， DStreams 还允许开发人员持久化 stream 数据在内存中。 也就是说， 使用 persist() 方法 DStream， DStream 会自动把每个抽样持续化到内存中 。 这个非常有用， 如果数据多次 DStream(如同样的数据进行多次操作)。 像 reduceByWindow、 reduceByKeyAndWindow 和 updateStateByKey 这些都隐式开启了 “persist()”。 因此， DStreams 生成的窗口操作会自动保存在内存中， 如果没有开发人员调用 persist() 。 对于通过网络接收数据的输入流 (如 Kafka、Flume、Sockets 等)， 默认的持久性级别被设置为复制两个节点的数据容错。注意， 与抽样不同， 默认的序列化数据持久性 DStreams。 这是进一步讨论的 性能调优 部分。 更多的不同的持久性信息中可以通过 Spark 编程指南 找到标准。 CheckPointing一个 Streaming 应用程序必须全天候运行，所有必须能够解决应用程序逻辑无关的故障（如系统错误，JVM 崩溃等）。为了使这成为可能，Spark Streaming 需要 checkpoint 足够的信息到容错存储系统中， 以使系统从故障中恢复。 Metadata checkpointing : 保存流计算的定义信息到容错存储系统如 HDFS 中。这用来恢复应用程序中运行 worker 的节点的故障。元数据包括 Configuration : 创建 Spark Streaming 应用程序的配置信息 DStream operations : 定义 Streaming 应用程序的操作集合 Incomplete batches : 操作存在队列中的未完成的批 Data checkpointing : 保存生成的 RDD 到可靠的存储系统中，这在有状态 transformation（如结合跨多个批次的数据）中是必须的。在这样一个 transformation 中，生成的 RDD 依赖于之前批的 RDD，随着时间的推移，这个依赖链的长度会持续增长。在恢复的过程中，为了避免这种无限增长。有状态的 transformation 的中间 RDD 将会定时地存储到可靠存储系统中，以截断这个依赖链。 元数据 checkpoint 主要是为了从 driver 故障中恢复数据。如果 transformation 操作被用到了，数据 checkpoint 即使在简单的操作中都是必须的。 启用 CheckPointing应用程序在下面两种情况下必须开启 checkpoint 使用有状态的 transformation。如果在应用程序中用到了 updateStateByKey 或者 reduceByKeyAndWindow，checkpoint 目录必需提供用以定期 checkpoint RDD。 从运行应用程序的 driver 的故障中恢复过来。使用元数据 checkpoint 恢复处理信息。 注意，没有前述的有状态的 transformation 的简单流应用程序在运行时可以不开启 checkpoint。在这种情况下，从 driver 故障的恢复将是部分恢复（接收到了但是还没有处理的数据将会丢失）。 这通常是可以接受的，许多运行的 Spark Streaming 应用程序都是这种方式。 如何配置 CheckPointing在容错、可靠的文件系统（HDFS、s3 等）中设置一个目录用于保存 checkpoint 信息。着可以通过 streamingContext.checkpoint(checkpointDirectory) 方法来做。这运行你用之前介绍的 有状态 transformation。另外，如果你想从 driver 故障中恢复，你应该以下面的方式重写你的 Streaming 应用程序。 当应用程序是第一次启动，新建一个 StreamingContext，启动所有 Stream，然后调用 start() 方法 当应用程序因为故障重新启动，它将会从 checkpoint 目录 checkpoint 数据重新创建 StreamingContext Scala： 12345678910111213141516171819// Function to create and setup a new StreamingContextdef functionToCreateContext(): StreamingContext = &#123; val ssc = new StreamingContext(...) // new context val lines = ssc.socketTextStream(...) // create DStreams ... ssc.checkpoint(checkpointDirectory) // set checkpoint directory ssc&#125;// Get StreamingContext from checkpoint data or create a new oneval context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)// Do additional setup on context that needs to be done,// irrespective of whether it is being started or restartedcontext. ...// Start the contextcontext.start()context.awaitTermination() Java: 123456789101112131415161718192021// Create a factory object that can create and setup a new JavaStreamingContextJavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() &#123; @Override public JavaStreamingContext create() &#123; JavaStreamingContext jssc = new JavaStreamingContext(...); // new context JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...); // create DStreams ... jssc.checkpoint(checkpointDirectory); // set checkpoint directory return jssc; &#125;&#125;;// Get JavaStreamingContext from checkpoint data or create a new oneJavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);// Do additional setup on context that needs to be done,// irrespective of whether it is being started or restartedcontext. ...// Start the contextcontext.start();context.awaitTermination(); Python: 12345678910111213141516171819# Function to create and setup a new StreamingContextdef functionToCreateContext(): sc = SparkContext(...) # new context ssc = new StreamingContext(...) lines = ssc.socketTextStream(...) # create DStreams ... ssc.checkpoint(checkpointDirectory) # set checkpoint directory return ssc# Get StreamingContext from checkpoint data or create a new onecontext = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)# Do additional setup on context that needs to be done,# irrespective of whether it is being started or restartedcontext. ...# Start the contextcontext.start()context.awaitTermination() 如果 checkpointDirectory 存在，上下文将会利用 checkpoint 数据重新创建。如果这个目录不存在，将会调用 functionToCreateContext 函数创建一个新的上下文，建立 DStream。 请看 RecoverableNetworkWordCount 例子。 除了使用 getOrCreate，开发者必须保证在故障发生时，driver 处理自动重启。只能通过部署运行应用程序的基础设施来达到该目的。在部署章节将有更进一步的讨论。 注意，RDD 的 checkpointing 有存储成本。这会导致批数据（包含的 RDD 被 checkpoint）的处理时间增加。因此，需要小心的设置批处理的时间间隔。在最小的批容量 (包含 1 秒的数据) 情况下，checkpoint 每批数据会显著的减少 操作的吞吐量。相反，checkpointing 太少会导致谱系以及任务大小增大，这会产生有害的影响。因为有状态的 transformation 需要 RDD checkpoint。默认的间隔时间是批间隔时间的倍数，最少 10 秒。它可以通过 dstream.checkpoint 来设置。典型的情况下，设置 checkpoint 间隔是 DStream 的滑动间隔的 5-10 大小是一个好的尝试。 累加器，广播变量和 Checkpoints累加器 和 广播变量 是不能从 Spark Streaming 中恢复。 如果要使 checkpoint 可用，累加器 或 广播变量，需要使用懒加载的方式实例化这两种变量以至于他们能够可以在驱动程序失败重启之后进行再次实例化。 下面的示例所示。 Scala（源码）: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849object WordBlacklist &#123; @volatile private var instance: Broadcast[Seq[String]] = null def getInstance(sc: SparkContext): Broadcast[Seq[String]] = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; val wordBlacklist = Seq("a", "b", "c") instance = sc.broadcast(wordBlacklist) &#125; &#125; &#125; instance &#125;&#125;object DroppedWordsCounter &#123; @volatile private var instance: Accumulator[Long] = null def getInstance(sc: SparkContext): Accumulator[Long] = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; instance = sc.accumulator(0L, "WordsInBlacklistCounter") &#125; &#125; &#125; instance &#125;&#125;wordCounts.foreachRDD((rdd: RDD[(String, Int)], time: Time) =&gt; &#123; // Get or register the blacklist Broadcast val blacklist = WordBlacklist.getInstance(rdd.sparkContext) // Get or register the droppedWordsCounter Accumulator val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext) // Use blacklist to drop words and use droppedWordsCounter to count them val counts = rdd.filter &#123;case (word, count) =&gt; if (blacklist.value.contains(word)) &#123; droppedWordsCounter += count false &#125; else &#123; true &#125; &#125;.collect() val output = "Counts at time" + time + " " + counts&#125;) Java（源码）: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class JavaWordBlacklist &#123; private static volatile Broadcast&lt;List&lt;String&gt;&gt; instance = null; public static Broadcast&lt;List&lt;String&gt;&gt; getInstance(JavaSparkContext jsc) &#123; if (instance == null) &#123; synchronized (JavaWordBlacklist.class) &#123; if (instance == null) &#123; List&lt;String&gt; wordBlacklist = Arrays.asList("a", "b", "c"); instance = jsc.broadcast(wordBlacklist); &#125; &#125; &#125; return instance; &#125;&#125;class JavaDroppedWordsCounter &#123; private static volatile Accumulator&lt;Integer&gt; instance = null; public static Accumulator&lt;Integer&gt; getInstance(JavaSparkContext jsc) &#123; if (instance == null) &#123; synchronized (JavaDroppedWordsCounter.class) &#123; if (instance == null) &#123; instance = jsc.accumulator(0, "WordsInBlacklistCounter"); &#125; &#125; &#125; return instance; &#125;&#125;wordCounts.foreachRDD(new Function2&lt;JavaPairRDD&lt;String, Integer&gt;, Time, Void&gt;() &#123; @Override public Void call(JavaPairRDD&lt;String, Integer&gt; rdd, Time time) throws IOException &#123; // Get or register the blacklist Broadcast final Broadcast&lt;List&lt;String&gt;&gt; blacklist = JavaWordBlacklist.getInstance(new JavaSparkContext(rdd.context())); // Get or register the droppedWordsCounter Accumulator final Accumulator&lt;Integer&gt; droppedWordsCounter = JavaDroppedWordsCounter.getInstance(new JavaSparkContext(rdd.context())); // Use blacklist to drop words and use droppedWordsCounter to count them String counts = rdd.filter(new Function&lt;Tuple2&lt;String, Integer&gt;, Boolean&gt;() &#123; @Override public Boolean call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123; if (blacklist.value().contains(wordCount._1())) &#123; droppedWordsCounter.add(wordCount._2()); return false; &#125; else &#123; return true; &#125; &#125; &#125;).collect().toString(); String output = "Counts at time" + time + " " + counts; &#125;&#125; Python（源码）: 123456789101112131415161718192021222324252627def getWordBlacklist(sparkContext): if ('wordBlacklist' not in globals()): globals()['wordBlacklist'] = sparkContext.broadcast(["a", "b", "c"]) return globals()['wordBlacklist']def getDroppedWordsCounter(sparkContext): if ('droppedWordsCounter' not in globals()): globals()['droppedWordsCounter'] = sparkContext.accumulator(0) return globals()['droppedWordsCounter']def echo(time, rdd): # Get or register the blacklist Broadcast blacklist = getWordBlacklist(rdd.context) # Get or register the droppedWordsCounter Accumulator droppedWordsCounter = getDroppedWordsCounter(rdd.context) # Use blacklist to drop words and use droppedWordsCounter to count them def filterFunc(wordCount): if wordCount[0] in blacklist.value: droppedWordsCounter.add(wordCount[1]) False else: True counts = "Counts at time %s %s" % (time, rdd.filter(filterFunc).collect())wordCounts.foreachRDD(echo) 应用程序部署本节讨论部署 Spark Streaming 应用程序的步骤。 要求要运行一个 Spark Streaming 应用，你需要有以下几点。 有管理器的集群 - 这是任何 Spark 应用程序都需要的需求，详见 部署指南。 将应用程序打为 jar 包 - 你必须编译你的应用程序为 jar 包。如果你用 spark-submit 启动应用程序，你不需要将 Spark 和 Spark Streaming 打包进这个 jar 包。 如果你的应用程序用到了 高级源（如 kafka，flume），你需要将它们关联的外部 artifact 以及它们的依赖打包进需要部署的应用程序 jar 包中。例如，一个应用程序用到了 TwitterUtils，那么就需要将 spark-streaming-twitter_2.10 以及它的所有依赖打包到应用程序 jar 中。 为 executors 配置足够的内存 - 因为接收的数据必须存储在内存中，executors 必须配置足够的内存用来保存接收的数据。注意，如果你正在做 10 分钟的窗口操作，系统的内存要至少能保存 10 分钟的数据。所以，应用程序的内存需求依赖于使用 它的操作。 配置 checkpointing - 如果 stream 应用程序需要 checkpointing，然后一个与 Hadoop API 兼容的容错存储目录必须配置为检查点的目录，流应用程序将 checkpoint 信息写入该目录用于错误恢复。更多信息见 checkpointing 配置应用程序 driver 的自动重启 - 为了自动从 driver 故障中恢复，运行流应用程序的部署设施必须能监控 driver 进程，如果失败了能够重启它。不同的 集群管理器，有不同的工具得到该功能 Spark Standalone：一个 Spark 应用程序 driver 可以提交到 Spark 独立集群 运行，也就是说 driver 运行在一个 worker 节点上。进一步来看，独立的集群管理器能够被指示用来监控 driver，并且在 driver 失败（或者是由于非零的退出代码如 exit(1)， 或者由于运行 driver 的节点的故障）的情况下重启 driver。更多信息见 Spark Standalone guide YARN：YARN 为自动重启应用程序提供了类似的机制。 Mesos： Mesos 可以用 Marathon 提供该功能 配置 write ahead logs - 在 Spark 1.2 中，为了获得极强的容错保证，我们引入了一个新的实验性的特性 - 预写日志（write ahead logs）。如果该特性开启，从 receiver 获取的所有数据会将预写日志写入配置的 checkpoint 目录。 这可以防止 driver 故障丢失数据，从而保证零数据丢失。这个功能可以通过设置配置参数 spark.streaming.receiver.writeAheadLogs.enable 为 true 来开启。然而，这些较强的语义可能以 receiver 的接收吞吐量为代价。这可以通过 并行运行多个 receiver 增加吞吐量来解决。另外，当预写日志开启时，Spark 中的复制数据的功能推荐不用，因为该日志已经存储在了一个副本在存储系统中。可以通过设置输入 DStream 的存储级别为 StorageLevel.MEMORY_AND_DISK_SER 获得该功能。 升级应用程序代码如果运行的 Spark Streaming 应用程序需要升级，有两种可能的方法 启动升级的应用程序，使其与未升级的应用程序并行运行。一旦新的程序（与就程序接收相同的数据）已经准备就绪，旧的应用程序就可以关闭。这种方法支持将数据发送到两个不同的目的地（新程序一个，旧程序一个） 首先，平滑的关闭（StreamingContext.stop(…) 或 JavaStreamingContext.stop(…)）现有的应用程序。在关闭之前，要保证已经接收的数据完全处理完。然后，就可以启动升级的应用程序，升级 的应用程序会接着旧应用程序的点开始处理。这种方法仅支持具有源端缓存功能的输入源（如 flume，kafka），这是因为当旧的应用程序已经关闭，升级的应用程序还没有启动的时候，数据需要被缓存。 监控应用程序除了 Spark 自己的 监控功能 之外，针对 Spark Streaming 它也有一些其他的功能。当使用 StreamingContext 时，Spark Web UI 显示了一个额外的 Streaming 标签，它显示了有关正在运行的 Receivers（接收器）（是否 Receivers 处于活动状态，记录接受数量，接收器误差，等等）和已完成的 Batches（批处理时间，查询延迟，等等）的统计信息。这可以用于监控 Spark 应用程序的进度。 在 Web UI 中以下两个指标特别重要 : Processing Time（处理时间） - 用来处理每批数据的时间。 Scheduling Delay（调度延迟） - 一批数据在队列中等待，直到上一批数据处理完成所需的时间。 如果批处理时间始终大于批的间隔 和 / 或者 队列延迟不断增加，那么说明系统不能尽可能快的处理 Batches，它们（Batches）正在被生成并且落后（处理），在这种情况下，可以考虑 reduce 批处理时间。 Spark Streaming 应用程序处理的进度，也可以使用 StreamingListener 接口监控，它允许你获取 Receiver 的状态以及批处理时间。注意，这是一个开发者 API 并且在将来它很可能被改进（即上报更多的信息）。 性能优化为了在集群中获得 Spark Streaming 应用程序的最佳性能需要一些优化。这部分解释了一部分能够调整用来提升您应用程序性能的参数和配置。在一个较高的水平上，您需要考虑两件事情 : 通过有效的利用群集资源来减少每批数据的处理时间。 设置正确的 Batch 大小，这样的话当它们被接受时 Batch 数据能够被尽量快的处理（换言之，数据处理能够赶得上数据摄取）。 降低批处理的时间有许多优化可以在 Spark 中来完成，使每批数据的处理时间最小化。这些都在 优化指南 中详细讨论。本章介绍一些最重要的问题。 数据接收的并行级别通过网络（像 Kafka，Flume，socket，等等）接受数据需要数据反序列化然后存在 Spark 中。如果数据接收成为了系统中的瓶颈，则需要考虑并行的数据接收。注意，每个 Input DStream（输入流）创建一个接受单个数据流的单独的 Receiver（接收器）（运行在一个 Worker 机器上）。接受多个数据流因此可以通过创建多个 Input DStreams 以及配置他们去从数据源（S）的不同分区接收数据流来实现。例如，一个单一的 Kafka Input DStream 接收两个 Topic（主题） 的数据能够被拆分成两个 Kafka 输入流，每个仅接收一个 Topic。这将运行两个 Receiver（接收器），使得数据可以被并行接受，因此将提高整体吞吐量。这些多个 DStream 可以合并在一起以创建一个单独的 DStream。然后被用于在一个单独的 Input DStream 上的 Transformations（转换）可以在统一的流上被应用。按照以下步骤进行 : Scala : 1234val numStreams = 5val kafkaStreams = (1 to numStreams).map &#123; i =&gt; KafkaUtils.createStream(...) &#125;val unifiedStream = streamingContext.union(kafkaStreams)unifiedStream.print() Java : 1234567int numStreams = 5;List&lt;JavaPairDStream&lt;String, String&gt;&gt; kafkaStreams = new ArrayList&lt;&gt;(numStreams);for (int i = 0; i &lt; numStreams; i++) &#123; kafkaStreams.add(KafkaUtils.createStream(...));&#125;JavaPairDStream&lt;String, String&gt; unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));unifiedStream.print(); Python : 1234numStreams = 5kafkaStreams = [KafkaUtils.createStream(...) for _ in range (numStreams)]unifiedStream = streamingContext.union(*kafkaStreams)unifiedStream.pprint() 应当考虑的另一个参数是 Receiver（接收器）的阻塞间隔，它通 过 配置参数 spark.streaming.blockInterval 来决定。对于大部分 Receiver（接收器） 来说，在存储到 Spark 的 Memory（内存）之前时接收的数据被合并成数据 Block（块）。每批 Block（块）的数量确定了将用于处理在一个类似 Map Transformation（转换）的接收数据的任务数量。每个 Receiver（接收器）的每个 Batch 的任务数量大约为（Batch 间隔 / Block 间隔）。例如，200ms 的 Block（块）间隔和每 2s 的 Batch 将创建 10 个任务。如果任务的数量过低（即，小于每台机器的 Core（CPU）数量），那么效率会很低，因为所有可用的 Core（CPU）将不会用来处理数据。以增加给定的 Batch 间隔的任务数量，降低该 Block（间隔）。然而，Block（块）间隔推荐的最低值约为 50ms，低于该推荐值的任务的运行开销可能是一个问题。 与多个 Input Streams / Receivers 接受数据的另一种方法是明确的 Repartition （重分区）输入的数据流（使用 inputStream.repartition（））。这样在进一步处理之前就分发接收数据的 Batch 到群集中指定数量的机器上去。 数据处理的并行级别集群资源的利用率可能会很低，如果在任何计算阶段中并行任务数量的不是很多的话。例如，对于像 reduceByKey 和 reduceByKeyAndWindow 这样的分布式 Reduce 操作来说，默认的并行任务数量由 spark.default.parallelism 配置属性 控制。您可以传递并行的的参数（请看 PairDStreamFunctions 文档），或者设置 spark.default.parallelism 配置属性 来改变默认值。 数据序列化可以通过调整序列化的格式来减少数据序列化的开销。在流式传输的情况下，有两种数据类型会被序列化。 Input Data（输入的数据）: 默认情况下，通过 Receivers（接收器）接收的输入数据被存储在 Executor 的内存与 StorageLevel.MEMORY_AND_DISK_SER_2 中。也就是说，数据被序列化成 Bytes（字节）以降低 GC 开销，以及被复制用于 Executor 的失败容错。此外，数据首先保存在 Memory（内存）中，如果内存不足已容纳所有用于流计算的输入数据将被溢出到硬盘上。这个序列化操作显然也需要开销 - Receiver（接收器）必须反序列化接收到的数据并且使用 Spark 的序列化格式重新序列化它们。 通过 Streaming 操作产生的持久的 RDDs : 通过流计算产生的 RDDs 可能被持久化在内存中。例如，Window（窗口）操作将数据持久化在内存中，因为他们可能被多次处理。然而，不像 Spark Core 默认的 StorageLevel.MEMORY_ONLY，通过流计产生的持久化 RDDs 被使用 StorageLevel.MEMORY_ONLY_SER（也就是序列化）存储，默认 GC 开销降至最低。 在这两种情况下，使用 Kryo 序列化能够减少 CPU 和 内存的开销。更多细节请看 Spark 优化指南。对于 Kyro 来说，考虑注册自定义的 Class，并且禁用 Object（对象）引用跟踪（在 配置指南 中看 Kyro 相关的配置）。 在特定的情况下，需要用于保留 Streaming 应用程序的数据量不是很大，这样也许是可行的，来保存反序列化的数据（两种类型）不用引起过度的 GC 开销。例如，如果您使用 Batch 的间隔有几秒钟并且没有 Window（窗口）操作，然后你可以通过显式地设置存储相应的级别来尝试禁用序列化保存数据。这将减少由于序列化的 CPU 开销，可能不需要太多的 GC 开销就能提升性能。 任务启动开销如果每秒任务启动的数据很高（比如，每秒 50 个任务或者更多），那么发送 Task（任务） 到 Slave 的负载可能很大，将很难实现 亚秒级 延迟。可以通过下面的改变来降低负载 : Execution Mode（运行模式）: 以 Standlone 模式或者 coarse-grained（粗粒度的） Mesos 模式运行 Spark 会比 fine-grained（细粒度的）Mesos 模式运行 Spark 获得更佳的任务启动时间。更详细的信息请参考 Mesos 运行指南。 这些改变也许能减少批处理的时间（100s of milliseconds），因此亚秒级的 Batch 大小是可行的。 设置合理的批处理间隔对于群集上运行的 Spark Streaming 应用程序来说应该是稳定的，系统应该尽可能快的处理接收到的数据。换句话说，批数据在生成后应该尽快被处理。对于应用程序来说无论这个是不是真的都可以通过在 Streaming Web UI 找到 监控 的处理时间，批处理时间应该小于批间隔。 取决于流计算的性质，所用的批间隔可能在通过群集中一组固定资源上的应用程序持续的数据速率有显著的影响。例如，让我们考虑下更早的 WordCountNetwork 例子。对于一个特定的数据速率。系统可能能够保持每 2 秒报告一次单词统计（即，批间隔是 2 秒），而不是每 500 毫秒一次。因此批间隔需要被设置，使得线上的预期数据速率可持续。 要为您的应用程序找出一个合理的批大小是去使用一个保守的批间隔（例如，5~10 秒）和一个较低的数据速率来测试它。为了验证系统是否能够保持数据速率，您可以通过每个被处理的 Batch 来检查端到端的延迟情况（或者在 Spark Driver log4j 日志中看 “Total delay”，或者使用 StreamingListener 接口）。如果延迟与批大小相比较处于一个稳定的状态，那么系统是稳定的。否则，如果延迟继续增加，它意味着系统不能保持下去，因此它是不稳定的。一旦你有了一个稳定的配置，你可以去试着增加数据速率 和 / 或者 降低批大小。注意一个短暂的延迟增加是由于临时的数据速率增加可能会变好，只要延迟降低到一个比较低的值。（即，小于批大小）。 内存优化调优 Spark 应用程序的内存使用情况和 GC 行为已经在 调优指南 中详细讨论了。强烈推荐您阅读它。在这一章，我们讨论在 Spark Streaming 应用程序 Context 中指定的一些优化参数。 Spark Streaming 应用程序在群集中需要的 Memory（内存） 数据取决于使用的 Transformations（转换）上的类型行为。例如，如果您想要在最近 10 分钟的时候上使用 Window（窗口）函数，那么您的群集应有有足够的 Memory 以保存 10 分钟值的数据在内存中。或者您想要在大量的 keys 中使用 updateStateByKey，那么所需要的内存将会更高。与之相反，如果您想要去做一个简单的 map-filter-store 操作，那么所需的内存将会更少。 在一般情况下，从数据通过 Receiver（接收器）被接收时起使用 StorageLevel.MEMORY_AND_DISK_SER_2 存储，数据在内存中放不下时将拆分到硬盘中去。这样也许降低了 Streaming 应用程序的性能，因此建议为您的 Streaming 应用程序提供足够的内存。它最好去试一试，并且在小范围看看 Memory（内存）使用情况然后估算相应的值。 内存调优的另一个方面是垃圾收集。Streaming 应用程序需要低延迟，JVM 垃圾回收造成的大量暂停是不可取的。 这里有一些能够帮助你调整内存使用情况以及 GC 开销的参数 : Persistence Level of DStreams（DStream 的持久化级别）: 像前面所提到的 数据序列化 部分，输入的数据和 RDDs 默认持久化为序列化的字节。和反序列化持久性相比，这样减少了内存使用和 GC 开销。启用 Kryo 序列化进一步减少了序列化大小和内存使用。进一步减少内存使用可以用压缩实现（请看 Spark 配置 spark.rdd.compress），付出的是 CPU 时间。 Clearing old data（清除旧数据）: 默认情况下，所有的输入数据和通过 DStream transformation（转换）产生的持久化 RDDs 将被自动的清除。Spark Streaming 决定何时清除基于使用 transformation（转换）的数据。例如，如果你使用一个 10 分钟的 Window（窗口）操作，那么 Spark Streaming 将保存最近 10 分钟的数据，并积极扔掉旧的数据。数据也能够通过设置 streamingContext.remeber 保持更久（例如，交互式查询旧数据）。 CMS Garbage Collector（CMS 垃圾回收器）: 使用并发的 mark-sweep GC 是强烈推荐的用于保持 GC 相关的暂停更低。即使知道并发的 GC 降低了整个系统处理的吞吐量。仍然建议使用，以获得更一致的批处理时间。确定你在 Driver（在 spark-submit 中使用 –driver-java-options）和 Executor（使用 Spark 配置 spark.executor.extraJavaOptions）上设置的 CMS GC。 Other tips（其它建议）: 为了进一步降低 GC 开销，这里有些更多的建议可以尝试。 持久化 RDDs 使用 OFF_HEAP 存储界别。更多详情请看 Spark 编程指南 使用更多的 Executor 和更小的 heap size（堆大小）。这将在每个 JVM heap 内降低 GC 压力。 应该记住的要点 一个 DStream 和一个单独的 Receiver（接收器）关联。为了达到并行的读取多个 Receiver（接收器）。例如，多个 DStreams 需要被创建。一个 Receiver 运行在一个 Executor 内。它占有一个 Core（CPU）。确保在 Receiver Slot 被预定后有足够的 Core 。例如，spark.cores.max 应该考虑 Receiver Slot。Receiver 以循环的方式被分配到 Executor。 当数据从一个 Stream 源被接收时，Receiver（接收器） 创建了数据块。每个块间隔的毫秒内产生一个新的数据块。N 个数据块在批间隔（N = 批间隔 / 块间隔）的时候被创建。这些 Block（块）通过当前的 Executor 的 BlockManager 发布到其它 Executor 的 BlockManager。在那之后，运行在 Driver 上的 Network Input Tracker 获取 Block 位置用于进一步处理。 一个 RDD 创建在 Driver 上，因为 Block 创建在 batchInterval（批间隔）期间。Block 在 batchInterval 划分成 RDD 时生成。每个分区是 Spark 中的一个任务。blockInterval == batchInterval 将意味着那是一个单独的分区被创建并且可能它在本地被处理过了。 Block 上的 Map 任务在 Executor 中被处理（一个接收 Block，另一个 Block 被复制）无论块的间隔，除非非本地调度死亡。有更大的块间隔意味着更大的块。在本地节点上一个高的值 spark.locality.wait 增加处理 Blcok 的机会。需要发现一个平衡在这两个参数来确保更大的块被本都处理之间。 而不是依靠 batchInterval 和 blockInterval，你可以通过调用 inputDstream.repartition(n) 来定义分区的数量。这样会 reshuffles RDD 中的数据随机来创建 N 个分区。是的，为了更好的并行，虽然增加了 shuffle 的成本。一个 RDD 的处理通过 Driver 的 JobScheduler 作为一个 Job 来调度。在给定的时间点仅有一个 Job 是活跃的。所以，如果一个 Job 正在执行那么其它的 Job 会排队。 如果你有两个 DStream，那将有两个 RDDs 形成并且将有两个 Job 被创建，他们将被一个一个的调度。为了避免这个，你可以合并两个 DStream。这将确保两个 RDD 的 DStream 形成一个单独的 unionRDD。这个 unionRDD 被作为一个单独的 Job 考虑。然而分区的 RDDs 不受影响。 如果批处理时间超过了批间隔，那么显然 Receiver（接收器）的内存将开始填满，最走将抛出异常（最可能的是 BlockNotFoundException）。当前没有方法去暂停 Receiver。使用 SparkConf 配置 spark.streaming.receiver.maxRate，Receiver（接收器）的速率可以被限制。 容错语义在这部分，我们将讨论 Spark Streaming 应用程序在发生故障时的行为。 背景为了理解 Spark Streaming 提供的语义，让我们记住 Spark RDDs 最基本的容错语义。 RDD 是不可变的，确定重新计算的，分布式的 DataSet（数据集）。 如果任何分区的 RDD 由于 Worker 节点失败而丢失，那么这个分区能够从原来原始容错的 DataSet（数据集）使用操作的继承关系来重新计算。 假设所有的 RDD 转换是确定的，在最终转换的 RDD 中的数据总是相同的，无论 Spark 群集中的失败。 Spark 在数据上的操作像 HDFS 或者 S3 这样容错的文件系统一样。因此，所有从容错的数据中产生的 RDDs 也是容错的。然而，这种情况在 Spark Streaming 中不适用，因为在大多数情况下数据通过网络被接收（除非使用 fileStream）。为了对所有产生的 RDDs 实现相同的容错语义属性，接收的数据被复制到群集中 Worker 节点的多个 Spark Executor 之间（默认复制因子是 2）。这将会造成需要在发生故障时去恢复两种文件系统的数据类型 : Data received and replicated（数据接收并且被复制）- 这份数据幸存于一个单独的 Worker 节点故障中因为其它的节点复制了它。 Data received but buffered for replication（数据接收但是缓冲了副本）- 因为这份数据没有被复制，唯一恢复这份数据的方式是从 Source（源 / 数据源）再次获取它。 此外，我们应该关注两种类型的故障 : Failure of a Worker Node（Worker 节点的故障）- 任何运行 Executor 的 Worker 节点都是可以故障，并且这些节点所有在 Memory（内存）中的数据将会丢失。如果任何 Receiver（接收器）运行在故障的节点，那么他们缓存的数据将丢失。 Failure of the Driver Node（Driver 节点的故障）- 如果 Driver 节点运行的 Spark Streaming 应用程序发生故障，那么很显然 SparkContext 将会丢失，所有 Executor 与它们在内存中的数据都会丢失。 与这个基础知识一起，让我们理解 Spark Streaming 的容错语义。 定义Streaming 处理系统经常会捕获系统异常并记录执行次数以保障系统容错，其中在这三种条件下可以保障服务，等等。 最多一次：每个记录将被处理一次或者根本不处理。 至少一次：每个记录将被处理一次或多次。这主要是在最后一次次，因为它确保数据不会丢失。但也有可能是重复处理的。 只有一次：每个记录将被处理一次，确保这一次数据完整。这显然是三者的最强保障。 基础语义在任何流处理系统，从广义上讲，处理数据有三个步骤。 接收数据：接受数据或者从其他数据源接受数据。 转换数据：所接收的数据使用 DSTREAM 和 RDD 变换转化。 输出数据：最后变换的数据被推送到外部系统，如文件系统，数据库，DashBoard 等。 如果一个 Stream 应用程序来实现端到端的数据传输，则每个环节都需要一次性完整保障。也就是说，每个记录都必须被接收正好一次，恰好转化一次，被推到下游系统一次。让我们了解在 Spark Stream 的情况下这些步骤的语义。 接收数据：不同的输入源提供不同的容错。详细过程在下一小节中讨论。 转换数据：已经接收将被处理一次的所有数据，这要依靠于 RDDS 提供保证。即使有故障，只要接收到的输入数据是可读的，最终通过 RDDS 的转化将始终具有相同的内容。 输出数据：定义了数据至少一次输出，因为它依赖于输出数据操作类型（是否支持转换）和定义的下游系统（是否支持事务）。但是，用户可以实现自己的传输机制，以实现只要一次定义。会在后面小节里有更详细的讨论。 接收数据的语义不同的输入源提供不同的保障，从至少一次到正好一次。阅读更多的细节。 关于文件如果所有的输入数据已经存在如 HDFS 的容错文件系统，Spark Stream 总是可以从任何故障中恢复所有数据。这种定义，在一次处理后就能恢复。 关于基于 Receiver 的 Source（源）对于基于 receiver 的输入源，容错的语义既依赖于故障的情形也依赖于 receiver 的类型。正如 之前讨论 的，有两种类型的 receiver Reliable Receiver：这些 receivers 只有在确保数据复制之后才会告知可靠源。如果这样一个 receiver 失败了，缓冲（非复制）数据不会被源所承认。如果 receiver 重启，源会重发数 据，因此不会丢失数据。 Unreliable Receiver：当 worker 或者 driver 节点故障，这种 receiver 会丢失数据 选择哪种类型的 receiver 依赖于这些语义。如果一个 worker 节点出现故障，Reliable Receiver 不会丢失数据，Unreliable Receiver 会丢失接收了但是没有复制的数据。如果 driver 节点 出现故障，除了以上情况下的数据丢失，所有过去接收并复制到内存中的数据都会丢失，这会影响有状态 transformation 的结果。 为了避免丢失过去接收的数据，Spark 1.2 引入了一个实验性的特征（预写日志机制）write ahead logs，它保存接收的数据到容错存储系统中。有了 write ahead logs 和 Reliable Receiver，我们可以 做到零数据丢失以及 exactly-once 语义。 下表总结了根据故障的语义： Deployment Scenario Worker Failure Driver Failure Spark 1.1 或者更早， 没有 write ahead log 的 Spark 1.2 在 Unreliable Receiver 情况下缓冲数据丢失；在 Reliable Receiver 和文件的情况下，零数据丢失 在 Unreliable Receiver 情况下缓冲数据丢失；在所有 receiver 情况下，过去的数据丢失；在文件的情况下，零数据丢失 带有 write ahead log 的 Spark 1.2 在 Reliable Receiver 和文件的情况下，零数据丢失 在 Reliable Receiver 和文件的情况下，零数据丢失 Kafka Direct API在 Spark 1.3，我们引入了一个新的 kafka Direct 的 API，它可以保证所有 kafka 数据由 spark stream 收到一次。如果实现仅一次输出操作，就可以实现保证终端到终端的一次。这种方法（版本 Spark2.0.0）中进一步讨论 kafka 集成指南。 输出操作的语义输出操作（例如 foreachRDD）至少被定义一次，即变换后的数据在一次人工故障的情况下可能会不止一次写入外部实体。虽然可以通过操作 saveAs*Files 保存文件到系统上（具有相同的数据将简单地被覆盖），额外的尝试可能是必要的，以实现一次准确的语义。有两种方法。 幂等更新：多次尝试写相同的数据。例如，saveAs*Files 总是写入相同数据生成的文件。 事务更新：所有更新事务作出这样的更新是恰好遵循原子性。要做到这一点， 如下 2 种方式。 使用批处理时间创建一个标识符（foreachRDD）和 RDD 的分区索引。给这个应用定义唯一标识符标识 blob 数据。 更新外部系统与当前事务（即原子性），如果已经被标识过得数据应用已经存在，那么就跳过更新。 1234567dstream.foreachRDD &#123;(rdd, time) =&gt; rdd.foreachPartition &#123;partitionIterator =&gt; val partitionId = TaskContext.get.partitionId() val uniqueId = generateUniqueId(time.milliseconds, partitionId) // use this uniqueId to transactionally commit the data in partitionIterator &#125;&#125; 迁移指南（从 0.9.1 或者更低版本至 1.x 版本）在 Spark 0.9.1 和 Spark 1.0 之间，它们有一些 API 的改变以确保未来 API 的稳定性。这一章阐述需要迁移您已经在的代码到 1.0 版本的步骤。 Input DSteams（输入流）所有创建 Input Steam（输入流）的操作（例如，StreamingContext.socketStream，FlumeUtils.createStream，等等。）现在返回 InputDStream / ReceiverInputDStream (而不是 DStream) 的 Scala 版本，和 JavaInputDStream / JavaPairInputDStream /JavaReceiverInputDStream / JavaPairReceiverInputDStream (而不是 JavaDStream) 的 Java 版。这将确保特定输入流的功能可以添加到这些类中在未来不需要破坏二进制兼容性。注意您已经存在的 Spark Streaming 应用程序应该不需要任何改变（因为那些类是 DStream / JavaDStream 的子类），但可能需要使用 Spark 1.0 重新编译。 Custom Network Receivers（自定义网络接收器）从发布 Spark Streaming 以来，自定义网络接收器能够在 Scala 使用 NetworkReceiver 类自定义，该 API 在错误处理和报告中被限制了，并且不能够被 Java 使用。从 Spark 1.0 开始，自个类被 Receiver 替换了并且带来了下面的好处。 像 stop 和 restart 这样的方法已经被添加以更好的控制 Receiver（接收器）的生命周期。更多详细信息请看 自定义 Receiver 指南。 自定义 Receiver 能够使用 Scala 和 Java 实现。 为了迁移你已存在的自定义 Receiver，从更早的 NetworkReceiver 到新的 Receiver。您需要去做如下事情。 让您自定义的 Receiver 继承 org.apache.spark.streaming.receiver.Receiver 而不是 org.apache.spark.streaming.dstream.NetworkReceiver。 之前，一个 BlockGenerator 对象必须去通过自定义 Receiver 创建，它接收的数据被添加然后存在 Spark 中。它必须显示的从 onStart() 和 stop() 方法来启动和停止。新的 Receiver 类让这个是不必要的因为它添加了一组名为 store（）的方法，能够在 Spark 中被调用以存储数据。所以，为了迁移你自定义的网络接收器，删除任何的 BlockGenerator 对象（在 Spark 1.0 中再也不存在），并且在接收数据时使用 storm(…) 方法。 Actor-base Receivers（基于 Actor 的接收器）这个基于 Actor 的接收器 API 已经被移到 DStream Akka 中去了。更多详细信息请参考该工程。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 优化指南]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-tuning%2F</url>
      <content type="text"><![CDATA[大多数 Saprk 计算本质是内存，Spark 程序可以碰到集群中的 CPU、网络带宽或存储 资源上的瓶颈。大多数情况下，如果数据加载到内存中，网络带宽就是瓶颈。但有时候，还需要做一些调整，比如 序列化形式存储 RDD （storing RDDs in serialized），以减少内存使用情况。还涉及两个主要方面：原始数据的系列化，良好的网络性能，以减少内存使用情况。下面做几个主要的讨论。 数据序列化序列化在任何分布式应用程序提高性能方面都是至关重要的思路。格式对象序列化对降低消耗大量的字节数，将大大减少计算量。通常情况下，这将是 Spark 调整以优化应用程序的最先考虑的。 Spark 开发目的就在方便（可以让您在操作中与任何的 Java Type 类型一起工作）和性能之间最大化。它提供了两个序列化库 : Java serialization : 默认情况下，Spark 序列化使用 Java 对象 ObjectOutputStream 框架，你创建了 java.io.Serializable。 实现类就可以工作，可以更有效的延伸序列化的性能 java.io.Externalizable 中。Java 序列化优点灵活，但速度慢，序列化，并导致许多格式 class。 Kryo serialization : Spark 同样可以使用 Kryo 序列库（版本 2），Kryo 序列化比 java 序列化在速度上更快（一般在 10 倍左右） 缺点就是不支持所有 Serializable 类 ，但有时在项目中是比较好的性能提升。 你可以通过设置 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) 初始化 SparkConf 来转化成 Kryo 序列化此设置同样转化了 Spark 程序 Shuffling 内存数据和磁盘数据 RDDS 之间序列化，Kryo 不能成为默认方式的唯一原因是需要用户进行注册 在任何 “网络密集” 应用，跨语言支持较复杂。 Spark 底层自动把 Twitter chill 库中 AllScalaRegistrar Scala classes 包括在 Kryo 序列化中了 Kryo 注册的自定义类，使用 registerKryo 类的方法。 123val conf = new SparkConf().setMaster(...).setAppName(...)conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))val sc = new SparkContext(conf) Kryo documentation 提供更多高效的注册选项，比如添加自定义序列化。 针对对象是很大，你可能还需要增加的 spark.kryoserializer.buffer 配置。该值对大量序列化有带来一定个性能提升最后，如果您没有注册自定义类，Kryo 仍然可以使用，但它有完整的类名称存储每个对象时，这是一种浪费… 内存优化内存优化有三个方面的考虑 : 对象所占用的内存，访问对象的消耗以及垃圾回收所占用的开销。 默认情况下，Java 对象存取速度快，但可以很容易地比内部 “raw” 数据的字段的消耗的 2-5 倍以及更多空间。这是由于以下几个原因 : 每个不同的 Java 对象都有一个 “对象头” 大约是 16 个字节，并包含诸如一个指向它的类。并且包含了指向对象所对应的类的指针等信息。如果对象本身包含的数据非常少（比如就一个 Int 字段），那么对象头可能会比对象数据还要大。 Java String 在实际的 raw 字符串数据之外，还需要大约 40 字节的额外开销（因为 String 使用一个 Char 数组来保存字符串，而且需要保存长度等额外数据）；同时，因为 String 在内部使用 UTF-16 编码，每一个字符需要占用两个字节，所以，一个长度为 10 字节的字符串需要占用 60 个字节。 常见的集合类，如 LinkedList 和 HashMap，使用 LinkedList 的数据结构，每条是一个 “warpper” 的对象（例如为 Map.Entry）。每一个条目不仅包含对象头，还包含了一个指向下一条目的指针（通常每个指针占 8 字节）。 基本类型的集合常常将它们存储为 “boxed” 对象 比如 java.lang.Integer 中的对象。 先介绍 Spark 内存管理的概述，然后再讨论具体策略使 “用户” 更有效地在应用去申请内存。特别是，我们将介绍对象的内存使用情况在哪里寻找，以及从哪些方面提升 - 改变你的数据结构，或存储的数据就是序列化格式。以及调整 Spark 缓存、Java 的垃圾收集器。 内存管理概述Execution 与 Storge : 在 Spark 内存使用两类主要类型。execution 是指内存在 shuffles，joins，sorts 和 aggregations 的计算，而对于缓存存储器是指使用整个集群内部和数据。在 Spark，执行和存储共享统一区域（M）。当没有执行占用内存，存储可以获取所有的可用内存，反之亦然。如果必要的执行可能驱动存储，但只有等到总存储内存满足使用情况下一定的阈值（R）下降。换句话说，RM 区域 在哪里缓存块从未驱逐。存储可能无法执行复杂应用。 这种设计保证几个显著的特性。首先，应用程序不需要缓存整个执行到空间中，避免不必要的磁盘溢出。其次，应用程序都使用缓存，一个最小的可以保留的存储空间（R）模块，随着数据增加会别移出缓存中。最后，这种方法提供了现成合理的性能，适用于各种工作负载，而无需懂内部存储器划分的专业知识。 尽管提供两种相关配置，特殊用户一般是不会只用这两种配置作为适用于大多数的工作负载来调整的默认值 : spark.memory.fraction : 表示配置当前的内存管理器的最大内存使用比例，（默认 0.6）剩余 40% 部分被保留用于用户数据的结构，在 Spark 内部元数据，保障 OOM 错误，在异常大而稀疏的记录情况。 spark.memory.storageFraction : 表示配置用于配置 rdd 的 storage 与 cache 的默认分配的内存池大小（默认值 0.5）。 spark.memory.fraction 值的配置不仅仅以调试的 JVM 堆空间或 “tenured” 设置。还有一些 GC 优化。 确定内存消耗确定数据集所需内存量的最佳方法就是创建一个 RDD，把它放到缓存中，并查看网络用户界面 “Storage” 页面。该页面将显示有多少内存占用了 RDD。为了估算特定对象的内存消耗，使用 SizeEstimator 的方法估算内存使用情况，每个分区占用了多少内存量，合计这些内存占用量以确定 RDD 所需的内存总量。 数据结构优化减少内存消耗的首要方式是避免了 Java 特性开销，如基于指针的数据结构和二次封装对象。有几个方法可以做到这一点 : 使用对象数组以及原始类型数组来设计数据结构，以替代 Java 或者 Scala 集合类（eg : HashMap）fastutil 库提供了原始数据类型非常方便的集合类，同时兼容 Java 标准类库。 尽可能地避免使用包含大量小对象和指针的嵌套数据结构。 采用数字 ID 或者枚举类型以便替代 String 类型的主键。 假如内存少于 32GB，设置 JVM 参数 -XX:+UseCom­pressedOops 以便将 8 字节指针修改成 4 字节。将这个配置添加到 spark-env.sh 中。 序列化 RDD 存储当上面的优化都尝试过了对象同样很大。那么，还有一种减少内存的使用方法 “以序列化形式存储数据”，在 RDD 持久化 API 中（RDD persistence API）使用序列化的 StorageLevel 例如 MEMORY_ONLY_SER 。 Spark 将每个 RDD 分区都保存为 byte 数组。序列化带来的唯一不足就是会降低访问速度，因为需要将对象反序列化（using Kryo）。如果需要采用序列化的方式缓存数据，我们强烈建议采用 Kryo，Kryo 序列化结果比 Java 标准序列化的更小（某种程度，甚至比对象内部的 raw 数据都还要小）。 垃圾回收优化如果你需要不断的 “翻动” 程序保存的 RDD 数据，JVM 内存回收就可能成为问题（通常，如果只需进行一次 RDD 读取然后进行操作是不会带来问题的）。当需要回收旧对象以便为新对象腾内存空间时，JVM 需要跟踪所有的 Java 对象以确定哪些对象是不再需要的。需要记住的一点是，内存回收的代价与对象的数量正相关；因此，使用对象数量更小的数据结构（例如使用 int 数组而不是 LinkedList）能显著降低这种消耗。另外一种更好的方法是采用对象序列化，如上面所描述的一样；这样，RDD 的每一个 partition 都会保存为唯一一个对象（一个 byte 数组）。如果内存回收存在问题，在尝试其他方法之前，首先尝试使用 序列化缓存（serialized caching） 。 每项任务（task）的工作内存（运行 task 所需要的空间）以及缓存在节点的 RDD 之间会相互影响，这种影响也会带来内存回收问题。下面我们讨论如何为 RDD 分配空间以便减轻这种影响。 估算 GC 的影响优化内存回收的第一步是获取一些统计信息，包括内存回收的频率、内存回收耗费的时间等。为了获取这些统计信息，我们可以把参数 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps 添加到 java 选项中（配置指南 里面有关于传 java 选项参数到 Spark job 的信息）。设置完成后，Spark 作业运行时，我们可以在日志中看到每一次内存回收的信息。注意，这些日志保存在集群的工作节点（在他们工作目录下的 stout 文件中）而不是你的驱动程序（driver program )。 高级 GC 优化为了进一步优化内存回收，我们需要了解 JVM 内存管理的一些基本知识。 Java 堆（heap）空间分为两部分 : 新生代和老生代。新生代用于保存生命周期较短的对象；老生代用于保存生命周期较长的对象。 新生代进一步划分为三部分 [Eden，Survivor1，Survivor2] 内存回收过程的简要描述 : 如果 Eden 区域已满则在 Eden 执行 minor GC 并将 Eden 和 Survivor1 中仍然活跃的对象拷贝到 Survivor2 。然后将 Survivor1 和 Survivor2 对换。如果对象活跃的时间已经足够长或者 Survivor2 区域已满，那么会将对象拷贝到 Old 区域。最终，如果 Old 区域消耗殆尽，则执行 full GC 。 Spark 内存回收优化的目标是确保只有长时间存活的 RDD 才保存到老生代区域；同时，新生代区域足够大以保存生命周期比较短的对象。这样，在任务执行期间可以避免执行 full GC 。下面是一些可能有用的执行步骤 : 通过收集 GC 信息检查内存回收是不是过于频繁。如果在任务结束之前执行了很多次 full GC ，则表明任务执行的内存空间不足。 如果有过多的 minor GC 而不是 full GC，那么为 Eden 分配更大的内存是有益的。你可以为 Eden 分配大于任务执行所需要的内存空间。如果 Eden 的大小确定为 E，那么可以通过 -Xmn=4/3*E 来设置新生代的大小（将内存扩大到 4/3 是考虑到 survivor 所需要的空间)。 在打印的内存回收信息中，如果老生代接近消耗殆尽，那么减少用于缓存的内存空间。可这可以通过属性 spark.storage.memoryFraction 来完成。减少缓存对象以提高执行速度是非常值得的。 尝试设置 -XX:+UseG1GC 来使用垃圾回收器 G1GC 。在垃圾回收是瓶颈的场景使用它有助改善性能。当 executor 的 heap 很大时，使用 -XX:G1HeapRegionSize 增大 G1 区 大小很有必要。 举一个例子，如果任务从 HDFS 读取数据，那么任务需要的内存空间可以从读取的 block 数量估算出来。注意，解压后的 blcok 通常为解压前的 2-3 倍。所以，如果我们需要同时执行 3 或 4 个任务，block 的大小为 64M，我们可以估算出 Eden 的大小为 4364MB。 监控内存回收的频率以及消耗的时间并修改相应的参数设置。 我们的经历表明有效的内存回收优化取决于你的程序和内存大小。 在网上还有很多 更多其他调优选项 ， 总体而言有效控制内存回收的频率非常有助于降低额外开销。 executor 的 GC 调优标志位可以在 job 的配置中设置 spark.executor.extraJavaOptions 来指定。 其他优化并行度级别除非你为每步操作设置的 并行度 足够大，否则集群的资源是无法被充分利用的。Spark 自动根据文件大小设置运行在每个文件上的 map 任务的数量（虽然你可以通过 SparkContext.textFile 的可选参数来控制数量，等等），而且对于分布式 reduce 操作，例如 groupByKey 和 reduceByKey ，它使用最大父 RDD 的分区数。你可以通过第二个参数传入并行度（阅读文档 spark.PairRDDFunctions）或者通过设置系统参数 spark.default.parallelism 来改变默认值。通常来讲，在集群中，我们建议为每一个 CPU 核（core）分配 2-3 个任务。 Reduce Tasks 的内存使用有时，你会碰到 OutOfMemory 错误，这不是因为你的 RDD 不能加载到内存，而是因为 task 执行的数据集过大，例如正在执行 groupByKey 操作的 reduce 任务。Spark 的 shuffle 操作（sortByKey 、groupByKey 、reduceByKey 、join 等）为了实现 group 会为每一个任务创建哈希表，哈希表有可能非常大。最简单的修复方法是增加并行度，这样，每一个 task 的输入会变小。Spark 能够非常有效的支持短的 task（例如 200ms)，因为他会复用一个 executor 的 JVM 来执行多个 task，这样能减小 task 启动的消耗，所以你可以放心的增加任务的并行度到大于集群的 CPU 核数。 广播大的变量使用 SparkContext 的 广播功能 可以有效减小每个序列化的 task 的大小以及在集群中启动 job 的消耗。如果 task 使用 driver program 中比较大的对象（例如静态查找表），考虑将其变成广播变量。Spark 会在 master 打印每一个 task 序列化后的大小，所以你可以通过它来检查 task 是不是过于庞大。通常来讲，大于 20KB 的 task 可能都是值得优化的。 数据本地性数据本地性会对 Spark jobs 造成重大影响。如果数据和操作数据的代码在一起，计算就会变快。但如果代码和数据是分离的，其中一个必须移动到另一个。一般来说，移动序列化的代码比移动数据块来的快，因为代码大小远小于数据大小。Spark 根据该数据本地性的统一法则来构建 scheduling 计划。 数据本地性是指数据和操作该数据的代码有多近。根据数据的当前路径，有以下几个本地性级别。根据从近到远的数据排列 : PROCESS_LOCAL 数据跟代码在同一个 JVM。这个是最好的本地性。 NODE_LOCAL 所有数据都在同一个节点。例如在同一个节点的 HDFS，或在同一个节点的另一个 executor 上。这个级别比 PROCESS_LOCAL 慢一点因为数据需要在多个 process（进程）间移动。 NO_PREF 从任何地方访问数据都一样快，没有本地性的偏好。 RACK_LOCAL 数据在同样的服务器机架上。数据在同一个机架的不同服务器上，所以需要通过网络传输，典型场景是通过单个交换机。 ANY 数据在网络的不同地方，不咋同一个机架。 Spark 倾向于调度所有的 task 在最好的本地性级别，但未必总是行得通。如果所有空闲的 executor 都没有未处理的数据，Spark 就会切换到更低的本地性级别。 这样就有两个选择 : 一直等待直到忙碌的 CPU 释放下来对同一个服务器的数据启动 task 。 马上在远的、需要传输数据的地方启动一个 task。 Spark 典型做法是等待一段 timeout（超时）时间直到 CPU 释放资源。一旦 timeout 结束，它就开始移动数据到远处、有空闲 CPU 的地方。各个级别切换所等待的 timeout 时间可以单独配置或统一通过一个参数配置，需要更多细节可以查看 配置页面 的 spark.locality 参数。如果你的 task 看起来很长而且本地性差，就要考虑增加这些设置值，但默认设置一般都运行良好。 总结该文指出了 Spark 程序优化所需要关注的几个关键点 - 最主要的是 数据序列化 和 内存优化 。对于大多数程序而言，采用 Kryo 序列化以及以序列化方式存储数据能够解决大部分性能问题。非常欢迎在 Spark mailing list 提问优化相关的问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 集群模式概述]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-cluster-overview%2F</url>
      <content type="text"><![CDATA[该文档给出了 Spark 如何在集群上运行、使之更容易来理解所涉及到的组件的简短概述。通过阅读 应用提交指南 来学习关于在集群上启动应用。 组件Spark 应用在集群上作为独立的进程组来运行，在您的 main 程序中通过 SparkContext 来协调（称之为 driver 程序）。 具体的说，为了运行在集群上，SparkContext 可以连接至几种类型的 Cluster Manager（既可以用 Spark 自己的 Standlone Cluster Manager，或者 Mesos，也可以使用 YARN），它们会分配应用的资源。一旦连接上，Spark 获得集群中节点上的 Executor，这些进程可以运行计算并且为您的应用存储数据。接下来，它将发送您的应用代码（通过 JAR 或者 Python 文件定义传递给 SparkContext）至 Executor。最终，SparkContext 将发送 Task 到 Executor 运行。 这里有几个关于这个架构需要注意的地方 : 每个应用获取到它自己的 Executor 进程，它们会保持在整个应用的生命周期中并且在多个线程中运行 Task（任务）。这样做的优点是把应用互相隔离，在调度方面（每个 driver 调度它自己的 task）和 Executor 方面（来自不同应用的 task 运行在不同的 JVM 中）。然而，这也意味着若是不把数据写到外部的存储系统中的话，数据就不能够被不同的 Spark 应用（SparkContext 的实例）之间共享。 Spark 是不知道底层的 Cluster Manager 到底是什么类型的。只要它能够获得 Executor 进程，并且它们可以和彼此之间通信，那么即使是在一个也支持其它应用的 Cluster Manager（例如，Mesos / YARN）上来运行它也是相对简单的。 Driver 程序必须在自己的生命周期内（例如，请看 在网络中配置 spark.driver.port 章节）监听和接受来自它的 Executor 的连接请求。同样的，driver 程序必须可以从 worker 节点上网络寻址（即网络没问题）。 因为 driver 调度了集群上的 task（任务），更好的方式应该是在相同的局域网中靠近 worker 的节点上运行。如果您不喜欢发送请求到远程的集群，倒不如打开一个 RPC 至 driver 并让它就近提交操作而不是从很远的 worker 节点上运行一个 driver。 Cluster Manager 类型该系统当前支持三种 Cluster Manager : Standalone – 包含在 Spark 中使得它更容易来安装集群的一个简单的 Cluster Manager。 Apache Mesos – 一个通用的 Cluster Manager，它也可以运行 Hadoop MapReduce 和其它服务应用。 Hadoop YARN – Hadoop 2 中的 resource manager（资源管理器）。 提交应用使用 spark-submit 脚本可以提交应用至任何类型的集群。在 应用提交指南 中介绍了如何来做到这一点。 监控每个 driver 程序有一个 Web UI，通常在端口 4040 上，它展示了关于运行 task，executor，和存储使用情况的信息。在网页浏览器中访问这个 UI : http://\:4040。 监控指南 也描述了其它的监控选项。 Job 调度Spark 即可以在应用间（Cluster Manager 级别），也可以在应用内（如果多个计算发生在相同的 SparkContext 上时）控制资源分配。Job 调度指南 描述的更详细。 术语下表中总结了您将会看到用于涉及到集群时的术语 : Term Meaning Application 用户构建在 Spark 上的程序。由集群上的一个 driver 程序和多个 executor 组成。 Application jar 一个包含用户 Spark 应用的 Jar。有时候用户会想要去创建一个包含他们应用以及它的依赖的 “uber jar”。用户的 Jar 应该没有包括 Hadoop 或者 Spark 库，然而，它们将会在运行时被添加。 Driver program 该进程运行应用的 main() 方法并且创建了 SparkContext。 Cluster manager 一个外部的用于获取集群上资源的服务。（例如，Standlone Manager，Mesos，YARN） Deploy mode 根据 driver 程序运行的地方区别。在 “Cluster” 模式中，框架在群集内部启动 driver。在 “Client” 模式中，submitter（提交者）在 Custer 外部启动 driver。 Worker node 任何在集群中可以运行应用代码的节点。 Executor 一个为了在 worker 节点上的应用而启动的进程，它运行 task 并且将数据保持在内存中或者硬盘存储。每个应用有它自己的 Executor。 Task 一个将要被发送到 Executor 中的工作单元。 Job 一个由多个 task 组成的并行计算，并且能从 Spark action 中获取响应（例如，save，collect），您将在 driver 的日志中看到这个术语。 Stage 每个 Job 被拆分成更小的被称作 stage（阶段） 的 task（任务） 组，stage 彼此之间是相互依赖的（与 MapReduce 中的 map 和 reduce stage 相似）。您将在 driver 的日志中看到这个术语。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[构建 Spark]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-building-spark%2F</url>
      <content type="text"><![CDATA[Apache Maven构建 Apache Spark 参考了基于 Maven 的构建。构建 Spark 会使用 Maven ，而且需要 3.3.9 或者更新版本的 Maven 以及 Java 7 及以上版本。 设置 Maven 的内存使用率想要使用更多内存，需要配置 Maven 设置中的 MAVEN_OPTS 项 1export MAVEN_OPTS=&quot;-Xmx2g -XX : ReservedCodeCacheSize=512m&quot; 在使用 Java 7 来进行编译时，您需要添加额外选项 “-XX : MaxPermSize=512M” 到 MAVEN_OPTS 项 如果不添加这些参数，那么您可能看到如下错误和警告 : 123456789[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.11/classes...[ERROR] PermGen space -&gt; [Help 1][INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.11/classes...[ERROR] Java heap space -&gt; [Help 1][INFO] Compiling 233 Scala sources and 41 Java sources to /Users/me/Development/spark/sql/core/target/scala-&#123;site.SCALA_BINARY_VERSION&#125;/classes...OpenJDK 64-Bit Server VM warning : CodeCache is full. Compiler has been disabled.OpenJDK 64-Bit Server VM warning : Try increasing the code cache size using -XX : ReservedCodeCacheSize= 您可以通过以上方法设置 MAVEN_OPTS 项来解决这一问题。 注意 : 如果使用 build/mvn 但是没有设置 MAVEN_OPTS，那么脚本会自动的将以上选项添加到 MAVEN_OPTS 环境变量 构建 Spark 的测试阶段将自动把这些选项添加到 MAVEN_OPTS ，即使不使用 build/mvn 在您使用 Java 8 和 build/mvn 来构建和运行测试时，您可能会看到类似于 “ignoring option MaxPermSize=1g; support was removed in 8.0” 的警告。这些警告无伤大雅。 build / mvnSpark 现在与一个独立的 Maven 安装包封装到了一起，使从源目录 build/ 下的原始位置来构建和部署 Spark 更加容易。这个脚本能自动在本地的 build/ 目录下载和安装所有需要的构建需求包，例如 Maven、Scala、Zinc。其尊重任何已经存在的 mvn 二进制文件，但是却无论如何将自己的 Scala 和 Zinc 的副本摧毁，来确保需求包的版本是合适的。build/mvn 的执行通过 mvn 调用允许过渡到之前的构建方法。例如，可以使用如下方法构建一个版本的 Spark : 1./build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package 其他的构建例子能在下文找到 构建一个可运行的 Distribution 版本想要创建一个像是那些 Spark 下载页中的 Spark 的分布式版本，并且能使其运行，使用项目中 root 目录下的 ./dev/make-distribution.sh 脚本。它可以使用 Maven 的配置文件进行配置，如直接使用 Maven 构建。例如 : 1./dev/make-distribution.sh --name custom-spark --tgz -Psparkr -Phadoop-2.4 -Phive -Phive-thriftserver -Pyarn 想要获取更详细的信息，可以运行 1./dev/make-distribution.sh --help 指定 Hadoop 版本因为 HDFS 并非跨版本协议兼容的，如果您想从 HDFS 读数据，那么您就需要构建与您的环境中 HDFS 版本相符的 Spark 。您可以通过设置属性 hadoop.version 来实现。如果不设置，那么 Spark 将默认按照 Hadoop 2.2.0 来进行构建。注意对于特定的 Hadoop 版本，需要某些特定的构造文件。 Hadoop version Profile required 2.2.x hadoop-2.2 2.3.x hadoop-2.3 2.4.x hadoop-2.4 2.6.x hadoop-2.6 2.7.x and later 2.x hadoop-2.7 您可以配置 yarn 的配置文件和可选设置 yarn.version 属性，如果其与 hadoop.version 不同的时候。 Spark 只支持 Yarn2.2.0 或者之后版本。 例如 : 1234567891011121314151617# Apache Hadoop 2.2.X./build/mvn -Pyarn -Phadoop-2.2 -DskipTests clean package# Apache Hadoop 2.3.X./build/mvn -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -DskipTests clean package# Apache Hadoop 2.4.X or 2.5.X./build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=VERSION -DskipTests clean package# Apache Hadoop 2.6.X./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package# Apache Hadoop 2.7.X and later./build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=VERSION -DskipTests clean package# Different versions of HDFS and YARN../build/mvn -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Dyarn.version=2.2.0 -DskipTests clean package 构建支持 Hive 和 JDBC 的支持想要使 Spark SQL 集成 Hive 以及 JDBC 服务和 CLI ，请增加 -Phive 和 Phive-thriftserver 配置到您已经存在的配置选项。默认 Spark 绑定 Hive 1.2.1 12# Apache Hadoop 2.4.X with Hive 1.2.1 support./build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package 打包时排除针对 YARN 的 Hadoop 依赖mvn package 生成的汇编目录，默认情况下将包括所有 Spark 的依赖，包括 Hadoop 和一些生态系统项目。在 YARN 的部署上，这会导致这些项目的许多版本都出现在执行目录中 : 包装在 Spark 组建在没个结点的版本，包括在属性 yarn.application.classpath 中。配置 hadoop-provided 可以在构建时不包括 Hadoop 生态项目，例如 ZooKeeper 和 Hadoop 本身。 构建适用于 Scala 2.10如果希望使用 Scala 2.10 来编译 Spark 包，使用属性 -Dscala-2.10 12./dev/change-scala-version.sh 2.10./build/mvn -Pyarn -Phadoop-2.4 -Dscala-2.10 -DskipTests clean package 构建单个子模块可以使用选项 mvn -pl 来构建 Spark 的子模块。 例如，您可以构建 Spark Streaming 通过 : 1./build/mvn -pl : spark-streaming_2.11 clean install 这里 spark-streaming_2.11 是在文件 streaming/pom.xml 中定义的 artifactId 连续的编译我们使用插件 scala-maven-plugin 来支持增量和连续编译。例如 : 1./build/mvn scala : cc 这里应该运行连续编译 (即等待变化)。然而这并没得到广泛的测试，有一些需要注意的陷阱 : 它只扫描路径 src/main 和 src/test ，所以其只对含有该结构的某些子模块起作用 您通常需要在工程的根目录运行 mvn install 在编译某些特定子模块时。这是因为依赖于其他子模块的子模块需要通过 spark-parent 模块实现。 因此，运行 core 子模块的连续编译总流程如下 : 123$ ./build/mvn install$ cd core$ ../build/mvn scala : cc 用 Zinc 加速编译Zinc 是 SBT 增量编译器的长期运行的服务器版本，当作为后台进程在本地运行时，它可有加速编译基于 Scala 的项目 : 如 Spark 。定期使用 Maven 重新编译 Spark 的开发人员会对 Zinc 最感兴趣。此章节将说明如何构建和运行 Zinc.OS X 用户可有通过 brew 安装 Zinc。 如果使用 bulid/mvn 软件包，ZInc 将会自动下载并且用于所有版本。 此进程将会在第一次 build/mvn 编译后自动开启并绑定到 3030 端口，除非自定义设置了环境变量 ZINC_PORT。可有在任何时候通过运行 build/zinc-/bin/zinc -shutdown 来停止 Zinc 进程，但每当通过 build/mvn 编译的时候它又会重新启动。 通过 SBT 编译Maven 是 Spark 官方推荐的编译工具。但是当 SBT 能够提供快速迭代编译的功能时，它逐渐被应用于日常开发。更多高级的开发者希望能够运用 SBT 。 SBT 通过 Maven POM 文件进行编译，因此也可以设置相同的 Maven 配置文件和变量来控制 SBT 的构建。 例如 : 1./build/sbt -Pyarn -Phadoop-2.3 package 为了避免每次启动 SBT 都需要重新编译，可以通过运行 build/sbt 以交互模式来启动 SBT，然后在命令提示符下运行所有 build 命令。 有关减少构建时间的更多建议，请参阅 wiki 页面。 加密的文件系统当构建在加密的文件系统上（例如，如果您的主目录被加密），那么 Spark 构建可能会失败，并出现 “Filename too long” 错误。 作为解决方法，在项目的 pom.xml 中的 scala-maven-plugin 的配置参数中添加以下代码 : 12&lt;arg&gt;-Xmax-classfile-name&lt;/arg&gt;&lt;arg&gt;128&lt;/arg&gt; 并在 project/SparkBuild.scala 中添加 : 1scalacOptions in Compile ++= Seq("-Xmax-classfile-name", "128"), 关于 sharedSettings val，如果您不确定在哪里添加这些行，请参阅此 PR。 IntelliJ IDEA 或者 Eclipse有关设置 IntelliJ IDEA 或 Eclipse 以用来进行 Spark 开发和故障排除的帮助文档，请参阅 IDE 设置的 wiki 页面。 运行测试默认情况下通过 ScalaTest Maven 组件进行测试。 一些测试需要首先打包 Spark ，因此，第一次总是使用 -DskipTests 运行 mvn package。 以下是正确（构建，测试）顺序的示例 : 12./build/mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive -Phive-thriftserver clean package./build/mvn -Pyarn -Phadoop-2.3 -Phive -Phive-thriftserver test ScalaTest 组件还支持运行特定的 Scala test 套件。如下 : 12./build/mvn -P... -Dtest=none -DwildcardSuites=org.apache.spark.repl.ReplSuite test./build/mvn -P... -Dtest=none -DwildcardSuites=org.apache.spark.repl.* test 或者一个 java 测试用例 : 1./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.streaming.JavaAPISuite 使用 SBT 测试一些测试需要首先打包 Spark，所以总是第一次总是运行 build/sbt。 以下是正确（构建，测试）顺序的示例 : 12./build/sbt -Pyarn -Phadoop-2.3 -Phive -Phive-thriftserver package./build/sbt -Pyarn -Phadoop-2.3 -Phive -Phive-thriftserver test 要运行特定的测试套件 12./build/sbt -Pyarn -Phadoop-2.3 -Phive -Phive-thriftserver "test-only org.apache.spark.repl.ReplSuite"./build/sbt -Pyarn -Phadoop-2.3 -Phive -Phive-thriftserver "test-only org.apache.spark.repl.*" 要运行特定子项目的测试套件，如下所示 : 1./build/sbt -Pyarn -Phadoop-2.3 -Phive -Phive-thriftserver core/test 运行 JAVA 8 测试组件只能运行 JAVA 8 测试组件，不能运行其他版本 12./build/mvn install -DskipTests./build/mvn -pl : java8-tests_2.11 test 或者 1./build/sbt java8-tests/test 当检测到 Java 8 JDK 时，将自动启用 Java 8 测试。 如果您安装了 JDK 8，但它不是系统默认值，则可以在运行测试之前将 JAVA_HOME 设置为指向 JDK 8 。 用 Maven 做 PySpark 测试如果您正在构建 PySpark 并希望运行 PySpark 测试，您将需要使用 Hive 支持构建 Spark 。 12./build/mvn -DskipTests clean package -Phive./python/run-tests 运行测试脚本也可以限于特定的 Python 版本或特定的模块 1./python/run-tests --python-executables=python --modules=pyspark-sql 运行 R 测试要运行 SparkR 测试，您需要安装 R 包 testthat（从 R shell 运行 install.packages（testthat））。 您可以使用命令只运行 SparkR 测试 : 1./R/run-tests.sh 注意 : 您还可以使用 SBT 构建运行 Python 测试，前提是您使用 Hive 支持构建 Spark 。 运行基于 Docker 集成的测试组件 为了运行 Docker 集成测试，您必须在您的盒子上安装 docker 引擎。 有关安装的说明，请访问 Docker 站点。 一旦安装，docker 服务需要启动，如果还没有运行。 在 Linux 上，这可以通过 sudo service docker start 来完成。 12./build/mvn install -DskipTests./build/mvn -Pdocker-integration-tests -pl : spark-docker-integration-tests_2.11 或者 1./build/sbt docker-integration-tests/test]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 提交应用]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-submitting-applications%2F</url>
      <content type="text"><![CDATA[在 Spark 的 bin 目录中的 spark-submit 脚本用于在集群上启动应用程序。它可以通过一个统一的接口使用所有 Spark 支持的 Cluster Manager，所以您不需要专门的为每个 Cluster Manager 来配置您的应用程序。 打包应用依赖如果您的代码依赖了其它的项目，为了分发代码到 Spark 集群中您将需要将它们和您的应用程序一起打包。为此，创建一个包含您的代码以及依赖的 assembly jar（或 “uber” jar）。无论是 sbt 还是 Maven 都有 assembly 插件。在创建 assembly jar 时，列出 Spark 和 Hadoop 提供的依赖。它们不需要被打包，因为在运行时它们已经被 Cluster Manager 提供了。如果您有一个 assembled jar 您就可以调用 bin/spark-submit 脚本（如下所示）来传递您的 jar。 对于 Python 来说，您可以使用 spark-submit 的 –py-files 参数来添加 .py，.zip 和 .egg 文件以与您的应用程序一起分发。如果您依赖了多个 Python 文件我们推荐将它们打包成一个 .zip 或者 .egg 文件。 用 spark-submit 启动应用如果用户的应用程序被打包好了，它可以使用 bin/spark-submit 脚本来启动。这个脚本负责设置 Spark 和它的依赖的 classpath，并且可以支持 Spark 所支持的不同的 Cluster Manager 以及 deploy mode（部署模式）: 12345678./bin/spark-submit \ --class &lt;main-class&gt; \ --master &lt;master-url&gt; \ --deploy-mode &lt;deploy-mode&gt; \ --conf &lt;key&gt;=&lt;value&gt; \ ... # other options &lt;application-jar&gt; \ [application-arguments] 一些常用的选项有 : –class : 您的应用程序的入口点（例如。org.apache.spark.examples.SparkPi）。 –master : 集群的 Master URL（例如。spark://23.195.26.187:7077）。 –deploy-mode : 是在 worker 节点（cluster）上还是在本地作为一个外部的客户端（client）部署您的 driver（默认 : client）†。 –conf : 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。 application-jar : 包括您的应用以及所有依赖的一个打包的 Jar 的路径。该 URL 在您的集群上必须是全局可见的，例如，一个 hdfs:// path 或者一个 file:// path 在所有节点是可见的。 application-arguments : 传递到您的 main class 的 main 方法的参数，如果有的话。 † 常见的部署策略是从一台 gateway 机器物理位置与您 worker 在一起的机器（比如，在 standalone EC2 集群中的 Master 节点上）来提交您的应用。在这种设置中，client 模式是合适的。在 client 模式中，driver 直接运行在一个充当集群 client 的 spark-submit 进程内。应用程序的输入和输出直接连到控制台。因此，这个模式特别适合那些设计 REPL（例如，Spark shell）的应用程序。 另外，如果您从一台远离 worker 机器的机器（例如，本地的笔记本电脑上）提交应用程序，通常使用 cluster 模式来降低 driver 和 executor 之间的延迟。目前，Standalone 模式不支持 Cluster 模式的 Python 应用。 对于 Python 应用，在 的位置简单的传递一个 .py 文件而不是一个 JAR，并且可以用 –py-files 添加 Python .zip，.egg 或者 .py 文件到 search path（搜索路径）。 这里有一些选项可用于特定的 Cluster Manager 中。例如，Spark standalone cluster 用 cluster 部署模式，您也可以指定 –supervise 来确保 driver 在 non-zero exit code 失败时可以自动重启。为了列出所有 spark-submit 可用的选项，用 –help 来运行它。这里是一些常见选项的例子 : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# Run application locally on 8 cores./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master local[8] \ /path/to/examples.jar \ 100# Run on a Spark standalone cluster in client deploy mode./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000# Run on a Spark standalone cluster in cluster deploy mode with supervise./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000# Run on a YARN clusterexport HADOOP_CONF_DIR=XXX./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ # can be client for client mode --executor-memory 20G \ --num-executors 50 \ /path/to/examples.jar \ 1000# Run a Python application on a Spark standalone cluster./bin/spark-submit \ --master spark://207.184.161.138:7077 \ examples/src/main/python/pi.py \ 1000# Run on a Mesos cluster in cluster deploy mode with supervise./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master mesos://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ http://path/to/examples.jar \ 1000 Master URL传递给 Spark 的 master URL 可以使用下列格式中的一种 : Master URL Meaning local 使用一个线程本地运行 Spark（即，没有并行性）。 local[K] 使用 K 个 worker 线程本地运行 Spark（理想情况下，设置这个值的数量为您机器的 core 数量）。 local[*] 使用更多的 worker 线程作为逻辑的 core 在您的机器上来本地的运行 Spark。 spark://HOST:PORT 连接至给定的 Spark standalone cluster master。该 port（端口）必须有一个作为您的 master 配置来使用，默认是 7077. mesos://HOST:PORT 连接至给定的 Mesos cluster。该 port（端口）必须有一个作为您的配置来使用，默认是 5050。或者，对于使用了 ZooKeeper 的 Mesos cluster 来说，使用 mesos://zk://…。使用 –deploy-mode cluster 来提交，该 HOST:PORT 应该被配置以连接到 MesosClusterDispatcher。 yarn 连接至一个 YARN cluster，取决于 –deploy-mode 的值在 client 或者 cluster 模式中。该 cluster 的位置将根据 HADOOP_CONF_DIR 或者 YARN_CONF_DIR 变量来找到。 从文件中加载配置spark-submit 脚本可以从一个 properties 文件加载默认的 Spark configuration values 并且传递它们到您的应用中去。默认情况下，它将从 Spark 目录下的 conf/spark-defaults.conf 读取配置。更多详细信息，请看 加载默认配置 部分。 加载默认的 Spark 配置，这种方式可以消除某些标记到 spark-submit 的必要性。例如，如果 spark.master 属性被设置了，您可以在 spark-submit 中安全的省略。一般情况下，明确设置在 SparkConf 上的配置值的优先级最高，然后是传递给 spark-submit 的值，最后才是 default value（默认文件）中的值。 如果您不是很清楚其中的配置设置来自哪里，您可以通过使用 –verbose 选项来运行 spark-submit 打印出细粒度的调试信息。 先进的依赖管理在使用 spark-submit 时，使用 –jars 选项包括的应用程序的 jar 和任何其它的 jar 都将被自动的传输到集群。在 –jars 后面提供的 URL 必须用逗号分隔。该列表传递到 driver 和 executor 的 classpath 中 –jars 不支持目录的形式。 Spark 使用下面的 URL 方案以允许传播 jar 时使用不同的策略 : file: - 绝对路径和 file:/URI 通过 driver 的 HTTP file server 提供服务，并且每个 executor 会从 driver 的 HTTP server 拉取这些文件。 hdfs:，http:，https:，ftp: - 如预期的一样拉取下载文件和 JAR。 local: - 一个用 local:/ 开头的 URL 预期作在每个 worker 节点上作为一个本地文件存在。这样意味着没有网络 IO 发生，并且并且非常适用于那些通过 NFS，GlusterFS，等推入到（pushed to）每个 worker 或共享大型的 file/JAR。 注意，那些 JAR 和文件被复制到 working directory（工作目录）用于在 executor 节点上的每个 SparkContext。这可以使用最多的空间显著量随着时间的推移，将需要清理。在 Spark On YARN 模式中，自动执行清理操作。在 Spark standalone 模式中，可以通过配置 spark.worker.cleanup.appDataTtl 属性来执行自动清理。 用户也可以通过使用 –packages 来提供一个逗号分隔的 maven coordinates（maven 坐标）以包含任何其它的依赖。在使用这个命令时所有可传递的依赖将被处理。其它的 repository（或者在 SBT 中被解析的）可以使用 –repositoies 该标记添加到一个逗号分隔的样式中。这些命令可以与 pyspark，spark-shell 和 spark-submit 配置会使用以包含 Spark Packages（Spark 包）。 对于 Python 来说，也可以使用 –py-files 选项用于分发 .egg，.zip 和 .py libraries 到 executor 中。 更多信息如果您已经部署了您的应用程序，集群模式概述 描述了在分布式执行中涉及到的组件，以及如何去监控和调试应用程序。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark Standalone 模式]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-standalone%2F</url>
      <content type="text"><![CDATA[Spark 不仅可以运行在 Mesos 或者 Yarn 上，而且还提供独立部署模式。可以手动启动一个 master 和 多个 worker，或选用我们提供的 脚本 来启动 standalone 集群。 安装 Spark standalone 集群独立安装 Spark 集群，只需要把编译好的版本部署在每个节点上，然后启动，或者也可以自己 编译指定版本。 手动启动集群通过下面脚本启动集群的主节点: 1./sbin/start-master.sh 通过上面脚本启动 master ，master 会通过 spark://host:portURL 生成链接，你可以手动访问，或者通过 master 的 SparkContext 来创建访问，或者通过浏览器访问 Spark 的 Web UI , 默认情况： http://localhost:8080 同样，你可以启动一个和多个 worker ，并通过 主节点把他们连接到一起: 1./sbin/start-slave.sh &lt;master-spark-URL&gt; 一旦你已经启动成功了一个 worker , 即可访问 master 的 WEB UI（http://localhost:8080，默认），不仅 可以看到新添加的节点，还可以看到新的节点列表、节点的 CPU 数以及内存。 最后, 下面的配置参数可以传递给 master 和 worker 参数 说明 -h HOST， –host HOST 监听的主机名 -i HOST， –ip HOST 同上，不建议使用 -p PORT， –port PORT 监听的服务的端口（master 默认是 7077，worker 随机） –webui-port PORT web UI 的端口 (master 默认是 8080，worker 默认是 8081) -c CORES， –cores CORES Spark 应用程序可以使用的 CPU 核数（默认是所有可用）；这个选项仅在 worker 上可用 -m MEM， –memory MEM Spark 应用程序可以使用的内存数（默认情况是你的机器内存数减去 1g）；这个选项仅在 worker 上可用 -d DIR， –work-dir DIR 用于暂存空间和工作输出日志的目录（默认是 SPARK_HOME/work）；这个选项仅在 worker 上可用 –properties-file FILE 自定义的 Spark 配置文件的加载目录（默认是 conf/spark-defaults.conf） 脚本启动集群为了用启动脚本启动 Spark 独立集群，你应该在你的 Spark 目录下建立一个名为 conf/slaves 的文件，这个文件必须包含所有你要启动的 Spark worker 所在机器的主机名，一行一个。如果 conf/slaves 不存在，启动脚本默认为单个机器（localhost），这台机器对于测试是有用的。注意，master 机器通过 ssh 访问所有的 worker。在默认情况下，SSH 是并行运行，需要设置无密码（采用私有密钥）的访问。 如果你没有设置为无密码访问，你可以设置环境变量 SPARK_SSH_FOREGROUND，为每个 worker 提供密码。 一旦你设置了这个文件，你就可以通过下面的 shell 脚本启动或者停止你的集群。 sbin/start-master.sh：在机器上启动一个 master 实例 sbin/start-slaves.sh：在每台机器上启动一个 slave 实例 sbin/start-slave.sh - Starts a slave instance on the machine the script is executed on. sbin/start-all.sh：同时启动一个 master 实例和所有 slave 实例 sbin/stop-master.sh：停止 master 实例 sbin/stop-slaves.sh：停止所有 slave 实例 sbin/stop-all.sh：停止 master 实例和所有 slave 实例 注意，这些脚本必须在你的 Sparkmaster 运行的机器上执行，而不是在你的本地机器上面。 你可以在 conf/spark-env.sh 中设置环境变量进一步配置集群。利用 conf/spark-env.sh.template 创建这个文件，然后将它复制到所有的 worker 机器上使设置有效。下面的设置可以起作用： 环境变量 说明 SPARK_MASTER_HOST 绑定 master 到一个指定的 ip 地址 SPARK_MASTER_PORT 不同的端口上启动主（默认值：7077） SPARK_MASTER_WEBUI_PORT master web UI 的端口（默认是 8080） SPARK_MASTER_OPTS 应用到 master 的配置属性，格式是 “-Dx=y”（默认是 none），查看下面的表格的选项以组成一个可能的列表 SPARK_LOCAL_DIRS Spark 中暂存空间的目录。包括 map 的输出文件和存储在磁盘上的 RDDs(including map output files and RDDs that get stored on disk)。这必须在一个快速的、可靠的系统本地磁盘上。它可以是一个逗号分隔的列表，代表不同磁盘的多个目录 SPARK_WORKER_CORES Spark 应用程序可以用到的核心数（默认是所有可用） SPARK_WORKER_MEMORY Spark 应用程序用到的内存总数（默认是内存总数减去 1G）。注意，每个应用程序个体的内存通过 spark.executor.memory 设置 SPARK_WORKER_PORT 在指定的端口上启动 Spark worker(默认是随机) SPARK_WORKER_WEBUI_PORT worker UI 的端口（默认是 8081） SPARK_WORKER_DIR Spark worker 运行目录，该目录包括日志和暂存空间（默认是 SPARK_HOME/work） SPARK_WORKER_OPTS 应用到 worker 的配置属性，格式是 “-Dx=y”（默认是 none），查看下面表格的选项以组成一个可能的列表 SPARK_DAEMON_MEMORY 分配给 Spark master 和 worker 守护进程的内存（默认是 512m） SPARK_DAEMON_JAVA_OPTS Spark master 和 worker 守护进程的 JVM 选项，格式是 “-Dx=y”（默认为 none） SPARK_PUBLIC_DNS Spark master 和 worker 公共的 DNS 名（默认是 none） 注意，启动脚本还不支持 windows。为了在 windows 上启动 Spark 集群，需要手动启动 master 和 workers。 SPARK_MASTER_OPTS 支持以下系统属性： 参数 默认 说明 spark.deploy.retainedApplications 200 展示完成的应用程序的最大数目。老的应用程序会被删除以满足该限制 spark.deploy.retainedDrivers 200 展示完成的 drivers 的最大数目。老的应用程序会被删除以满足该限制 spark.deploy.spreadOut true 这个选项控制独立的集群管理器是应该跨节点传递应用程序还是应努力将程序整合到尽可能少的节点上。在 HDFS 中，传递程序是数据本地化更好的选择，但是，对于计算密集型的负载，整合会更有效率。 spark.deploy.defaultCores （infinite） 在 Spark 独立模式下，给应用程序的默认核数（如果没有设置 spark.cores.max）。如果没有设置，应用程序总数获得所有可用的核，除非设置了 spark.cores.max。在共享集群上设置较低的核数，可用防止用户默认抓住整个集群 spark.deploy.maxExecutorRetries 10 限制上的独立的集群管理器中删除有问题的应用程序之前可能发生的后端到回执行人失败的最大数量。应用程序将永远不会被删除，如果有任何正在运行的执行人。如果一个应用程序的经验超过 spark.deploy.maxExecutorRetries 连续的失败，没有遗嘱执行人成功启动这些失败之间运行，并且应用程序没有运行执行人那么独立的集群管理器将删除该应用程序并将其标记为失败。要禁用此自动删除，设置 spark.deploy.maxExecutorRetries 为 -1 spark.worker.timeout 60 独立部署的 master 认为 worker 失败（没有收到心跳信息）的间隔时间 SPARK_WORKER_OPTS 支持以下系统属性： 参数 默认 说明 spark.worker.cleanup.enabled false 周期性的清空 worker / 应用程序目录。注意，这仅仅影响独立部署模式。不管应用程序是否还在执行，用于程序目录都会被清空 spark.worker.cleanup.interval 1800（30 分钟） 在本地机器上，worker 清空老的应用程序工作目录的时间间隔 spark.worker.cleanup.appDataTtl 7 24 3600（7 天） 每个 worker 中应用程序工作目录的保留时间。这个时间依赖于你可用磁盘空间的大小。应用程序日志和 jar 包上传到每个应用程序的工作目录。随着时间的推移，工作目录会很快的填满磁盘空间，特别是如果你运行的作业很频繁 spark.worker.ui.compressedLogFileLengthCacheSize 100 For compressed log files, the uncompressed file can only be computed by uncompressing the files. Spark caches the uncompressed file size of compressed log files. This property controls the cache size. 提交应用程序到集群为了简单的运行 Spark 的应用程序，只需要通过 SparkContext Constructor spark://IP:PORT 这个 URL 。 集群的交互式运行可以通过 Spark shell，运行下面命名: 1./bin/spark-shell --master spark://IP:PORT 你也可以通过选项 –total-executor-cores 传递参数去控制 spark-shell 的核数 启动 Spark 应用程序spark-submit 脚本提供了最直接的方式将一个 Spark 应用程序提交到集群。对于独立部署的集群，Spark 目前支持两种部署模式。在 client 模式中，driver 与客户端提交应用程序运行在同一个进程中。然而，在 cluster 模式中，driver 在集群的某个 worker 进程中启动，一旦客户端进程完成了已提交任务，它就会立即退出，并不会等到应用程序完成后再退出。 如果你的应用程序通过 Spark submit 启动，你的应用程序 jar 包将会自动分发到所有的 worker 节点。对于你的应用程序依赖的其它 jar 包，你应该用 –jars 符号指定（如 –jars jar1,jar2）。 更多配置详见 Spark Configuration. 另外，如果你的应用程序以非 0 (non-zero) 状态退出，独立集群模式支持重启程序。要支持自动重启，需要向 spark-submit 传递 –supervise 标志。如果你想杀掉一个重复失败的应用程序，你可以使用如下方式： 1./bin/spark-class org.apache.spark.deploy.Client kill &lt;master url&gt; &lt;driver ID&gt; 你可以在 Master 的 web UI 上 http://\:8080 找到 driver ID。 资源调度独立部署的集群模式仅仅支持简单的 FIFO 调度器。然而，为了允许多个并行的用户，你能够控制每个应用程序能用的最大资源数。在默认情况下，它将获得集群的所有核，这只有在某一时刻只允许一个应用程序才有意义。你可以通过 spark.cores.max 在 SparkConf 中设置核数。 12345val conf = new SparkConf() .setMaster(...) .setAppName(...) .set("spark.cores.max", "10")val sc = new SparkContext(conf) 另外，你可以在集群的 master 进程中配置 spark.deploy.defaultCores 来改变默认的值。在 conf/spark-env.sh 添加下面的行： 1export SPARK_MASTER_OPTS="-Dspark.deploy.defaultCores=&lt;value&gt;" 这在用户没有配置最大核数的共享集群中是有用的。 监控和日志记录Spark 的独立模式提供了一个基于 Web 的用户界面来监控集群。Master 和 每个 Worker 都有自己的 web 用户界面，显示集群和作业统计信息。默认情况下你可以在 8080 端口访问 Web UI 的主端口也可以在配置文件中或通过命令行选项进行更改。 此外，对于每个作业详细的日志也会输出每个 worker 目录下（默认情况工作目录 SPARK_HOME/work）。你会看到两个文件为每个作业，stdout 并 stderr 与所有输出它写信给其控制台。 与 hadoop 的集成你可以在 Hadoop 上单独启动一个 Spark 服务。若要从 Spark 访问 Hadoop 的数据，只需使用一个 HDFS:// URL（通常是 hdfs://\:9000/path，也可以找到 Hadoop 的 Web UI 找到 Namenode 的 URL）。另外，您也可以单独启动 Spark 集群，而且还有它通过网络进行访问 HDFS， 但这将是比本地磁盘访问速度慢，最好是在一个局域网之内（例如，Hadoop 各项服务或者各个节点在同一个机架上）。 端口配置和网络安全Spark 对网络的需求比较高，对网络环境有紧密联系，需要严谨设置防火墙。端口配置列表，请参阅 安全网页。 高可用性默认情况下，独立的调度集群对 worker 失败是有弹性的（在 Spark 本身的范围内是有弹性的，对丢失的工作通过转移它到另外的 worker 来解决）。然而，调度器通过 master 去执行调度决定， 这会造成单点故障：如果 master 死了，新的应用程序就无法创建。为了避免这个，我们有两个高可用的模式。 用 ZooKeeper 的备用 master概述利用 ZooKeeper 去支持 leader 选举以及一些状态存储，你能够在你的集群中启动多个 master，这些 master 连接到同一个 ZooKeeper 实例上。一个被选为 “leader” ，其它的保持备用模式。如果当前的 leader 死了，另一个 master 将会被选中，恢复老 master 的状态，然后恢复调度。整个的恢复过程大概需要 1 到 2 分钟。注意，这个恢复时间仅仅会影响调度新的应用程序——运行在失败 master 中的 应用程序将不受影响。 了解更多关于如何开始使用的 ZooKeeper 这里。 配置为了开启这个恢复模式，你可以用下面的属性在 spark-env 中设置 SPARK_DAEMON_JAVA_OPTS spark.deploy.recoveryMode 及相关 spark.deploy.zookeeper.* 的配置。有关这些配置的详细信息，请参阅 配置文档 可能的陷阱：如果你在集群中有多个 masters，但是没有用 zookeeper 正确的配置这些 masters，这些 masters 不会发现彼此，会认为它们都是 leaders。这将会造成一个不健康的集群状态（因为所有的 master 都会独立的调度）。 细节zookeeper 集群启动之后，开启高可用是简单的。在相同的 zookeeper 配置（zookeeper URL 和目录）下，在不同的节点上简单地启动多个 master 进程。master 可以随时添加和删除。 为了调度新的应用程序或者添加 worker 到集群，它需要知道当前 leader 的 IP 地址。这可以通过简单的传递一个 master 列表来完成。例如，你可能启动你的 SparkContext 指向 spark://host1:port1,host2:port2。 这将造成你的 SparkContext 同时注册这两个 master – 如果 host1 死了，这个配置文件将一直是正确的，因为我们将找到新的 leader-host2。 “registering with a Master” 和正常操作之间有重要的区别。当启动时，一个应用程序或者 worker 需要能够发现和注册当前的 leader master。一旦它成功注册，它就在系统中了。如果 错误发生，新的 leader 将会接触所有之前注册的应用程序和 worker，通知他们领导关系的变化，所以它们甚至不需要事先知道新启动的 leader 的存在。 由于这个属性的存在，新的 master 可以在任何时候创建。你唯一需要担心的问题是新的应用程序和 workers 能够发现它并将它注册进来以防它成为 leader master。 用本地文件系统做单节点恢复概述zookeeper 是生产环境下最好的选择，但是如果你想在 master 死掉后重启它，FILESYSTEM (文件系统) 模式可以解决。当应用程序和 worker 注册，它们拥有足够的状态写入提供的目录，以至于在重启 master 进程时它们能够恢复。 配置为了开启这个恢复模式，你可以用下面的属性在 spark-env 中设置 SPARK_DAEMON_JAVA_OPTS。 System property Meaning spark.deploy.recoveryMode 设置为 FILESYSTEM 开启单节点恢复模式（默认为 none） spark.deploy.recoveryDirectory 用来恢复状态的目录 细节 这个解决方案可以和监控器 / 管理器（如 monit）相配合，或者仅仅通过重启开启手动恢复。 虽然文件系统的恢复似乎比没有做任何恢复要好，但对于特定的开发或实验目的，这种模式可能是次优的。特别是，通过 stop-master.sh 杀掉 master 不会清除它的恢复状态，所以，不管你何时启动一个新的 master，它都将进入恢复模式。这可能使启动时间增加到 1 分钟。 虽然它不是官方支持的方式，你也可以创建一个NFS目录作为恢复目录。如果原始的 master节点完全死掉，你可以在不同的节点启动 master，它可以正确的恢复之前注册的所有应用程序和 workers。未来的应用程序会发现这个新的 master。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark Standalone 模式案例]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-standalone-example%2F</url>
      <content type="text"><![CDATA[先决条件：环境 rhel 7.2 jdk-8u102-linux-x64 spark-2.0.2-bin-hadoop2.7 Scala 2.11，注意：2.11.x 版本是不兼容的，见官网：http://spark.apache.org/docs/latest/。 准备 master 主机和 worker 分机 server1 机器：10.8.26.197，master server2 机器：10.8.26.196，worker server3 机器：10.8.26.195，worker 修改 host1234567[root@server1 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.8.26.197 server110.8.26.196 server210.8.26.195 server3 关闭所有节点机防火墙123# systemctl status firewalld# systemctl stop firewalld# systemctl disable firewalld 启动集群主节点1./sbin/start-master.sh 查看输出日志： 1234567891011121314151617181920212223cat logs/spark....[root@server1 spark-2.0.2-bin-hadoop2.7]# cat logs/spark-root-org.apache.spark.deploy.master.Master-1-server1.outSpark Command: /usr/local/jdk1.8.0_102/bin/java -cp /usr/local/spark-2.0.2-bin-hadoop2.7/conf/:/usr/local/spark-2.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host server1 --port 7077 --webui-port 8080========================================Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties16/12/26 10:35:34 INFO Master: Started daemon with process name: 8671@server116/12/26 10:35:34 INFO SignalUtils: Registered signal handler for TERM16/12/26 10:35:34 INFO SignalUtils: Registered signal handler for HUP16/12/26 10:35:34 INFO SignalUtils: Registered signal handler for INT16/12/26 10:35:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable16/12/26 10:35:35 INFO SecurityManager: Changing view acls to: root16/12/26 10:35:35 INFO SecurityManager: Changing modify acls to: root16/12/26 10:35:35 INFO SecurityManager: Changing view acls groups to:16/12/26 10:35:35 INFO SecurityManager: Changing modify acls groups to:16/12/26 10:35:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set()16/12/26 10:35:36 INFO Utils: Successfully started service 'sparkMaster' on port 7077.16/12/26 10:35:36 INFO Master: Starting Spark master at spark://server1:707716/12/26 10:35:36 INFO Master: Running Spark version 2.0.216/12/26 10:35:36 INFO Utils: Successfully started service 'MasterUI' on port 8080.16/12/26 10:35:36 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://10.8.26.197:808016/12/26 10:35:36 INFO Utils: Successfully started service on port 6066.16/12/26 10:35:36 INFO StandaloneRestServer: Started REST server for submitting applications on port 606616/12/26 10:35:37 INFO Master: I have been elected leader! New state: ALIVE 通过 master-ip:8080 访问 master 的 web UI 各 worker 节点1./sbin/start-slave.sh spark://server1:7077 节点输出日志： 123456789101112131415161718192021222324cat logs/spark....[root@server2 spark-2.0.2-bin-hadoop2.7]# cat logs/spark-root-org.apache.spark.deploy.worker.Worker-1-server2.outSpark Command: /usr/local/jdk1.8.0_102/bin/java -cp /usr/local/spark-2.0.2-bin-hadoop2.7/conf/:/usr/local/spark-2.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://server1:7077========================================Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties16/12/26 10:43:04 INFO Worker: Started daemon with process name: 7466@server216/12/26 10:43:04 INFO SignalUtils: Registered signal handler for TERM16/12/26 10:43:04 INFO SignalUtils: Registered signal handler for HUP16/12/26 10:43:04 INFO SignalUtils: Registered signal handler for INT16/12/26 10:43:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable16/12/26 10:43:05 INFO SecurityManager: Changing view acls to: root16/12/26 10:43:05 INFO SecurityManager: Changing modify acls to: root16/12/26 10:43:05 INFO SecurityManager: Changing view acls groups to:16/12/26 10:43:05 INFO SecurityManager: Changing modify acls groups to:16/12/26 10:43:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set()16/12/26 10:43:06 INFO Utils: Successfully started service 'sparkWorker' on port 47422.16/12/26 10:43:06 INFO Worker: Starting Spark worker 10.8.26.196:47422 with 1 cores, 1024.0 MB RAM16/12/26 10:43:06 INFO Worker: Running Spark version 2.0.216/12/26 10:43:06 INFO Worker: Spark home: /usr/local/spark-2.0.2-bin-hadoop2.716/12/26 10:43:06 INFO Utils: Successfully started service 'WorkerUI' on port 8081.16/12/26 10:43:06 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://10.8.26.196:808116/12/26 10:43:06 INFO Worker: Connecting to master server1:7077...16/12/26 10:43:07 INFO TransportClientFactory: Successfully created connection to server1/10.8.26.197:7077 after 109 ms (0 ms spent in bootstraps)16/12/26 10:43:07 INFO Worker: Successfully registered with master spark://server1:7077 通过 master-ip:8080 访问 master 的 web UI 通过 worker-ip:8081 访问 worker 的 web UI 提交应用程序到集群集成 shell 测试环境切换至 bin 目录1[root@server1 spark-2.0.2-bin-hadoop2.7]# cd bin 进入运行在集群上的 spark 的 集成调试环境。Python 1234567891011121314151617[root@server1 bin]# ./pyspark --master spark://server1:7077Python 2.7.5 (default, Nov 6 2016, 00:28:07)[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2Type "help", "copyright", "credits" or "license" for more information.Using Spark's default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to "WARN".To adjust logging level use sc.setLogLevel(newLevel).16/12/26 10:59:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ '_/ /__ / .__/\_,_/_/ /_/\_\ version 2.0.2 /_/Using Python version 2.7.5 (default, Nov 6 2016 00:28:07)SparkSession available as 'spark'. 以统计文本行数为例： 123&gt;&gt;&gt; textFile=sc.textFile("../README.md")&gt;&gt;&gt; textFile.count()99 输出 README.md 中共有 99 行 scala 123456[root@server1 spark-2.0.2-bin-hadoop2.7]# ./bin/spark-shell --master spark://server1:7077scala&gt; val textFile=sc.textFile("README.md")textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[9] at textFile at &lt;console&gt;:24scala&gt; textFile.count()res0: Long = 99 可以在 master 的 web 界面里面看到任务执行情况 也可以在 worker 的 web 界面里面看单个 worker 的情况]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark on YARN 部署案例]]></title>
      <url>%2F2016%2F12%2F24%2Fspark-running-on-yarn-example%2F</url>
      <content type="text"><![CDATA[环境准备1. 服务器角色分配 ip hostname role 10.8.26.197 server1 主名字节点 (NodeManager) 10.8.26.196 server2 备名字节点 (SecondaryNameNode) 10.8.26.196 server2 数据字节点 (DataNode) 2. 软件设施 jdk1.8.0_102 scala-2.11.0: hadoop-2.7.0 spark-2.0.2-bin-hadoop2.7：对应 scala 版本不能是 scala-2.11.x 3. HOSTS 设置在每台服务器的 “/etc/hosts” 文件，添加如下内容： 12310.8.26.197 server1 10.8.26.196 server2 10.8.26.196 server2 4. SSH 免密码登录Redhat 7/CentOS 7 SSH 免密登录 Hadoop YARN 分布式集群配置 注：1-8 所有节点都做同样配置 1. 环境变量设置1234567891011121314151617181920212223242526# vim /etc/profile# hadoop env setexport HADOOP_HOME=/usr/local/hadoop-2.7.0export HADOOP_PID_DIR=/data/hadoop/pidsexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"export HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HDFS_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native# jdk env setexport JAVA_HOME=/usr/local/jdk1.8.0_102export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar# scala env setexport SCALA_HOME=/usr/local/scala-2.11.0export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$SCALA_HOME/bin:$PATH 变量立即生效 1# source /etc/profile 2. 相关路径创建123# mkdir -p /data/hadoop/&#123;pids,storage&#125;# mkdir -p /data/hadoop/storage/&#123;hdfs,tmp&#125;# mkdir -p /data/hadoop/storage/hdfs/&#123;name,data&#125; 3. 配置 core-site.xml 目录：$HADOOP_HOME/etc/hadoop/core-site.xml 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://server1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/storage/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.native.lib&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. 配置 hdfs-site.xml 目录：$HADOOP_HOME/etc/hadoop/hdfs-site.xml 1234567891011121314151617181920212223&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;server2:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/storage/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/storage/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. 配置 mapred-site.xml 目录：$HADOOP_HOME/etc/hadoop/mapred-site.xml 123456789101112131415&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;server1:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;server1:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6. 配置 yarn-site.xml 目录：$HADOOP_HOME/etc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;server1:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;server1:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;server1:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;server1:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;server1:80&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7. 配置 hadoop-env.sh、mapred-env.sh、yarn-env.sh 均在文件开头添加 目录： $HADOOP_HOME/etc/hadoop/hadoop-env.sh $HADOOP_HOME/etc/hadoop/mapred-env.sh $HADOOP_HOME/etc/hadoop/yarn-env.sh 在以上三个文件开头添加如下内容： 12345678910111213141516171819202122export JAVA_HOME=/usr/local/jdk1.8.0_102export CLASS_PATH=$JAVA_HOME/lib:$JAVA_HOME/jre/libexport HADOOP_HOME=/usr/local/hadoop-2.7.0export HADOOP_PID_DIR=/data/hadoop/pidsexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"export HADOOP_PREFIX=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HDFS_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 8. 数据节点配置1234# vim $HADOOP_HOME/etc/hadoop/slavesserver1server2server3 9. Hadoop 简单测试 工作目录 master $HADOOP_HOME 1# cd $HADOOP_HOME 首次启动集群时，做如下操作 [主名字节点上执行] 123# hdfs namenode -format# sbin/start-dfs.sh# sbin/start-yarn.sh 检查进程是否正常启动 主名字节点 - server1： 123456# jps11842 Jps11363 ResourceManager10981 NameNode11113 DataNode11471 NodeManager 备名字节点 - server2： 12345# jps7172 SecondaryNameNode7252 NodeManager7428 Jps7063 DataNode 数据节点 - server3： 1234# jps6523 NodeManager6699 Jps6412 DataNode hdfs 与 mapreduce 测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# hdfs dfs -mkdir -p /user/root# hdfs dfs -put ~/text /user/root# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar wordcount /user/root /user/out16/12/30 14:01:51 INFO client.RMProxy: Connecting to ResourceManager at server1/10.8.26.197:803216/12/30 14:01:55 INFO input.FileInputFormat: Total input paths to process : 116/12/30 14:01:55 INFO mapreduce.JobSubmitter: number of splits:116/12/30 14:01:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1483076482233_000116/12/30 14:01:58 INFO impl.YarnClientImpl: Submitted application application_1483076482233_000116/12/30 14:01:58 INFO mapreduce.Job: The url to track the job: http://server1:80/proxy/application_1483076482233_0001/16/12/30 14:01:58 INFO mapreduce.Job: Running job: job_1483076482233_000116/12/30 14:02:23 INFO mapreduce.Job: Job job_1483076482233_0001 running in uber mode : false16/12/30 14:02:24 INFO mapreduce.Job: map 0% reduce 0%16/12/30 14:02:36 INFO mapreduce.Job: map 100% reduce 0%16/12/30 14:02:44 INFO mapreduce.Job: map 100% reduce 100%16/12/30 14:02:45 INFO mapreduce.Job: Job job_1483076482233_0001 completed successfully16/12/30 14:02:46 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=242 FILE: Number of bytes written=230317 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=493 HDFS: Number of bytes written=172 HDFS: Number of read operations=6 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Launched reduce tasks=1 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=7899 Total time spent by all reduces in occupied slots (ms)=6754 Total time spent by all map tasks (ms)=7899 Total time spent by all reduce tasks (ms)=6754 Total vcore-seconds taken by all map tasks=7899 Total vcore-seconds taken by all reduce tasks=6754 Total megabyte-seconds taken by all map tasks=8088576 Total megabyte-seconds taken by all reduce tasks=6916096 Map-Reduce Framework Map input records=8 Map output records=56 Map output bytes=596 Map output materialized bytes=242 Input split bytes=99 Combine input records=56 Combine output records=16 Reduce input groups=16 Reduce shuffle bytes=242 Reduce input records=16 Reduce output records=16 Spilled Records=32 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=231 CPU time spent (ms)=1720 Physical memory (bytes) snapshot=293462016 Virtual memory (bytes) snapshot=4158427136 Total committed heap usage (bytes)=139976704 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=394 File Output Format Counters Bytes Written=172 执行完成后查看输出， 1234# hdfs dfs -ls /user/outFound 2 items-rw-r--r-- 3 root supergroup 0 2016-12-30 14:02 /user/out/_SUCCESS-rw-r--r-- 3 root supergroup 172 2016-12-30 14:02 /user/out/part-r-00000 也可以通过 UI （http://server1/cluster/apps） 查看： HDFS 信息查看 12# hdfs dfsadmin -report# hdfs fsck / -files -blocks UI（http://server1:50070/） ： 集群的后续维护 12# sbin/start-all.sh# sbin/stop-all.sh 监控页面 URL http://10.8.26.197:80 Spark 分布式集群配置 注：所有节点都做同样配置 1. Spark 相关配置Spark 环境变量设置12345# vim /etc/profile# spark env setexport SPARK_HOME=/usr/local/spark-2.0.2-bin-hadoop2.7export PATH=$SPARK_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$SCALA_HOME/bin:$PATH 1# source /etc/profile 配置 spark-env.sh1234567# cd $SPARK_HOME/conf# mv spark-env.sh.template spark-env.sh# vim spark-env.sh## 添加如下内容export JAVA_HOME=/usr/local/jdk1.8.0_102export SCALA_HOME=/usr/local/scala-2.11.0export HADOOP_HOME=/usr/local/hadoop-2.7.0 配置 worker 节点的主机名列表1# cd $SPARK_HOME/conf 1234# vim slavesserver1server2server3 其他配置12# cd $SPARK_HOME/conf# mv log4j.properties.template log4j.properties 在 Master 节点上执行1# cd $SPARK_HOME &amp;&amp; sbin/start-all.sh 检查进程是否启动在 master 节点上出现 “Master”，在 slave 节点上出现 “Worker” Master 节点：12345678[root@server1 spark-2.0.2-bin-hadoop2.7]# jps11363 ResourceManager10981 NameNode13176 Master13256 Worker11113 DataNode13435 Jps11471 NodeManager Slave 节点：123456[root@server2 conf]# jps7172 SecondaryNameNode7252 NodeManager7063 DataNode8988 Worker9133 Jps 相关测试监控页面 URL http://10.8.26.197:8080/ 或 http://server1:8080/ 切换到 “$SPARK_HOME/bin” 目录 1. 本地模式 1# .bin/run-example org.apache.spark.examples.SparkPi local 2. 普通集群模式 12# ./bin/run-example org.apache.spark.examples.SparkPi spark://10.8.26.197:7077# ./bin/run-example org.apache.spark.examples.SparkLR spark://10.8.26.197:7077 3. 结合 HDFS 的集群模式 工作目录 $SPARK_HOME 123456789scala&gt; val file=sc.textFile("hdfs://server1:9000/user/root/README.md")file: org.apache.spark.rdd.RDD[String] = hdfs://server1:9000/user/root/README.md MapPartitionsRDD[5] at textFile at &lt;console&gt;:24scala&gt; val count = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_+_)count: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:26scala&gt; count.collect()res3: Array[(String, Int)] = Array((package,1), (this,1), (Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (YARN,,1), (locally,2), (changed,1), (locally.,1), (sc.parallelize(1,1), (only,1), (Configuration,1), (This,2), (basic,1), (first,1), (learning,,1), ([Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse),1), (documentation,3), (graph,1), (Hive,2), (several,1), (["Specifying,1), ("yarn",1), (page](http://spark.apache.org/documentation.html),1), ([params]`.,1), ([project,2), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation...scala&gt; :quit 4. 基于 YARN 模式 12345678# ./bin/spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ --driver-memory 1g \ --executor-memory 1g \ --executor-cores 1 \ examples/jars/spark-examples*.jar \ 10 执行结果： $HADOOP_HOME/logs/userlogs/application_*/container*_***/stdout 或 http://10.8.26.197/logs/userlogs/application_1483076482233_0009/container_1483076482233_0009_01_000001/stdout]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[配置 Hadoop 单节点集群]]></title>
      <url>%2F2016%2F12%2F24%2Fhadoop-single-cluster%2F</url>
      <content type="text"><![CDATA[目的本文描述如何配置一个单节点 Hadoop 集群，使得你可以使用 Hadoop MapReduce 和 Hadoop 分布式文件系统（HDFS）快速执行简单的操作。 先决条件平台支持 GNU/Linux windows 所需软件Linux 所需软件： Java SSH 软件安装以 ubuntu 为例： 12$ sudo apt-get install ssh$ sudo apt-get install rsync 下载在 Apache Download Mirrors 下载最新的稳定版 Hadoop。 准备启动 Hadoop 集群解压已下载 Hadoop 包， 编辑 etc/hadoop/hadoop-env.sh 文件添加如下变量： 12345# set to the root of your Java installationexport JAVA_HOME=/usr/java/latest---------------------------------------------## 例如export JAVA_HOME=/usr/local/jdk1.8.0_102 尝试执行如下命令 1[root@server1 hadoop-2.7.0]# bin/hadoop 这条命令将显示 hadoop 脚本如何使用的。 现在已经装备好启动 Hadoop 集群了，集群支持一下三种模式： 单机模式 - Local (Standalone) Mode 伪集群模式 - Pseudo-Distributed Mode 集群分布式模式 - Fully-Distributed Mode 单机运行 - Standalone Operation1234[root@server1 hadoop-2.7.0]# mkdir input[root@server1 hadoop-2.7.0]# cp etc/hadoop/*.xml input[root@server1 hadoop-2.7.0]# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar grep input output 'dfs[a-z.]+'[root@server1 hadoop-2.7.0]# cat output/* 伪分布模式运行 - Pseduo-Distributed Operation配置修改下面的文件 etc/hadoop/core-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 设置 SSH 免密登录 - Setup passphraseless ssh现在，检查是否可以 SSH 免密登录 localhost: 1# ssh localhost 如果登录失败，执行以下命令： 123# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa# cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys# export HADOOP\_PREFIX=/usr/local/hadoop ps: 免密登录配置详情见 Redhat7/CentOS7 SSH 免密登录 执行1. 格式化文件系统： 1[root@server1 hadoop-2.7.0]# bin/hdfs namenode -format Hadoop 临时文件放在 /tmp/hadoop-root/ 目录下，如果需要可以在重置前删除。 2. 启动 NameNode daemon 和 DataNode daemon: 1[root@server1 hadoop-2.7.0]# sbin/start-dfs.sh Hadoop daemon 日志会将输出到 $HADOOP_LOG_DIR (默认 $HADOOP_HOME/logs)。 3. web 界面浏览 NameNode；默认 URL： NameNode - http://localhost:50070/ 4. 为执行 MapReduce jobs 创建所需的 HDFS 目录： 1234[root@server1 hadoop-2.7.0]# bin/hdfs dfs -mkdir /user[root@server1 hadoop-2.7.0]# bin/hdfs dfs -mkdir /user/&lt;username&gt;## 这里我用 tpfs 代替 username:[root@server1 hadoop-2.7.0]# bin/hdfs dfs -mkdir /user/root/ 5. 将 etc/hadoop 目录复制到分布式文件系统（HDFS）的 input 目录下 1[root@server1 hadoop-2.7.0]# bin/hdfs dfs -put etc/hadoop input 6. 执行一个已经提供的案例 1[root@server1 hadoop-2.7.0]# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar grep input output 'dfs[a-z.]+' 7. 检查输出文件，拷贝 HDFS 上的输出文件到本机，执行如下命令： 123[root@server1 hadoop-2.7.0]# bin/hdfs dfs -get output output[root@server1 hadoop-2.7.0]# ls output/part-r-00000 _SUCCESS 或 在 HDFS 上查看输出文件 123[root@server1 hadoop-2.7.0]# bin/hdfs dfs -ls output/*-rw-r--r-- 1 root supergroup 0 2016-12-29 11:21 output/part-r-00000-rw-r--r-- 1 root supergroup 0 2016-12-29 11:21 output/_SUCCESS 8. 当你执行完成后，执行如下停止 daemons： 1[root@server1 hadoop-2.7.0]# sbin/stop-dfs.sh 集群分布式运行详情见Cluster Setup]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在 Kubernetes 上部署 Harbor]]></title>
      <url>%2F2016%2F12%2F24%2Fharbor-kubernetes-deployment%2F</url>
      <content type="text"><![CDATA[在 Kubernetes 上部署 HarborTo deploy Harbor on Kubernetes, it requires some additional steps because When Harbor registry uses https, so we need cert or workaround to avoid errors like this: 123Error response from daemon: invalid registry endpoint https://&#123;HOST&#125;/v0/: unable to ping registry endpoint https://&#123;HOST&#125;/v0/v2 ping attempt failed with error: Get https://&#123;HOST&#125;/v2/: EOFv1 ping attempt failed with error: Get https://&#123;HOST&#125;/v1/_ping: EOF. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry &#123;HOST&#125;` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/&#123;HOST&#125;/ca.crt There is a workaround if you don’t have a cert. The workaround is to add the host into the list of insecure registry by editting the file:12```bashsudo vi /etc/default/docker add the line at the end of file: 1DOCKER_OPTS="$DOCKER_OPTS --insecure-registry=&#123;HOST&#125;" restart docker service 1sudo service docker restart The registry config file needs to have the IP (or DNS name) of the registry, but on Kubernetes, you don’t know the IP before the service is created. There are several workarounds to solve this problem for now: Use DNS name and link the DNS name with the IP after the service is created. Rebuild the registry image with the service IP after the service is created and use rolling-update``` to update to the new image.1234To start Harbor on Kubernetes, you first need to build the docker images. The docker images for deploying Harbor on Kubernetes depends on the docker images to deploy Harbor with docker-compose. So the first step is to build docker images with docker-compose. Before actually building the images, you need to first adjust the [configuration](https://github.com/vmware/harbor/blob/master/make/harbor.cfg):- Change the [hostname](https://github.com/vmware/harbor/blob/master/make/harbor.cfg#L5) to ```localhost Adjust the email settings according to your needs. Then you can run the following commends to build docker images:1234567891011cd make./preparedocker-compose builddocker build -f kubernetes/dockerfiles/proxy-dockerfile -t &#123;your_account&#125;/proxy .docker build -f kubernetes/dockerfiles/registry-dockerfile -t &#123;your_account&#125;/registry .docker build -f kubernetes/dockerfiles/ui-dockerfile -t &#123;your_account&#125;/deploy_ui .docker tag deploy_mysql &#123;your_account&#125;/deploy_mysqldocker push &#123;your_account&#125;/proxydocker push &#123;your_account&#125;/registrydocker push &#123;your_account&#125;/deploy_uidocker push &#123;your_account&#125;/deploy_mysql where “your_account” is your own registry. Then you need to update the “image” field in the files at:12345```make/kubernetes/mysql-rc.yamlmake/kubernetes/proxy-rc.yamlmake/kubernetes/registry-rc.yamlmake/kubernetes/ui-rc.yaml Further more, the following configuration could be changed according to your need: harbor_admin_password: The password for the administrator of Harbor, by default the password is Harbor12345. You can changed it here. auth_mode: The authentication mode of Harbor. By default it is db_auth, i.e. the credentials are stored in a database. Please set it to ldap_auth if you want to verify user’s credentials against an LDAP server. You can change the configuration here. ldap_url: The URL for LDAP endpoint, for example ldaps://ldap.mydomain.com. It is only used when auth_mode is set to ldap_auth. It could be changed here. ldap_basedn: The basedn template for verifying the user’s credentials against LDAP, for example uid=%s,ou=people,dc=mydomain,dc=com. It is only used when auth_mode is set to ldap_auth. It could be changed here. db_password: The password of root user of mySQL database. Change this password for any production use. You need to change both here and here to make the change. Please note, you need to change the before building the docker images.12Finally you can start the jobs by running: kubectl create -f make/kubernetes```]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之用户与文件]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-users-and-files%2F</url>
      <content type="text"><![CDATA[linux 系统是一个多用户多任务的分时操作系统，但系统并不能识别人，它通过账号来区别每个用户。每个 linux 系统在安装的过程中都要为 root 账号设置密码，这个 root 即为系统的第一个账号。每一个用这个账号登录系统的用户都是超级管理员，他们对此系统有绝对的控制权。通过向系统管理员进行申请，还可以为系统创建普通账号。每个用普通账号登录系统的用户，对系统都只有部分控制权。 我们知道计算机中的数据是以二进制 0、1 的形式存储在硬件之上的。在 linux 中，为了管理的方便，系统将这些数据组织成目录和文件，并以一个树形的结构呈现给用户。如下图所示： 其中处于顶端的 / 是根目录，linux 下所有的文件均起始于根目录。另外很重要的一点，linux 中不仅普通文档是文件，目录是文件，甚至设备、进程等等都被抽象成文件。这样做的目的是为了简化操作和方便管理。 于是，本文开始所说的控制权，即为用户对系统中文件的控制权。通常所说的某某文件的权限，是针对特定用户而言的。 另外，每一个登录的用户，在任意的时刻均处于某个目录之内，称为当前目录 (current directory)。用户在刚刚登录的时候所处的目录是家目录，root 用户的家目录是 / root, 普通用户的家目录通常为 / home/user_name。在这里第一个字符 / 即是上文所说的根目录，root 和 home 是根目录下的两个子目录名，要注意 home 后面的 / 是目录分隔符，而不是目录名的一部分，user_name 是普通用户家目录的名字。 下面我们来看具体命令： 1、pwd 打印当前目录1pwd [OPTION]... 例如： 12$ pwd/root 2、cd 切换目录1cd [DIR] 例如切换到根目录然后打印当前目录 (注意命令提示符的变化)： 123$ cd /$ pwd/ 这两个命令非常简单，简单到它们的选项都不常用，其中 cd 命令后面跟一个路径名。这个路径名可以是 “绝对的” 也可以是“相对的”。绝对的表示成以 / 为开头的路径名，如命令 cd /usr/local/src 中的路径名；相对的表示成相对于当前目录的路径名，若将 linux 中目录的包含与被包含关系比喻成父子关系的话，符号.. 代表的是父目录，符号. 代表当前目录。 例：假设当前所处目录为 / usr/local/src，那么切换到根目录可以用两种方法：cd / 和 cd ../../.. 123$ cd ../../..$ pwd/ 然后再切换回 root 的家目录： cd root 和 cd ./root 123$ cd ./root$ pwd/root 另外如果 cd 后面任何参数都没有的时候，执行的效果是切换回家目录： 123$ cd$ pwd/root 3、ls 列出目录内容1ls [OPTION]... [FILE]... 当命令 ls 后不跟任何参数的时候显示当前目录的内容 12$ lsanaconda-ks.cfg install.log install.log.syslog 上面的例子显示了 / root 目录下的三个文件 anaconda-ks.cfg、anaconda-ks.cfg、anaconda-ks.cfg。选项 - l 可以使 ls 命令的结果以长格式显示： 12345$ ls -ltotal 84-rw------- 1 root root 1666 Jan 14 2016 anaconda-ks.cfg-rw-r--r-- 1 root root 55745 Jan 14 2016 install.log-rw-r--r-- 1 root root 5039 Jan 14 2016 install.log.syslog 显示结果的意思后述。 ## 4、mkdir 创建目录 1mkdir [OPTION]... DIRECTORY... 通常的使用方法是命令名之后直接跟目录名 (可以多个)，这里说一下 linux 文件命名的规则：linux 允许文件名使用除字符 / 之外的所有字符，文件名的最大字符数为 255(中文字符为 127)，linux 不鼓励在文件名中出现特殊字符 (容易引起混淆)，文件名对大小写敏感。文件或目录数量限制与所使用的文件系统有关。 如当前目录下创建 temp 目录并用 ls 查看： 123$ mkdir temp$ lsanaconda-ks.cfg install.log install.log.syslog temp 选项 - p 可以递归地创建子目录，如进入 temp 并创建目录 dir1 和 dir2，dir1 的子目录 test： 1234567$ cd temp$ mkdir -p dir1/test dir2$ lsdir1 dir2$ cd dir1$ lstest 5、touch “创建文件” 1touch [OPTION]... FILE... 其实此命令作用是修改文件时间，当指定的文件不存在时就会创建新文件。由于文件时间的更改可以通过许多其它途径，反而许多用户都误以为它就是创建文件的命令。如在 temp 目录下创建文件 file1 在 temp 的子目录 dir1 下创建文件 file2： 123456$ touch file1 dir1/file2$ lsdir1 dir2 file1$ cd dir1$ lsfile2 test 6、useradd 添加账号1useradd [options] name 如创建一个名叫 learner 的账号： 1$ useradd learner useradd 命令默认在创建用户账号的同时也会创建用户的家目录，同时更新系统中与用户相关的配置文件 (linux 中有许多配置文件，它们的作用是为软件运行设置环境信息、参数等，它们通常是纯文本的格式，方便用户变更其内容以改变软件运行环境。在 linux 中，大多数配置文件都处于目录 / etc 内，如与用户管理相关的配置文件：/etc/passwd，/etc/group，/etc/shadow，/etc/gshadow 等)。 让我们进入新创建的用户家目录并用 ls 命令查看目录内容： 123$ cd /home/learner$ ls$ 终端上并没有打印出任何信息，试试 ls 的 -a 选项： 12$ ls -a. .. .bash_logout .bash_profile .bashrc 选项 -a 作用是显示目录下所有文件，包括当前目录. 和父目录..，linux 中以. 开头的文件是隐藏文件。在这里的三个隐藏文件是用户 learner 登录系统时所要用到的配置文件。 7、passwd 添加或更改账号口令1passwd [OPTION]... [NAME] 注意通过命令 useradd 新添加的账号并不能马上进行登录，还必须为账号添加口令为新用户 learner 添加口令： 123456$ passwd learnerChanging password for user learner.New UNIX password: xxxxxx #此处的 xxxxxx 并不在屏幕上显示BAD PASSWORD: it is too simplistic/systematic #此处可能会给出密码太简单的警告Retype new UNIX password: xxxxxx #重复输入，此处的 xxxxxx 不在屏幕上显示passwd: all authentication tokens updated successfully. 当 passwd 命令后没有用户名直接执行时，它的作用是更改当前账号的口令。 8、cat 查看文件内容1cat [OPTION]... [FILE]... 如查看保存系统账号的配置文件 /etc/passwd 123456$ cat /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin....learner:x:1000:1000::/home/learner:/bin/bash 这里节选了部分输出，我们看到新创建的账号 learner 的信息在文件最后一行。文件中每一行都被: 分割为 7 列，拿第一行举例说明每一列所表示的含义： root 表示账号名。 x 是口令，在一些系统中，真正的口令加密保存在 /etc/shadow 里，这里保留 x 或 *。 0 是用户 ID。 0 是用户组 ID，对应着 / etc/group 文件中的一条记录。 root 是描述性信息。 /root 是用户家目录。 /bin/bash 是用户的登录 shell，每一个登录的用户，系统都要启动一个 shell 程序以供用户使用。对应于新创建的用户 learner 来说，它的用户 ID 是 1000，通常用户 ID(UID) 与用户名是一一对应的。root 的 UID 是 0。用户组 ID(GID) 如果在创建用户的时候没有被指定，那么系统会生成一个和 UID 号相同的 GID，并把新用户放到这个组里面。用户组的意义是为了给权限控制增加灵活性，比如把不同的用户归到一个组之内，然后使文件针对这个组设置权限。 系统中还有一些登录 shell 为 / sbin/nologin 的用户，这些用户是 “伪用户”，它们是不能登录的，它们的存在主要是为了方便管理，满足相应的系统进程对文件属主的要求。 9、head tail more less 查看内容这四个命令使用和 cat 类似，只是显示方式的区别。 head 从文件的第一行开始显示，默认显示 10 行，使用选项 - n 可以指定显示行数： 1234$ head -n 3 /etc/grouproot:x:0:bin:x:1:daemon:x:2: 显示文件 / etc/group 的前三行。 /etc/group 中每行被: 分隔成 4 列： 组名 口令，linux 中一般无组口令，此处一般为 x 或 * 组 ID(GID) 组内成员列表，多个用逗号分隔。如果字段为空表示用户组为 GID 的用户名。 tail 默认输出文件的倒数 10 行内容，也可以用选项 - n 指定行数： 12345$ tail -n 4 /etc/shadowpostfix:!!:16814::::::sshd:!!:16814::::::tcpdump:!!:16994::::::learner:$6$.U5pPYhu$h9TnYR9L4dbJY6b6VgnAQBG5qEg6s5fyJpxZVrAipHeeFhHAiHk6gjWa/xOfvWx.CzM2fvk685OEUc.ZdBYiC0:17095:0:99999:7::: 显示文件 / etc/shadow 的后 4 行。 /etc/shadow 中保存的是账号密码等信息，每行被: 分隔成 9 列： 用户名 加密的密码 上次修改口令的时间；这个时间是从 1970 年 01 月 01 日算起到最近一次修改口令的时间间隔（天数）。 两次修改口令间隔最少的天数；如果这个字段的值为空，帐号永久可用； 两次修改口令间隔最多的天数；如果这个字段的值为空，帐号永久可用； 提前多少天警告用户口令将过期；如果这个字段的值为空，帐号永久可用； 在口令过期之后多少天禁用此用户；如果这个字段的值为空，帐号永久可用； 用户过期日期；此字段指定了用户作废的天数（从 1970 年的 1 月 1 日开始的天数），如果这个字段的值为空，帐号永久可用； 保留字段，目前为空，以备将来发展之用； /etc/shadow 中的记录行与 / etc/passwd 中的一一对应，它由 pwconv 命令根据 / etc/passwd 中的数据自动产生。 另外命令 tail 还有个常用选项 - f，作用是随着文件内容的增加而输出，默认输出间隔为 1s。 more 和 less 两个命令的作用都是分页显示文件内容，区别是 more 不允许往回翻，只能用 enter 键和空格键分别显示下一行和下一页 (类似于 man 命令)，less 允许往回翻，向上箭头和 pageup 按键也是可用的。读者可自行实验这两个命令，这里不再举例。 10、groupadd 创建用户组1groupadd [OPTION] group 选项’-g’可以为新创建用户组指定 GID。如创建一个新用户组 group1 并指定其 GID 为 1005，然后再新创建一个用户 tom，使他的 UID 为 1002，GID 为 1000，登录 shell 为 / bin/sh： 123456$ groupadd -g 1005 group1$ useradd -u 1002 -g 1000 -s /bin/sh tom$ tail -n 1 /etc/passwdtom:x:1002:1000::/home/tom:/bin/sh$ tail -n 1 /etc/groupgroup1:x:1005: 这里 useradd 命令的选项 - u、-g 和 - s 分别指定新用户的 uid、gid 和登录 shell。 11、chmod 改变文件权限12chmod [OPTION]... MODE[,MODE]... FILE...chmod [OPTION]... OCTAL-MODE FILE... 在看此命令用法之前，我们先来解释一下命令 ls 的选项 - l 的输出： 12345$ ls -l总用量 0drwxr-xr-x 3 root root 29 10 月 21 20:34 dir1drwxr-xr-x 2 root root 6 10 月 21 20:33 dir2-rw-r--r-- 1 root root 0 10 月 21 20:34 file1 这部分输出被分为 7 个部分： -rw-r–r– 10 个字符中第一个字符 - 代表文件类型，linux 中文件共有 7 种类型，分别表示如下： d：代表文件是一个目录 l：符号链接 s：套接字文件 b：块设备文件 c：字符设备文件 p：命名管道文件 -：普通文件，或者说除上述文件外的其他文件 剩下的 9 个字符每三个一组，表示这个文件的权限，linux 中文件权限用二进制的 000-111(一位八进制数) 来分别代表文件的权限，其中： r(read)：读权限 (如果是文件表示读取文件内容，如果是目录表示浏览目录)。二进制第一位置 1 即 100，十进制为数字 4。 w(write)：写权限 (对文件而言，具有新增、修改文件内容的权限，对目录来说，具有删除、移动目录内文件的权限。)。二进制第二位置 1 即 010，十进制为数字 2。 x(execute)：执行权 (对文件而言，具有执行文件的权限；对目录来说具有进入该目录的权限。)。二进制第三位置 1 即 001，十进制为数字 1。 -(无权限)：当没有上述权限时。二进制表示为 000。 这样本例中最后一行文件 file1 权限： 前三个字符 rw- 表示文件的所有者 (owner) 对文件具有读和写的权限，十进制数字为 4+2=6。 中间三个字符 r-- 表示文件的所属组 (group) 对文件具有读权限，十进制数字为 4。 最后三个字符 r-- 表示系统中其他用户 (others) 对文件具有读权限，十进制数字为 4。 这样文件的权限可以用十进制数字 644 来表示。 对于目录 dir1 来说： 前三个字符 rwx 表示目录所有者 (owner) 对其具有读、写和执行的权限，十进制表示为 4+2+1=7。 中间三个字符 r-x 表示目录的所属组 (group) 对其具有读和执行的权限，十进制表示为 4+1=5。 后三个字符 r-x 表示系统中其他用户 (others) 对其具有读和执行的权限，十进制表示为 4+1=5。 这个目录权限用十进制表示即为 755，注意文件和目录相同权限之间的区别。 权限后面的数字代表文件的硬链接数 root 文件的所有者，有时表示为用户的 UID。 root 文件的所属组，有时表示为用户组的 GID。 文件大小，以字节 Byte 为单位。 10 月 21 表示文件内容最近一次被修改的时间。 最后一列为文件名。 如给文件 file1 的用户组增加执行权限： 123$ chmod g+x file1$ ls -l file1-rw-r-xr-- 1 root root 0 10 月 21 20:34 file1 这里 g+x 表示给 group 增加执行 x 的权限。 如给文件 file1 的其他人减少读权限： 123$ chmod o-r file1$ ls -l file1-rw-r-x--- 1 root root 0 10 月 21 20:34 file1 这里 o-r 表示给 others 减少读 r 权限。 如给文件 file1 的任何用户都设置成 rw - 权限： 123$ chmod a=rw file1$ ls -l file1-rw-rw-rw- 1 root root 0 10 月 21 20:34 file1 这里 a=rw 表示给所有人 all 设置成 rw - 权限。 或者用十进制表示法直接指定文件的权限： 123$ chmod 644 file1$ ls -l file1-rw-r--r-- 1 root root 0 10 月 21 20:34 file1 如给目录 dir1 和目录内的所有目录和文件权限都设置成 777： 123456$ chmod 777 -R dir1$ ls -l总用量 0drwxrwxrwx 3 root root 29 10 月 21 20:34 dir1drwxr-xr-x 2 root root 6 10 月 21 20:33 dir2-rw-r--r-- 1 root root 0 10 月 21 20:34 file1 选项 - R 作用是递归地改变目标权限。 另外如目录 /tmp 的权限： 1234$ ls -l /....drwxrwxrwt. 7 root root 88 10 月 22 21:14 tmp.... 我们看到权限最后一位是 t，这里代表粘滞位 (sticky)，它的作用是给目录特殊的权限：使用户不能删除该目录下不属于此用户的文件。 t 后面的. 表示该文件被 selinux 的安全上下文保护。 如可执行文件 /bin/su 的权限： 12$ ls -l /bin/su-rwsr-xr-x. 1 root root 32072 11 月 20 2015 /bin/su 所有者的权限 rws，这里的 s 代表 suid，如果在用户组位置的话代表 sgid，作用是给文件特殊的权限：当用户执行此文件的时候，把他当成是文件的所有者来执行。 这些特殊用途的的权限对普通用户来说知道即可。 12、lsattr 列出隐藏权限1lsattr [option] [files...] 如： 1234$ lsattr---------------- ./dir1---------------- ./dir2---------------- ./file1 列出了文件的隐藏权限位，共有 16 位 (由于隐藏权限是文件系统相关的，不同的文件系统对于文件的隐藏权限的设定不一定相同)。 13、chattr 给文件设置隐藏权限1chattr [+-=] [mode] files... 如给文件 file1 增加隐藏权限 a： 123$ chattr +a file1$ lsattr file1-----a---------- file1 这里的 a 权限表示：这个文件将只能添加数据，而不能删除也不能修改数据，只有 root 才能配置这个属性。给文件 file2 增加隐藏属性 i： 123$ chattr +i file2$ lsattr file2----i----------- file2 这里的 i 权限表示：使文件不能被修改、删除、改名、链接。只有 root 才能配置这个属性。这些隐藏权限都不常用，通常知道这两个就可以了。 14、chown 改变文件的所有者和所属组1chown [OPTION]... [OWNER][:[GROUP]] FILE... 如改变文件 file1 的所有者为 learner： 123$ chown learner file1$ ls -l file1-rw-r--r-- 1 learner root 0 10 月 21 20:34 file1 如递归地改变目录 dir1 和其下面的所有目录和文件，使它们的所有者和所属组均为 learner： 12345$ chown -R learner:learner dir1$ ls -l总用量 0drwxrwxrwx 3 learner learner 29 10 月 21 20:34 dir1.... 这里的用户和用户组可以用对应的 uid 和 gid 代替，冒号: 也可以换为点号.。 15、userdel 和 groupdel 用于删除用户和用户组。userdel 用于删除用户账号，选项 - r 可以将用户家目录一并删除。 groupdel 用于删除用户组，注意不能移除现有用户的主组。在移除此组之前，必须先移除此用户。 16、id 打印用户 ID 信息1id [OPTION]... [USER] 当不跟用户名时显示当前用户信息： 12$ iduid=0(root) gid=0(root) 组 = 0(root) 17、whoami,who,w 显示登录用户信息命令 whoami 打印出当前用户名： bash12$ whoamiroot 命令 who 打印当前登录用户信息： 12345$ whoroot tty1 2016-09-30 15:18root pts/0 2016-10-23 17:12 (192.168.78.140)learner pts/1 2016-10-23 17:49 (192.168.78.140)root pts/2 2016-10-23 17:50 (192.168.78.140) 显示信息中第一列为用户名，第二列为登录终端，第三列为登录时间，最后为登录 ip 地址。 命令 w 显示信息与命令 who 类似，增加了一些系统信息： 1234567$ w 17:56:59 up 23 days, 2:39, 4 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot tty1 309 月 16 23days 0.01s 0.01s -bashroot pts/0 192.168.78.140 17:12 7:31 0.01s 0.00s bashlearner pts/1 192.168.78.140 17:49 7:29 0.00s 0.00s -bashroot pts/2 192.168.78.140 17:50 3.00s 0.00s 0.00s w 输出的第一行显示了系统运行时间，当前有多少用户登录，cpu 的平均负载 (以后文章中会有详述)。余下的信息增加了空闲时间，cpu 的使用时间以及运行的命令。 18、su 执行用户命令1su [options...] [-] [user [args...]] 两种常用用法： 选项 - c command 用于使用目标账号执行 - c 指定的命令： 12$ su learner -c pwd/root 例子中使用账号 learner 执行了命令 pwd。当不使用选项 - c 时则为切换用户： 12345$ whoamiroot$ su learner[learner@centos7 root]$ whoamilearner 注意如果是从普通账号切换至 root 或其他账号时，需要输入对应密码。 带与不带选项 - 或 - l 或 –login 切换账号时，会有环境变量上的区别。同时带这些选项也会把当前目录切换至目标账号的家目录。使用命令 exit 可以退出： 12345678910$ pwd/root$ su - learner上一次登录：日 10 月 23 18:22:23 CST 2016pts/5 上[learner@centos7 ~]$ pwd/home/learner[learner@centos7 ~]$ exit登出$ whoamiroot 19、sudo 作为另一个用户来执行命令1sudo [OPTION]... command linux 中为了安全，往往并不允许每个用户都用 root 账号登录系统，通常都会创建一些普通用户。但有些命令是只有 root 用户才能执行的，为了更灵活的分配权限，使普通用户也能执行某些 root 命令，我们可以使用 sudo 来完成这一任务。 sudo 通过维护一个特权到用户名映射的数据库将特权分配给不同的用户，这些特权可由数据库中所列的一些不同的命令来识别。为了获得某一特权项，有资格的用户只需简单地在命令行输入 sudo 与命令之后，按照提示再次输入口令（用户自己的口令，不是 root 用户口令）。 使用 -l 选项可以查看当前用户可以执行的 root 命令有哪些： 1234567891011[learner@centos7 ~]$ sudo -lWe trust you have received the usual lecture from the local SystemAdministrator. It usually boils down to these three things:#1) Respect the privacy of others.#2) Think before you type.#3) With great power comes great responsibility.[sudo] password for learner:对不起，用户 learner 不能在 centos7 上运行 sudo。 这里看到 learner 用户并不能使用 sudo。若要设置用户使用 sudo，需要编辑 sudo 的配置文件 /etc/sudoers。该文件中以符号 #开头的都是注释行，用来解释或描述配置，并不起实际作用。 需要使用命令 visudo 来编辑修改 / etc/sudoers(使用方法和使用 vi/vim 编辑器类似，后面有文章详细描述)。配置文件中的一个条目格式为： 1user MACHINE=COMMANDS 如给用户 learner 在所有地方 (ALL) 运行任何命令(ALL)： 1learner ALL=(ALL) ALL 之后查看： 1234567[learner@centos7 ~]$ sudo -l[sudo] password for learner:匹配此主机上 learner 的默认条目：........用户 learner 可以在该主机上运行以下命令： (ALL) ALL 当然并不会给普通用户所有权限，这里只是举例。通常的做法是给某个用户某些特定的命令权限，如允许用户 tom 在主机 machine 上执行立即关机的命令，在 / etc/sudoers 中添加条目： 1tom machine=/usr/sbin/shutdown -h now 注意 machine 是 tom 登录系统所用的主机名，可以用 ip 地址代替，如使用命令 w 时 FROM 那一列所显示的登录 ip。等号后面的命令名必须是命令的绝对路径，-h now 是命令 / usr/sbin/shutdown 的参数，命令效果是立即关机。等号后面可以接多个命令，用逗号分隔它们。同时用户名也可以是用户组，用 % 组名代替。另外，用户 tom 在执行 sudo 命令时，sudo 后面的命令写法也必须和配置中的一致。 20、mv 移动文件或目录1mv [OPTION]... SOURCE... DIRECTORY mv 命令的作用是把文件或目录从源移动到目标目录，路径可以是绝对的也可以是相对的 如将文件 file2 从当前目录移动到 /root/temp/dir2 中： 1234567$ lsdir1 dir2 file1 file2$ mv file2 /root/temp/dir2/$ lsdir1 dir2 file1$ ls dir2/file2 命令 mv 还可以对文件进行改名，如将目录 dir2 移动到 dir1 内并改名为 dir3： 1234567$ lsdir1 dir2 file1$ mv dir2 ./dir1/dir3$ lsdir1 file1$ ls dir1/dir3 file2 test 21、cp 复制文件或目录1cp [OPTION]... SOURCE... DIRECTORY 如复制文件 file1 为 file3： 12345$ lsdir1 file1$ cp file1 file3$ lsdir1 file1 file3 复制目录 dir1 内目录 dir3 及其包含内容到当前目录下，起名为 dir2： 123$ cp -r dir1/dir3/ ./dir2$ lsdir1 dir2 file1 file3 复制目录的时候需要使用选项 - r，当目标已存在时，会需要用户确认是否覆盖，输入 y 或 yes 表示确认覆盖，输入 n 或 no 表示取消覆盖： 1234$ cp file1 file3cp：是否覆盖 "file3"？ y$ cp file1 file3cp：是否覆盖 "file3"？ no 可以使用选项 - f(force) 来强制复制，不需要确认。 1$ cp -rf dir1/test ./dir2/ 注意此处 - rf，当有多个选项作用于一个命令时，在不引起混淆的情况下可以连写。 22、rm 删除文件1rm [OPTION]... FILE... 选项 -r 作用是递归地删除目录，-f 的作用是强制删除： 12345$ lsdir1 dir2 file1 file3$ rm -rf dir2/$ lsdir1 file1 file3 23、whereis 查找系统命令1whereis [options] name... 命令作用是显示命令名称的绝对路径和命令的手册位置： 12$ whereis lsls: /usr/bin/ls /usr/share/man/man1/ls.1.gz 24、du 估算文件占用空间大小1du [OPTION]... [FILE]... 如查看文件 file1 的大小： 12$ du file14 file1 输出的结果第一列表示所占空间大小 (单位是 KB)。第二列是是文件名。 可以使用选项 - h 用人类可读 (human readable) 的方式显示： 12$ du -h file14.0K file1 当使用 - s 选项作用在目录上时，只显示总用量。不用时则显示该目录下的每个文件： 123456$ du dir10 dir1/test0 dir1/dir30 dir1$ du -sh dir10 dir1 linux秉承“一切皆文件”的思想，在这样的思想作用之下，linux内的所有操作都可以说是文件相关的。这里列出的命令都是最为基础的文件相关命令，每一个使用者都需要牢记。当然这里并不能将它们的所有用法一一列举，如想了解更多，一定要记得man！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之文本分析 awk]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-awk%2F</url>
      <content type="text"><![CDATA[awk 是一种模式扫描和处理语言，在对数据进行分析处理时，是十分强大的工具。 1awk [options] 'pattern &#123;action&#125;' file... awk 的工作过程是这样的：按行读取输入 (标准输入或文件)，对于符合模式 pattern 的行，执行 action。当 pattern 省略时表示匹配任何字符串；当 action 省略时表示执行&#39;{print}&#39;；它们不可以同时省略。 每一行输入，对 awk 来说都是一条记录 (record)，awk 使用 $0 来引用当前记录： 12$ head -1 /etc/passwd | awk '&#123;print $0&#125;'root:x:0:0:root:/root:/bin/bash 例子中将命令 head -1 /etc/passwd 作为 awk 的输入，awk 省略了 pattern，action 为 print $0，意为打印当前记录。 对于每条记录，awk 使用分隔符将其分割成列，第一列用 $1 表示，第二列用 $2 表示… 最后一列用 $NF 表示 选项 -F 表示指定分隔符 如输出文件 /etc/passwd 第一行第一列 (用户名) 和最后一列(登录 shell)： 12$ head -1 /etc/passwd | awk -F: '&#123;print $1,$NF&#125;'root /bin/bash 当没有指定分隔符时，使用一到多个 blank(空白字符，由空格键或 TAB 键产生) 作为分隔符。输出的分隔符默认为空格。 如输出命令 ls -l * 的结果中，文件大小和文件名： 12345678$ ls -l * | awk '&#123;print $5,$NF&#125;'13 b.txt58 c.txt12 d.txt0 e.txt0 f.txt24 test.sh$ 还可以对任意列进行过滤： 12$ ls -l *|awk '$5&gt;20 &amp;&amp; $NF ~ /txt$/'-rw-r--r-- 1 nobody nobody 58 11 月 16 16:34 c.txt 其中 $5&gt;20 表示第五列的值大于 20；&amp;&amp; 表示逻辑与；$NF ~ /txt$/ 中，~ 表示匹配，符号 // 内部是正则表达式。这里省略了 action，整条 awk 语句表示打印文件大小大于 20 字节并且文件名以 txt 结尾的行。 awk 用 NR 表示行号 123$ awk '/^root/ || NR==2' /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologin 例子中 || 表示逻辑或，语句表示：输出文件 /etc/passwd 中以 root 开头的行或者第二行。 在一些情况下，使用 awk 过滤甚至比使用 grep 更灵活 如获得 ifconfig 的输出中网卡名及其对应的 mtu 值 123456$ ifconfig|awk '/^\S/&#123;print $1"\t"$NF&#125;'ens32: 1500ens33: 1500lo: 65536$#这里的正则表示不以空白字符开头的行，输出内容中使用 \ t 进行了格式化。 以上所说的 NR、NF 等都是 awk 的内建变量，下面列出部分常用内置变量 12345678910$0 当前记录（这个变量中存放着整个行的内容）$1~$n 当前记录的第 n 个字段，字段间由 FS 分隔FS 输入字段分隔符 默认是空格或 TabNF 当前记录中的字段个数，就是有多少列NR 行号，从 1 开始，如果有多个文件话，这个值也不断累加。FNR 输入文件行号RS 输入的记录分隔符， 默认为换行符OFS 输出字段分隔符， 默认也是空格ORS 输出的记录分隔符，默认为换行符FILENAME 当前输入文件的名字 awk 中还可以使用自定义变量，如将网卡名赋值给变量 a，然后输出网卡名及其对应的 RX bytes 的值 (注意不同模式匹配及其 action 的写法)： 1234$ ifconfig|awk &apos;/^\S/&#123;a=$1&#125;/RX p/&#123;print a,$5&#125;&apos;ens32: 999477100ens33: 1663197120lo: 0 awk 中有两个特殊的 pattern：BEGIN 和 END；它们不会对输入文本进行匹配，BEGIN 对应的 action 部分组合成一个代码块，在任何输入开始之前执行；END 对应的 action 部分组合成一个代码块，在所有输入处理完成之后执行。 12345678#注意类似于 C 语言的赋值及 print 函数用法$ ls -l *|awk 'BEGIN&#123;print"size name\n---------"&#125;$5&gt;20&#123;x+=$5;print $5,$NF&#125;END&#123;print"---------\ntotal",x&#125;'size name---------58 c.txt24 test.sh---------total 82 awk 还支持数组，数组的索引都被视为字符串 (即关联数组)，可以使用 for 循环遍历数组元素 如输出文件 /etc/passwd 中各种登录 shell 及其总数量 1234567#注意数组赋值及 for 循环遍历数组的写法$ awk -F ':' '&#123;a[$NF]++&#125;END&#123;for(i in a) print i,a[i]&#125;' /etc/passwd/bin/sync 1/bin/bash 2/sbin/nologin 19/sbin/halt 1/sbin/shutdown 1 当然也有 if 分支语句 123#注意大括号是如何界定 action 块的$ netstat -antp|awk '&#123;if($6=="LISTEN")&#123;x++&#125;else&#123;y++&#125;&#125;END&#123;print x,y&#125;'6 3 pattern 之间可以用逗号分隔，表示从匹配第一个模式开始直到匹配第二个模式 12345$ awk '/^root/,/^adm/' /etc/passwd root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin 还支持三目操作符 pattern1 ? pattern2 : pattern3，表示判断 pattern1 是否匹配，true 则匹配 pattern2，false 则匹配 pattern3，pattern 也可以是类似 C 语言的表达式。 如判断文件 /etc/passwd 中 UID 大于 500 的登录 shell 是否为 /bin/bash，是则输出整行，否则输出 UID 为 0 的行： 12345#注意为避免混淆对目录分隔符进行了转义$ awk -F: '$3&gt;500?/\/bin\/bash$/:$3==0 &#123;print $0&#125;' /etc/passwd root:x:0:0:root:/root:/bin/bashlearner:x:1000:1000::/home/learner:/bin/bash#三目运算符也可以嵌套，例子略 选项 -f file 表示从 file 中读取 awk 指令 1234567891011121314#打印斐波那契数列前十项$ cat test.awkBEGIN&#123; $1=1 $2=1 OFS="," for(i=3;i&lt;=10;i++) &#123; $i=$(i-2)+$(i-1) &#125; print&#125;$ awk -f test.awk1,1,2,3,5,8,13,21,34,55 选项 -F 指定列分隔符 1234#多个字符作为分隔符时$ echo 1.2,3:4 5|awk -F '[., :]' '&#123;print $2,$NF&#125;'2 5#这里 - F 后单引号中的内容也是正则表达式 选项 -v var=val 设定变量 1234567#这里 printf 函数用法类似 C 语言同名函数$ awk -v n=5 'BEGIN&#123;for(i=0;i&lt;n;i++) printf"%02d\n",i&#125;' 0001020304 print 等函数还支持使用重定向符 &gt; 和 &gt;&gt; 将输出保存至文件 1234567#如按第一列 (IP) 分类拆分文件 access.log，并保存至 ip.txt 文件中$ awk '&#123;print &gt; $1".txt"&#125;' access.log$ ls -l 172.20.71.*-rw-r--r-- 1 root root 5297 11 月 22 21:33 172.20.71.38.txt-rw-r--r-- 1 root root 1236 11 月 22 21:33 172.20.71.39.txt-rw-r--r-- 1 root root 4533 11 月 22 21:33 172.20.71.84.txt-rw-r--r-- 1 root root 2328 11 月 22 21:33 172.20.71.85.txt 内建函数 length() 获得字符串长度 12$ awk -F: '&#123;if(length($1)&gt;=16)print&#125;' /etc/passwdsystemd-bus-proxy:x:999:997:systemd Bus Proxy:/:/sbin/nologin split() 将字符串按分隔符分隔，并保存至数组 12345678$ head -1 /etc/passwd|awk '&#123;split($0,arr,/:/);for(i=1;i&lt;=length(arr);i++) print arr[i]&#125;'rootx00root/root/bin/bash getline 从输入 (可以是管道、另一个文件或当前文件的下一行) 中获得记录，赋值给变量或重置某些环境变量 1234567891011121314151617181920212223#从 shell 命令 date 中通过管道获得当前的小时数$ awk 'BEGIN&#123;"date"|getline;split($5,arr,/:/);print arr[1]&#125;'09#从文件中获取，此时会覆盖当前的 $0。(注意逐行处理 b.txt 的同时也在逐行从 c.txt 中获得记录并覆盖 $0，当 getline 先遇到 eof 时 &lt;即 c.txt 文件行数较少&gt; 将输出空行)$ awk '&#123;getline &lt;"c.txt";print $4&#125;' b.txt"https://segmentfault.com/learnning"$#赋值给变量$ awk '&#123;getline blog &lt;"c.txt";print $0"\n"blog&#125;' b.txtaasdasdadsadBLOG ADDRESS IS "https://segmentfault.com/learnning"$#读取下一行 (也会覆盖当前 $0)$ cat fileanny100bob150cindy120$ awk '&#123;getline;total+=$0&#125;END&#123;print total&#125;' file370#此时表示只对偶数行进行处理 next 作用和 getline 类似，也是读取下一行并覆盖 $0，区别是 next 执行后，其后的命令不再执行，而是读取下一行从头再执行。 123456789101112131415161718192021#跳过以 a-s 开头的行，统计行数，打印最终结果$ awk '/^[a-s]/&#123;next&#125;&#123;count++&#125;END&#123;print count&#125;' /etc/passwd2$#又如合并相同列的两个文件$ cat f.txt学号 分值00001 8000002 7500003 90$ cat e.txt姓名 学号张三 00001李四 00002王五 00003$ awk 'NR==FNR&#123;a[$1]=$2;next&#125;&#123;print $0,a[$2]&#125;' f.txt e.txt 姓名 学号 分值张三 00001 80李四 00002 75王五 00003 90#这里当读第一个文件时 NR==FNR 成立，执行 a[$1]=$2，然后 next 忽略后面的。读取第二个文件时，NR==FNR 不成立，执行后面的打印命令 sub(regex,substr,string) 替换字符串 string(省略时为 $0) 中首个出现匹配正则 regex 的子串 substr 1234567891011$ echo 178278 world|awk 'sub(/[0-9]+/,"hello")'hello world$gsub(regex,substr,string) 与 sub() 类似，但不止替换第一个，而是全局替换$ head -n5 /etc/passwd|awk '&#123;gsub(/[0-9]+/,"----");print $0&#125;' root:x:----:----:root:/root:/bin/bashbin:x:----:----:bin:/bin:/sbin/nologindaemon:x:----:----:daemon:/sbin:/sbin/nologinadm:x:----:----:adm:/var/adm:/sbin/nologinlp:x:----:----:lp:/var/spool/lpd:/sbin/nologin substr(str,n,m) 切割字符串 str，从第 n 个字符开始，切割 m 个。如果 m 省略，则到结尾 12$ echo "hello, 世界！"|awk '&#123;print substr($0,8,1)&#125;'界 tolower(str) 和 toupper(str) 表示大小写转换 12$ echo "hello, 世界！"|awk '&#123;A=toupper($0);print A&#125;'HELLO, 世界！ system(cmd) 执行 shell 命令 cmd，返回执行结果，执行成功为 0，失败为非 0 123#此处 if 语句判断和 C 语言一致，0 为 false，非 0 为 true$ awk 'BEGIN&#123;if(!system("date&gt;/dev/null"))print"success"&#125;'success match(str,regex) 返回字符串 str 中匹配正则 regex 的位置 12$ awk 'BEGIN&#123;A=match("abc.f.11.12.1.98",/[0-9]&#123;1,3&#125;\./);print A&#125;'7 awk 作为一个编程语言可以处理各种各样的问题，甚至于编写应用软件，但它更常用的地方是命令行下的文本分析，生成报表等，这些场景下 awk 工作的很好。工作中如经常有文本分析的需求，那么掌握这个命令的用法将为你节省大量的时间。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之变量]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-variable%2F</url>
      <content type="text"><![CDATA[本篇开始，介绍 shell 脚本编程，更确切的说是 bash 脚本编程 (版本：4.2.46(1)-release)。我们从变量开始。 和所有的编程语言一样，bash 也提供变量，变量是一些用来指代数据并支持数据操作的名称。 类型环境变量概念当我们通过 ssh 等工具登录系统时，便获得一个 shell(一个 bash 进程)，bash 在启动过程中会加载一系列的配置文件，这些配置文件的作用就是为用户准备好 bash 环境，大部分 环境变量 都是在这些文件中被设置的。 登录 shell(login shell) 是指需要通过输入用户名、密码登录之后获得的 shell(或者通过选项 “--login“ 生成的 shell)。登录 shell 的进程名为 -bash，非登录 shell(比如在桌面环境下通过打开一个 “终端“ 窗口程序而获得的 shell) 的进程名为 bash。 1234$ ps -ef|grep [b]ashroot 2917 2915 0 14:25 pts/3 00:00:00 -bashroot 2955 2953 0 14:25 pts/5 00:00:00 -bashroot 3070 3068 0 14:42 pts/4 00:00:00 -bash 交互式 shell(interactive shell) 是指 shell 与用户进行交互，shell 需要等待用户的输入 (键入一条命令后并按下回车键)，用户需要等待命令的执行和输出。当把一到多个命令写入一个文件，并通过执行这个文件来执行这些命令时，bash 也会为这些命令初始化一个 shell 环境，这样的 shell 称为非交互式 shell。 环境变量 - 中存储了当前 shell 的选项标志，其中如果包含字符 i 则表示此 shell 是交互式 shell： 123#输出变量&apos;-&apos;的值$ echo $-himBH 通常，一个登录 shell(包括交互式登录 shell 和使用 “–login” 选项的非交互 shell) 首先读取并执行文件 /etc/profile(此文件会在结尾处判断并执行 /etc/profile.d/ 中所有以. sh 结尾的文件)；然后按顺序搜索用户家目录下的~/.bash_profile、~/.bash_login 和~/.profile，并执行找到的第一个可读文件 (在 centos7 系统中是文件~/.bash_profile，此文件会进一步判断并执行文件~/.bashrc，然后再进一步判断并执行文件 /etc/bashrc)。当一个登录 shell 登出时 (exit)，会执行文件~/.bash_logout 和 /etc/bash.bash_logout(如果文件存在的话)。 交互式非登录 shell 启动时，bash 会读取并执行文件~/.bashrc。 非交互式 shell 启动时 (如脚本中)，会继承派生出此 shell 的父 shell 的环境变量并执行环境变量 BASH_ENV 的值中所指代的文件。 作用环境变量的作用主要是影响 shell 的行为，在整个 bash 进程的生命周期中，会多次使用到环境变量。每个由当前 bash 进程派生出的子进程 (包括子 shell)，都会继承当前 bash 的环境变量 (除非子进程对继承的环境变量进行了重新赋值，否则它们的值将和父进程相同)。下面列出部分常用环境变量及其作用： PATH 其值是一个以冒号分隔的目录列表，定义了 shell 命令的搜索路径。 12$ echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin PS1 首要命令提示符。 1234567#笔者环境下变量 PS1 的值：$ echo $PS1[\u@\h \W]\$# \u 表示当前用户的用户名# \h 表示主机名字符串中直到第一个. 之前的字符# \W 表示当前路径的 basename，用户家目录会被缩写为波浪号 (~)# \$ 如果用户 UID 为 0，则为符号 #，否则为符号 $ PS2 连续性 交互式提示符。当输入的命令分为好几行时，新行前出现的字符串即为 PS2 变量的值。 12$ echo $PS2&gt; PS3 shell 脚本中 select 关键字提示符 PS4 shell 调试模式下的提示符 HOME 当前用户的家目录 PWD 当前工作目录 OLDPWD 前一个工作目录 123456789# cd 命令后如果没有任何参数时，则使用 $HOME 作为默认参数$ cd$ # cd 命令后的参数 - 等同于 $OLDPWD$ cd -/root/temp/tmp# cd 命令的成功执行会更新两个环境变量 (PWD 和 OLDPWD) 的值$ echo $PWD $OLDPWD/root/temp/tmp /root RANDOM 每次引用此变量，都会生成一个 0 到 32767 之间的随机数字 BASH_VERSION 其值为当前 bash 版本号 12$ echo $BASH_VERSION4.2.46(1)-release IFS 域分隔符，用来分隔单词。默认值为 空格键 TAB 键 回车键产生的字符 123#可以用 set 命令查看当前环境下的所有变量$ set|grep IFSIFS=$&apos;\t\n&apos; 本系列中在涉及到具体环境变量的时候还有更详细的解释和用法描述。 自定义变量普通变量bash 除了在初始化时自动设置的变量外，用户还可以根据需要手动设置变量。普通变量赋值语句写法： 1name=[value] 其中 name 为变量名，变量名必须以英文字母 ([a-zA-Z]) 或下划线 (_) 开头，其余字符可以是英文字母、下划线或数字 ([0-9])。变量名是大小写敏感的。在给变量赋值时，等号两边不能有任何空白字符。等号后的值(value) 可以省略，如果省略，则变量的值为空字符串(null)。 数组变量bash 提供一维的索引和关联数组变量，索引数组是以数字为下标的数组，关联数组是以字符串为下标的数组 (类似其他语言中的 map 或 dict)。 数组赋值语句写法： 1name=(value1 value2 ... valueN) 其中每一个 value 都是类似以 [subscript]=string 的格式，索引数组赋值时可以省略 [subscript]=，关联数组不能省略。 12345#索引数组赋值的一般形式：name_index=(aa bb cc dd ee)#关联数组赋值之前，必须先通过内置命令 declare 进行声明，然后才能赋值：declare -A name_associatename_associate=([aa]=1 [bb]=2 [cc]=3 [dd]=4) 所谓内置命令，是指由 bash 自身实现的命令，它们的执行就相当于执行 bash 的一个函数，并不需要派生出新的子进程。 外部命令是指那些不是由 bash 自身实现的命令 (如环境变量 PATH 目录内的命令)。原则上所有命令都应该外部实现 (避免臃肿及和其他系统耦合度过高)，但是，外部命令的执行，意味着创建子进程，而子进程对环境变量等的更改是无法影响父进程的。bash 想要更改自身的一些状态时，就得靠内置命令来实现。例如，改变工作目录命令 cd，就是一个典型的例子 (cd 命令会更改当前所处目录，并更新环境变量 PWD 和 OLDPWD，如果此功能由外部实现，更改目录的目的就无法实现了)。 特殊变量bash 中还支持一些表示特殊意义的变量，这些变量不能使用上述语句进行赋值。 12345678910$0 本程序所处的进程名。$n n 是从 1 开始的整数，表示当前进程参数，$1 表示第一个参数、$2 表示第二个参数...$n 表示第 n 个参数。如果 n 大于 10，取值时需要写成 $&#123;n&#125; 的格式。当执行函数时，这些位置变量被临时替换为函数的第一个参数、第二个参数、、、第 N 个参数。$* 表示当前进程的所有参数。$1 $2 ... $&#123;n&#125;。当处于双引号中取值时，所有结果被当成一个整体，即 "$*" 等同于 "$1 $2 ... $&#123;n&#125;"。$@ 表示当前进程的所有参数。$1 $2 ... $&#123;n&#125;。当处于双引号中取值时，每个结果被当成单独的单词，即 "$@" 等同于 "$1" "$2" ... "$&#123;n&#125;"。$# 表示当前进程的参数个数。$? 表示前一个命令的返回码，为 0 表示前一个命令执行成功，非 0 表示执行失败。$- 表示当前 shell 的选项标志。$$ 表示当前 shell 的 PID。$! 表示最近一次执行的后台命令的 PID。$_ 在 shell 初始启动时表示启动此 shell 命令的绝对路径或脚本名，随后，表示前一条命令的最后一个参数。 声明/定义及赋值通常 bash 的变量是不需要提前声明的，可以直接进行赋值。变量的值均被视为字符串 (在一些情况下也可以视为数字)。当对变量有特殊需要时，也可以先声明变量 (如前面关联数组的声明)。 bash 提供了几个和变量声明及赋值相关的内置命令，这些命令即可以和赋值语句写在同一行 (表示声明及赋值)，也可以只跟变量名 (表示声明)。 [] 表示可选： 12declare [options] name[=value] ...typeset [options] name[=value] ... 这是两个起同样作用的命令，用来声明变量； 1234567891011#如声明一个普通变量：declare name[=value]#如声明一个只能存储数字的变量：declare -i name[=value]#选项 - i 表示为变量增加一个数字属性，变量 name 中只能存储数字，如果将字符串赋给此变量时，变量的值为 0#如声明一个索引数组declare -a name_index[=(aa bb cc dd ee)]#如声明一个变量，并将其导出为环境变量declare -x name[=value]#如声明一个只读变量declare -r name[=value] 以上选项可以使用命令 declare +OPTION name 撤销变量 name 的属性 (只读变量除外)内置命令 export 作用于赋值语句时，和 declare -x 类似表示导出变量为环境变量 (临时有效，重启系统后这些环境变量消失；如需设置永久环境变量，需要将 export 语句写入前面所述的 bash 配置文件中)。 内置命令 readonly 作用于赋值语句时，和 declare -r 类似表示标记变量为只读： 123456789#如普通只读变量readonly name[=value]#如只读索引数组readonly -a name_index[=(aa bb cc dd ee)]#如只读关联数组readonly -A name_associate[=([aa]=1 [bb]=2 [cc]=3 [dd]=4)]#如标记函数为只读readonly -f function_name只读变量不能重新赋值，不能使用内置命令 unset 进行撤销，不能通过命令 declare +r name 取消只读属性。 内置命令 read 作用是从标准输入读入一行数据赋值给变量 1234$ read NAME1 2 3 #此处键入字符串 "1 2 3"$ echo $NAME1 2 3 当有多个变量名时，环境变量 IFS 用来将输入分隔成单词。当单词数大于变量数时，剩余的单词和分隔符会被赋值给最后一个变量。当单词数小于变量数时，剩余的变量被赋空值。 12345678$ read NUM_1 NUM_2 NUM_31 2 3 4 5$ echo $NUM_11$ echo $NUM_22$ echo $NUM_33 4 5 选项 -a 表示将读入的数据赋值给索引数组 12345$ read -a BLOG &lt; file #这里输入来自文件，当文件有多行时，第二行及后续行将被忽略。$ echo $&#123;BLOG[@]&#125; #取数组中所有元素的值this is vvpale\'s blog$ echo $&#123;#BLOG[@]&#125; #取数组元素个数4 选项 -p string 表示在等待输入时显示提示符字符串 string 1234$ read -p "请输入变量值：" NUM_4请输入变量值：345$ echo $NUM_4345 选项 -d 表示指定分隔符 (原分隔符为 \n) 123$ read -d ':' ROOT &lt; /etc/passwd$ echo $ROOTroot 内置命令 readarray 和 mapfile 表示从标准输入中读入数据并赋值给索引数组，每行赋给一个数组元素： 123456$ seq 10 &gt; file$ readarray NUM &lt;file$ echo $&#123;NUM[*]&#125;1 2 3 4 5 6 7 8 9 10$ echo $&#123;#NUM[*]&#125;10 变量有一个状态 set/unset：只要变量被赋过值，就称变量是 set 的状态 (即使变量的值为空 null)；否则，则称变量是 unset 的状态 (即使变量被 declare 或其他内置命令声明过)。 可以使用内置命令 unset 对变量进行撤销 (特殊变量和只读变量除外)。 12345678#撤销普通变量unset name#撤销整个数组unset array_name#撤销数组中单个值 (实际上是把相应的值置空，数组元素个数减一)unset array_name[index]#撤销函数unset function_name 对变量进行赋值时，可以使用操作符 += 表示对值的追加： 12345678#普通变量$ var=hello$ echo $varhello$ var+=_world$ echo $varhello_world$ unset var 123456789101112131415161718192021#数字变量使用 += 表示将原有值和新值进行数学运算 (加法)，注意与字符串变量的区别。$ declare -i NUM=5$ echo $NUM5$ NUM+=5$ echo $NUM10$ unset NUM$#数组变量使用 += 作用于上述复合赋值语句表示追加元素至数组$ array=([0]=hello [1]=world)$ echo $&#123;array[@]&#125;hello world$ echo $&#123;#array[@]&#125;2$ array+=(i am vvpale)$ echo $&#123;array[@]&#125;hello world i am vvpale$ echo $&#123;#array[@]&#125;5$ unset array 变量取值/扩展bash 使用符号 $ 对变量进行取值，并使用大括号 {} 对变量名的起始和结束进行界定，在不引起混淆的情况下，大括号可以省略。 在命令的执行过程中，变量被其值所替换，在替换的过程中能够对应于各种变换。bash 称对变量进行取值的过程为变量替换或变量扩展。 直接取值123456789101112#如果值中包含空白字符，赋值时需要用引号引起来表示一个整体。变量中实际存储的是除去引号的部分。$ var_1="hello world"$ echo $var_1hello world$#数组$ arr=(1000 2000 3000 4000)$ echo $&#123;arr[@]&#125;1000 2000 3000 4000$ echo $&#123;arr[*]&#125;1000 2000 3000 4000#注意当被双引号作用时两者的区别 (如前述，同特殊变量 $@和 $* 的情况一致) 间接引用在对变量进行取值时，变量名前的符号 ! 表示对变量的间接引用： 12345678910111213$ var_2=var_1$ echo $&#123;!var_2&#125; #必须用大括号hello world$#以上如果写成 $&#123;!var*&#125; 或 $&#123;!var@&#125; 则被替换为所有以 var 为前缀的变量名：$ echo $&#123;!var*&#125;var_1 var_2$ echo $&#123;!var@&#125;var_1 var_2#开头的! 如果用在数组变量上则被扩展成数组的所有下标：$ declare -A array=(["MON"]="星期一" ["TUE"]="星期二" ["WEN"]="星期三" ["THU"]="星期四" ["FRI"]="星期五" ["SAT"]="星期六" ["SUN"]="星期日")$ echo $&#123;!array[*]&#125;THU TUE WEN MON FRI SAT SUN 取长度在变量名前使用符号 # 表示取长度，普通变量表示变量值的字符数，数组变量表示数组参数的个数 123456$ echo $&#123;#var_1&#125;11$ echo $&#123;#var_2&#125;5$ echo $&#123;#array[*]&#125;7 判断状态对于变量 parameter 的状态 (set 或 unset) 和值是否为空(null)，bash 提供四种方式扩展： 这里的 word 会经过 波浪号扩展 (~ 替换为用户家目录)、变量扩展、命令替换、数学扩展 (以后的文章中会对后两种作详细描述) ${parameter:-word} 如果变量状态为 unset 或值为空，返回 word 的结果值，否则返回变量的值。 1234567$ echo $&#123;var_3:-$&#123;!var_2&#125;&#125;hello world$ var_3=learnning$ echo $&#123;var_3:-$&#123;!var_2&#125;&#125;learnning$ echo $&#123;var_4:-~&#125;/root ${parameter:=word} 如果变量状态为 unset 或值为空，word 的结果会赋值给变量，然后返回变量值。特殊变量 ($n $$$ $@ $# 等) 不能用这种方式进行赋值。 123456$ echo $&#123;var_4&#125; #变量 var_4 未被赋值，这里输出一个空行$ echo $&#123;var_4:=$&#123;var_3:-$&#123;!var_2&#125;&#125;&#125; #注意这里变量 var_3 已被赋值 learnning，所以没有输出 "hello world"learnning$ echo $&#123;var_4&#125; learnning ${parameter:?word} 如果变量状态为 unset 或值为空，word 的结果值会被输出到标准错误，如果 shell 是非交互的 (如脚本中) 则退出(exit)；否则展开为变量的值。 1234$ echo $&#123;var_4:?"exist"&#125;learnning$ echo $&#123;var_5:?"not exist"&#125;-bash: var_5: not exist ${parameter:+word} 如果变量状态为 unset 或值为空，什么也不返回，否则返回 word 的结果值。 1234$ echo $&#123;var_5:+$&#123;#var_2&#125;&#125; #变量 var_5 未被赋值，这里输出一个空行$ echo $&#123;var_4:+$&#123;#var_2&#125;&#125;5 以上四种判断变量的方式中，如果省略了冒号 :，则表示只判断 unset 的情况。 12345$ echo $&#123;var_5-"unset"&#125; unset$ var_5=$ echo $&#123;var_5+"set"&#125; set 取子串bash 支持使用 ${parameter:offset:length} 的格式对变量取部分值，其中 offset 和 length 都是数学表达式，分别代表位置和长度。 parameter 为普通变量时，表示从第 offset 个字符 (首字符是第 0 个) 开始，取 length 个字符，如果 :length 省略，表示从第 offset 个字符开始，取到变量值的结尾。 123456$ echo $var_1hello world$ echo $&#123;var_1:6&#125;world$ echo $&#123;var_1:1-1:2+3&#125;hello 如果 offset 的结果小于 0，则表示从后往前取子串。 12345#注意这里为了避免和判断变量状态的写法混淆，冒号和减号之间需要有空白字符或者用括号将负数括起来$ echo $&#123;var_1: -5&#125;world$ echo $&#123;var_1: -5:2&#125;wo 如果 length 的结果小于 0，则它表示距离最后一个字符往前 length 个字符的位置，和 offset 位置一起作用，变量替换的结果就是两个位置之间的值。 12$ echo $&#123;var_1:2:-2&#125;llo wor parameter 是 @ 或使用 @ 或 * 作为下标的数组时，则 offset 和 length 计算的是元素个数而不是字符数，并且 length 的结果不能为负。 12345678910111213141516171819$ ARRAY=("星期一" "星期二" "星期三" "星期四" "星期五" "星期六" "星期日")$ echo $&#123;ARRAY[@]:2:3&#125;星期三 星期四 星期五$ echo $&#123;ARRAY[@]:(-3)&#125;星期五 星期六 星期日#还要注意 $@是从 $1 开始的参数列表和关联数组取结果时的不确定性$ cat test.sh #将要执行的命令写入脚本echo $@echo $&#123;@:0&#125;echo $&#123;@:2:2&#125;$$ ./test.sh 1 2 3 4 5 6 7 8 9 #直接执行脚本，参数列表将赋值给特殊变量 @1 2 3 4 5 6 7 8 9./test.sh 1 2 3 4 5 6 7 8 9 #当 offset 为 0 时 (对应脚本第二条命令)，$0 会被添加至参数列表前。2 3$ echo $&#123;array[@]&#125; #关联数组输出时，结果和赋值时的元素顺序不一定相同星期四 星期二 星期三 星期一 星期五 星期六 星期日$ echo $&#123;array[@]:2:2&#125;星期二 星期三 删除bash 提供两种方式分别删除变量值的前缀或后缀： ${parameter#word} 和 ${parameter##word} 表示删除前缀。word 扩展后的结果会作为模式匹配 (通配符匹配，见这里) 变量的值，一个 #表示删除最短匹配前缀，## 表示删除最长匹配前缀： 123456$ echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin$ echo $&#123;PATH#*:&#125;/usr/local/bin:/usr/sbin:/usr/bin:/root/bin$ echo $&#123;PATH##*:&#125;/root/bin ${parameter%word} 和 ${parameter%%word} 表示删除后缀。 12345678910111213141516$ path=$PWD #赋值语句中等号右边部分也会经过 波浪号扩展、变量扩展、命令替换和数学扩展$ echo $path/root/temp/tmp$ echo $&#123;path%/*&#125; #类似于执行命令 dirname $path/root/temp#同样适用于特殊变量和环境变量$ vim /etc/yum.repos.d/CentOS-Base.repo$ cd $&#123;_%/*&#125;$ pwd/etc/yum.repos.d$ cd -/root/temp/tmp$ echo $BASH_VERSION4.2.46(1)-release$ echo $&#123;BASH_VERSION%%[()]*&#125; #注意这里的通配符匹配4.2.46 如果 parameter 是 @ 或 * 或以 @ 或 * 作为下标的数组变量，删除操作将作用于每个位置变量或数组的每个参数 1234$ echo $&#123;array[@]&#125;星期四 星期二 星期三 星期一 星期五 星期六 星期日$ echo $&#123;array[@]#??&#125;四 二 三 一 五 六 日 替换${parameter/pattern/string} 的形式表示用 pattern 对变量 parameter 进行匹配 (通配符匹配)，并使用 string 的结果值替换匹配(最长匹配) 的部分。 1234567$ string=1234567890abcdefghijklmnopqrstuvwxyz$ echo $&#123;string/1234567890/----&#125;----abcdefghijklmnopqrstuvwxyz$ echo $&#123;string/[0-9]/----&#125;----234567890abcdefghijklmnopqrstuvwxyz$ echo $&#123;string/a*/....&#125;1234567890.... 如果 pattern 以字符 / 开头，则所有被匹配的结果都被替换 12$ echo $&#123;string//[0-9]/-&#125;----------abcdefghijklmnopqrstuvwxyz 如果 pattern 以字符 # 开头，匹配的前缀被替换 12$ echo $&#123;string/#*0/---&#125;---abcdefghijklmnopqrstuvwxyz 如果 pattern 以字符 % 开头，匹配的后缀被替换 12$ echo $&#123;string/%a*/...&#125;1234567890... 使用 @ 和 * 的情况和前述一样，替换将作用于每个参数 12345678$ A=(100 101 102 103 104) B=.txt P= #多个赋值语句可以写在一行$ echo $&#123;A[@]&#125;100 101 102 103 104$ echo $B.txt$ echo -n $P #无输出$ echo $&#123;A[*]/%$P/$B&#125;100.txt 101.txt 102.txt 103.txt 104.txt 大小写转换${parameter^pattern}、${parameter^^pattern}、${parameter,pattern}、${parameter,,pattern} 大小写字母转换，如果 parameter 值的首字母匹配模式 pattern(通配符匹配，只能是一个字符，可以是 ? * [...] 或一个英文字母，多个字符不起作用。pattern 省略则表示使用 ?)，则 ^ 将首字母转换成大写，^^ 将所有匹配字母转换成大写；, 将首字母转换成小写，,, 将所有匹配字母转换成小写。 1234567891011$ var_5='hello WORLD' var_6='HELLO world'$ echo $&#123;var_5^[a-z]&#125;Hello WORLD$ echo $&#123;var_5^^*&#125;HELLO WORLD$ echo $&#123;var_5^^&#125;HELLO WORLD$ echo $&#123;var_6,&#125;hELLO world$ echo $&#123;var_6,,[A-Z]&#125;hello world 使用 @ 和 * 的情况和前述相同，大小写转换将作用于每个参数 由于 bash 变量赋值的随意性，自定义变量起名时不要和原有变量 (尤其是环境变量) 相冲突，撤销时也要注意不要将环境变量撤销掉(虽然撤销自定义变量并不是必须的)。 1$ unset $&#123;!var*&#125; $&#123;!NUM@&#125; ARRAY $&#123;!arr*&#125; ROOT BLOG NAME path string word A B P 作用域bash 变量的作用域分为多种： 写入到 bash 配置文件并用 export 导出的环境变量。影响每个启动时加载相应配置文件的 bash 进程及其子进程。 当前 shell 中自定义并通过内置命令 export 导出的环境变量。影响当前 bash 进程及其子进程。 当前 shell 中自定义但未导出的变量。影响当前 bash 进程及其子进程 (不包括需要重新初始化 shell 的进程)。 当前 shell 中某个函数中通过内置命令 local 自定义的局部变量。只影响此函数及嵌套调用的函数和命令。 当前 shell 中某个命令中的临时变量。只影响此命令。 bash变量作用域涉及到子shell和函数的用法，这里暂时不作举例说明，后续文章中会详细叙述。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之文本编辑 vim]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-vim%2F</url>
      <content type="text"><![CDATA[本文介绍 vim(版本 7.4) 的一般用法 vim 是功能强大的文本编辑器，是 vi 的增强版。 1vim [options] [file ..] 使用 vim 编辑一个文件的最常用命令就是： 1vim file 其中 file 可以是一个新文件，也可以是原有文件。这样的命令执行后将打开编辑器，显示文件 file 的内容。如图所示： 如果是一个新文件，底部左边会显示 “file” [新文件] 的字样，右边显示 0,0-1 表示当前光标所在行数和字符数。如果打开的是一个老文件，底部左边显示 “file” 3L, 66C 字样，表示文件名，当前光标所处行号，文件总字节数。文件中没有字符的地方会以字符~ 开头。 现在还不能进行编辑，vim 编辑文件有几种模式，当前所处模式是普通模式，可以移动光标、执行复制粘贴等命令；另外还有两种常用模式：插入模式和底行模式。开始进入的一般都是普通模式，按如下键将进入插入模式： a 在光标所在位置下一个字符开始输入 A 在光标所在行尾开始输入 i 在光标所在位置开始输入 I 在光标所在行首开始输入 o 在光标所在行下新增一行，并在新增行行首开始输入 O 在光标所在行上新增一行，并在新增行行首开始输入 进入插入模式后，底部会出现 – 插入 – 字样；这时就可以在光标位置进行输入了。从插入模式退回到普通模式按 ESC 键。 在普通模式按如下键将进入底行模式： : 执行命令 / 正向搜索 ? 反向搜索 从底行模式退回到普通模式需按两次 ESC 键 底行模式执行: q(quit) 表示退出编辑器，如果对文件内容进行过更改，需要执行: wq(write quit) 来保存退出；如果不保存退出则需要执行: q! 强制退出；强制保存退出为: wq!。 下面列出一些在普通模式下可以执行的命令及它们的作用： 文本修改： . 重复上一个命令 x 删除光标位置的字符 d 从光标处开始剪切 dd 剪切光标所在位置的整行 (保存在临时缓冲区) ndd n 为数字，表示从当前行开始，从上到下剪切 n 行 p 将缓冲区中的内容放到当前行之下 np n 是数字，相当于执行 n 次 p 命令 P 将缓冲区中的内容放到当前行之上 y 从光标处开始复制 yy 复制当前行 (保存在临时缓冲区) nyy n 为数字，表示从当前行开始，从上到下复制 n 行 r 替换光标所在位置的一个字符 R 从光标位置开始替换，并进入文本输入模式 (ESC 退出) u 撤销上一次操作 ZZ 保存退出 移动光标： h 或左箭头键 光标向左移动一格 l 或右箭头键 光标向右移动一格 j 或下箭头键 光标向下移动一格 k 或上箭头键 光标向上移动一格 0 光标移动到当前行开头 ^ 光标移动到当前行非空白字符 (如空格、tab 键等) 的开头 $ 光标移动到当前行结尾 g_ 光标移动到当前行非空白字符的结尾 w 光标移动到下一个单词的开头 e 光标移动到下一个单词的结尾 * 匹配光标所在单词，移动到下一个相同的单词 # 匹配光标所在单词，移动到上一个相同的单词 f{ 光标移动到当前行的下一个字符 {处，{可以换成其他字符 F} 光标移动到当前行的上一个字符} 处 t, 光标移动到当前行的下一个逗号之前一个字符处 T, 光标移动到当前行的上一个逗号之后一个字符处 % 移动到成对括号的另一半括号处，包括 () {} []. 需要先将光标移动到括号上。 gg 光标移动到文件第一行行首 G 光标移动到文件最后一行行首 回车键 光标移动到下一行行首 这里只列出一部分，初学者也许会觉得太多，记不住。其实只要记住其中几个，就完全可以使用了；其他的作用大多是使你的操作更简便快捷。 vim 中的各种命令，大多可以组合使用： 比如要删除光标当前所在位置一直到行尾，可以执行 d$； 比如要在当前位置插入 50 个‘word’，只需执行 50iword ESC 按完 ESC 键之后这 50 个单词就被插入了； 比如要粘贴被复制的内容，但希望粘贴 5 次，则执行 5p; 比如要再次执行上一次命令 5 次，则执行 5.; 熟练使用这些命令将极大提高编写文件的速度。 在插入模式下 (普通模式按 a、i、o 等)，输入一个单词的开头，然后按 CTRL-P 或 CTRL-N 就会自动补齐。 底行模式共三个开始字符 (:、/、?)，其中 / 和? 用来匹配模式搜索： 如搜索文件内字符串 centos： 1/centos 输入这个字符串之后按回车，vim 就会将所有匹配的字符串高亮显示，按下 n 键，光标就会跳到下一个匹配字符串处，按 N 键，光标就会跳到上一个匹配处。(想想 man 查询)使用? 和 / 的作用相同，不过是方向相反。 : 可以执行许多命令，如前面介绍过的保存退出命令: wq。下面介绍部分底行模式命令： :set nu 显示行号 :set nonu 隐藏行号 :r file - 读取文件 file 内容并写入当前编辑的文件中，内容从光标当前位置下一行开始插入。 :w file 将当前编辑的内容写入一个新文件 file 中。 :s /pattern/string/ 将匹配 pattern 的字符串替换成 string :x 作用和: wq 相同，保存并退出。 :! command 暂时离开 vim 并执行 shell 命令 command。 :help 查看帮助 :.= 显示当前行号 := 显示总行数 :n 移动光标到第 n 行行首 这些命令也可以组合如执行 shell 命令并将结果写入当前行的下一行： 1:r!ls -l 还能组合其他模式下的命令如替换当前行所有匹配模式的字符串： 1:s/pattern/string/g # g 表示全局 如替换本文件中所有匹配模式的字符串： 1:%s/pattern/string/g # % 表示所有行 如替换指定行的匹配字符串： 1:n,ms/pattern/string/g 这里 n 和 m 都是数字，代表行号。可以用点号. 代表当前行 如删除当前行到第五行的内容： 1:.,5d 当当前行处于第五行以下时，会有反向删除的提示。 正则表达式使用 /pattern 和: s/pattern/string 时，pattern 是一个正则表达式，用来匹配一个字符串的模式。 正则表达式和之前介绍的通配符 (基础命令介绍二) 有一些相似的地方，但要注意区分两者的不同。 通配符主要是用于对文件名的匹配，正则表达式不仅可以用于匹配文件名，事实上，它可以进行任何字符串的匹配。它要比通配符更通用，大多数编程语言和一些工具中 (如 vim、grep、awk、sed) 都有对正则表达式的直接支持。 下面介绍一部分将要用到的正则表达式的概念和用法： 匹配位置： ^ 表示行开头 $ 表示行结尾 &lt; 表示单词开头 > 表示单词结尾 匹配字符： . 表示匹配任意单个字符 (相当于通配符中的?) […] 表示匹配括号内任意单个字符 [^…] 表示匹配任意一个非列出字符 #参照通配符描述 \a 匹配英文字符，等同于 [a-zA-Z] 或[[:alpha:]]。 \A 匹配非英文字符，等同于 [^a-zA-Z]。 \d 匹配数字，等同于 [0-9] 或[[:digit:]]。 \D 匹配非数字，等同于 [^0-9]。 \x 匹配十六进制数字，等同于 [0-9A-Fa-f] 或[[:xdigit:]]。 \X 匹配非十六进制数字，等同于 [^0-9A-Fa-f]。 \w 匹配单词，等同于 [0-9A-Za-z_]。 \W 匹配非单词，等同于 [^0-9A-Za-z_]。 \t 匹配 TAB 字符。 \s 匹配空白字符，等同于 [\t] 或[[:blank:]]。 \S 匹配非空白字符，等同于 [^ \t]。 \u 匹配大写字母，等同于 [A-Z] 或[[:upper:]]。 \U 匹配非大写字母。 \n 匹配换行 \r 匹配回车 (…) 匹配并捕获，用 \ 1 \2 \3 … 来引用被捕获的字符串。 | 表示逻辑或 匹配数量： * 表示匹配前一个字符零到任意多次，相当于 {0,}。 + 表示匹配前一个字符一到任意多次，相当于 {1,}。 ? 表示匹配前一个字符零到一次，相当于 {0,1}。# 注意和通配符? 的区别 {n,m} 表示匹配前一个字符 n 到 m 次。 在使用正则表达式时，有时需要在特殊字符之前加上转义字符 “\” 来使特殊字符表示它的字面意思而不是它的特殊意义，在特定的工具中使用正则时，也需要这样做来避免特殊字符被工具本身解释。 vim 在使用如下正则表达式时需要将特殊字符转义：\&lt;…\&gt;、\{n,m}、\(…\)、\?、\+、\| 下面结合正则举例说明 vim 中模式匹配及部分命令用法 匹配字符串 world 并使光标停留在匹配行后第三行行首： 1/world/+3 将第三行到第八行行首添加注释符号 //： 1:3,8s/^/\/\// 注意这里的行首符 ^ 和转义符’\’的用法 如： 1:%g/^\sxyz/normal dd 此命令作用是全局匹配以空白后接 xyz 开头的行，并执行普通模式下的命令 dd 如匹配 6 个以上的小写字母： 1/\a\&#123;6,&#125; 如交换冒号: 两侧的字符串： 1:s/\(.*\):\(.*\)/\2:\1/ #注意这里是如何引用之前匹配的分组的 如将所有 tag、tog 和 tug 分别改为 hat、hot 和 hut 1:%s/t\([aou]\)g/h\1t/g 如匹配 hello 或 world 两个单词： 1/\&lt;hello\&gt;\|\&lt;world\&gt; 这里只列出部分 vim 用到的正则表达式，关于正则的更多内容，以后的文章中还会有描述和举例。 vim编辑器是很强大的，这里只描述了部分初级使用方法。vim还能使用视图模式，编辑多文本，设置键盘映射，多剪贴板，录制宏，使用插件等等。完全可以用vim作为IDE来使用。但相对来说，vim的学习曲线比较陡峭，不同的应用场景，需要记忆的命令和方法数量也是完全不同的。建议在学习中，每次只熟悉几个命令，一段时间后，就能流畅快捷的编辑文本了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之语法]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-syntax%2F</url>
      <content type="text"><![CDATA[本文开始正式介绍 shell 脚本的编写方法以及 bash 的语法。 定义元字符 用来分隔词 (token) 的单个字符，包括： 1| &amp; ; () &lt;&gt; space tab token 是指被 shell 看成一个单一单元的字符序列 bash 中包含三种基本的 token：保留关键字，操作符，单词。 保留关键字是指在 shell 中有明确含义的词语，通常用来表达程序控制结构。包括： 1! case coproc do done elif else esac fi for function if in select then until while &#123;&#125; time [[]] 操作符由一个或多个元字符组成，其中控制操作符包括： 1|| &amp; &amp;&amp; ; ;; () | |&amp; &lt;newline&gt; 余下的 shell 输入都可以视为普通的单词(word)。 shell 脚本是指包含若干 shell 命令的文本文件，标准的 bash 脚本的第一行形如 #!/bin/bash，其中顶格写的字符 #! 向操作系统申明此文件是一个脚本，紧随其后的 /bin/bash 是此脚本程序的解释器，解释器可以带一个选项 (选项一般是为了对一些情况做特殊处理，比如 -x 表示开启 bash 的调试模式)。 除首行外，其余行中以符号 # 开头的单词及本行中此单词之后的字符将作为注释，被解析器所忽略。 语法相比于其他更正式的语言，bash 的语法较为简单。大多数使用 bash 的人员，一般都先拥有其他语言的语法基础，在接触 bash 的语法之后，会自然的将原有语法习惯套用到 bash 中来。事实上，bash 的语法灵活多变，许多看起来像是固定格式的地方，实际上并不是。这让一些初学者觉得 bash 语法混乱不堪，复杂难记。这和 bash 的目的和使用者使用 bash 的目的有很大的关系，bash 本身是为了提供一个接口，来支持用户通过命令与操作系统进行交互。用户使用 bash，一般是为了完成某种系统管理的任务，而不是为了做一款独立的软件。这些，都使人难以像学习其他编程语言那样对 bash 认真对待。其实，只要系统学习一遍 bash 语法以及一条命令的执行流程，就可以说掌握了 bash 脚本编程的绝大多数内容。 bash 语法只包括六种：简单命令、管道命令、序列命令、复合命令、协进程命令 (bash 版本 4.0 及以上) 和函数定义。 简单命令shell 简单命令 (Simple Commands) 包括命令名称，可选数目的参数和重定向(redirection)。我们在 Linux 基础命令介绍系列里所使用的绝大多数命令都是简单命令。另外，在命令名称前也可以有若干个变量赋值语句(如上一篇所述，这些变量赋值将作为命令的临时环境变量被使用，后面有例子)。简单命令以上述控制操作符为结尾。 shell 命令执行后均有返回值 (会赋给特殊变量 $?)，是范围为 0-255 的数字。返回值为 0，表示命令执行成功；非 0，表示命令执行失败。(可以使用命令 echo $? 来查看前一条命令是否执行成功) 管道命令管道命令 (pipeline) 是指被 | 或 |&amp; 分隔的一到多个命令。格式为： 1[time [-p]] [ ! ] command1 [ | command2 ... ] 其中保留关键字 time 作用于管道命令表示当命令执行完成后输出消耗时间 (包括用户态和内核态占用时间)，选项 -p 可以指定时间格式。 默认情况下，管道命令的返回值是最后一个命令的返回值，为 0，表示 true，非 0，则表示 false；当保留关键字 ! 作用于管道命令时，会对管道命令的返回值进行取反。 之前我们介绍过管道的基本用法，表示将 command1 的标准输出通过管道连接至 command2 的标准输入，这个连接要先于命令的其他重定向操作 (试对比 &gt;/dev/null 2&gt;&amp;1 和 2&gt;&amp;1 &gt;/dev/null 的区别)。如果使用 |&amp;，则表示将 command1 的标准输出和标准错误都连接至管道。 管道两侧的命令均在子 shell(subshell) 中执行，这里需要注意：在子 shell 中对变量进行赋值时，父 shell 是不可见的。 12345#例如[root@centos7 ~]# echo 12345|read NUM[root@centos7 ~]# echo $NUM #由于 echo 和 read 命令均在子 shell 中执行，所以当执行完毕时，在父 shell 中输出变量的值为空[root@centos7 ~]# 序列命令序列命令 (list) 是指被控制操作符;,&amp;,&amp;&amp; 或 || 分隔的一到多个管道命令，以;、&amp; 或 &lt;newline&gt; 为结束。 在这些控制操作符中，&amp;&amp; 和 || 有相同的优先级，然后是 ; 和 &amp;(也是相同的优先级)。 如果命令以 &amp; 为结尾，此命令会在一个子 shell 中后台执行，当前 shell 不会等待此命令执行结束，并且不论它是否执行成功，其返回值均为 0。 以符号 ; 分隔的命令按顺序执行 (和换行符的作用几乎相同)，shell 等待每个命令执行完成，它们的返回值是最后一个命令的返回值。 以符号 &amp;&amp; 和 || 连接的两个命令存在逻辑关系。 command1 &amp;&amp; command2：先执行 command1，当且仅当 command1 的返回值为 0，才执行 command2。 command1 || command2：先执行 command1，当且仅当 command1 的返回值非 0，才执行 command2。 脚本举例： 12345678910#!/bin/bash#简单命令echo $PATH &gt; file#管道命令cat file|tr &apos;:&apos; &apos; &apos;#序列命令IFS=&apos;:&apos; read -a ARRAY &lt;file &amp;&amp; echo $&#123;ARRAY[4]&#125; || echo &quot;赋值失败&quot;echo &quot;命令返回值为：$?。&quot;#验证变量的临时作用域echo &quot;$IFS&quot;|sed &apos;N;s/[\t\n]/-/g&apos; 执行结果 (在脚本所在目录直接执行./test.sh)： 1234[root@centos7 ~]# ./test.sh /usr/local/sbin /usr/local/bin /usr/sbin /usr/bin /root/bin/root/bin命令返回值为：0。 注意例子中序列命令的写法，其中 IFS=’:’只临时对内置命令 read 起作用 (作为单词分隔符来分隔 read 的输入)，read 命令结束后，IFS 又恢复到原来的值：$’\d\n’。&amp;&amp; 和 || 在这里类似于分支语句，read 命令执行成功则执行输出数组的第五个元素，否则执行输出 “赋值失败”。 复合命令1、(list)list 将在 subshell 中执行 (注意赋值语句和内置命令修改 shell 状态不能影响当父 shell)，返回值是 list 的返回值。 此复合命令前如果使用扩展符 $，shell 称之为命令替换 (另一种写法为 \list`)。shell 会把命令的输出作为命令替换 ` 扩展之后的结果使用。 命令替换可以嵌套。 2、{list;}list 将在当前 shell 环境中执行，必须以换行或分号为结尾 (即使只有一个命令)。注意不同于 shell 元字符：(和)，{和} 是 shell 的保留关键字，因为保留关键字不能分隔单词，所以它们和 list 之间必须有空白字符或其他 shell 元字符。 3、((expression))expression 是数学表达式 (类似 C 语言的数学表达式)，如果表达式的值非 0，则此复合命令的返回值为 0；如果表达式的值为 0，则此复合命令的返回值为 1。 此种复合命令和使用内置命令 let “expression” 是一样的。 数学表达式中支持如下操作符，操作符的优先级，结合性，计算方法都和 C 语言一致 (按优先级从上到下递减排列)： 123456789101112131415161718id++ id-- # 变量后自加 后自减++id --id # 变量前自加 前自减- + # 一元减 一元加! ~ # 逻辑否定 位否定** # 乘方* / % # 乘 除 取余+ - # 加 减&lt;&lt;&gt;&gt; # 左位移 右位移&lt;=&gt;= &lt; &gt; # 比较大小== != # 等于 不等于&amp; # 按位与^ # 按位异或| # 按位或&amp;&amp; # 逻辑与|| # 逻辑或expr?expr:expr # 条件表达式= *= /= %= += -= &lt;&lt;=&gt;&gt;= &amp;= ^= |= # 赋值表达式expr1 , expr2 # 逗号表达式 在数学表达式中，可以使用变量作为操作数，变量扩展要先于表达式的求值。变量还可以省略扩展符号 $，如果变量的值为空或非数字和运算符的其他字符串，将使用 0 代替它的值做数学运算。 以 0 开头的数字将被解释为八进制数，以 0x 或 0X 开头的数字将被解释为十六进制数。其他情况下，数字的格式可以是 [base#]n。可选的 base# 表示后面的数字 n 是以 base(范围是 2-64) 为基的数字，如 2#11011 表示 11011 是一个二进制数字，命令 ((2#11011)) 的作用会使二进制数转化为十进制数。如果 base# 省略，则表示数字以 10 为基。 复合命令 ((expression)) 并不会输出表达式的结果，如果需要得到结果，需使用扩展符 $ 表示数学扩展(另一种写法为 $[expression])。数学扩展也可以嵌套。 括号 () 可以改变表达式的优先级。 脚本举例： 123456789101112131415161718192021#!/bin/bash# (list)(ls|wc -l)#命令替换并赋值给数组 注意区分数组赋值 array=(...) 和命令替换 $(...)array=($(seq 10 10 $(ls|wc -l) | sed -z 's/\n/ /g'))#数组取值echo "$&#123;array[*]&#125;"# &#123;list;&#125;#将文件 file1 中的第一行写入 file2，&#123;list;&#125; 是一个整体。&#123;read line;echo $line;&#125; &gt;file2 &lt;file1#数学扩展A=$(wc -c file2 |cut -b1)#此时变量 A 的值为 5B=4echo $((A+B))echo $(((A*B)**2))#赋值并输出echo $((A|=$B))#条件运算符 此命令意为：判断表达式 A&gt;=7 是否为真，如果为真则计算 A-1，否则计算 (B&lt;&lt;1)+3。然后将返回结果与 A 作异或运算并赋值给 A。((A^=A&gt;=7?A-1:(B&lt;&lt;1)+3))echo $A 执行结果： 1234567[root@centos7 temp]# ./test.sh4310 20 30 409400514 4、[[expression]]此处的 expression 是条件表达式 (并非数学扩展中的条件表达式)。此种命令的返回值取决于条件表达式的结果，结果为 true，则返回值为 0，结果为 false，则返回值为 1。 条件表达式除可以用在复合命令中外，还可以用于内置命令 test 和 [，由于 test、[[、]]、[和] 是内置命令或保留关键字，所以同保留关键字 {和} 一样，它们与表达式之间都要有空格或其他 shell 元字符。 条件表达式的格式包括： 1234567891011121314151617181920212223242526272829303132333435363738394041-b file #判断文件是否为块设备文件-c file #判断文件是否为字符设备文件-d file #判断文件是否为目录-e file #判断文件是否存在-f file #判断文件是否为普通文件-g file #判断文件是否设置了 SGID-h file #判断文件是否为符号链接-p file #判断文件是否为命名管道文件-r file #判断文件是否可读-s file #判断文件是否存在且内容不为空 (也可以是目录)-t fd #判断文件描述符 fd 是否开启且指向终端-u file #判断文件是否设置 SUID-w file #判断文件是否可写-x file #判断文件是否可执行-S file #判断文件是否为 socket 文件file1 -nt file2 #判断文件 file1 是否比 file2 更新 (根据 mtime)，或者判断 file1 存在但 file2 不存在file1 -ot file2 #判断文件 file1 是否比 file2 更旧，或者判断 file2 存在但 file1 不存在file1 -ef file2 #判断文件 file1 和 file2 是否互为硬链接-v name #判断变量状态是否为 set(见上一篇)-z string #判断字符串是否为空-n string #判断字符串是否非空string1 == string2 #判断字符串是否相等string1 = string2 #判断字符串是否相等string1 != string2 #判断字符串是否不相等string1 &lt;string2 #判断字符串 string1 是否小于字符串 string2(字典排序)，用于内置命令 test 中时，小于号需要转义：\&lt;string1 &gt; string2 #判断字符串 string1 是否大于字符串 string2(字典排序)，用于内置命令 test 中时，大于号需要转义：\&gt;NUM1 -eq NUM2 #判断数字是否相等NUM1 -ne NUM2 #判断数字是否不相等NUM1 -lt NUM2 #判断数字 NUM1 是否小于数字 NUM2NUM1 -le NUM2 #判断数字 NUM1 是否小于等于数字 NUM2NUM1 -gt NUM2 #判断数字 NUM1 是否大于数字 NUM2NUM1 -ge NUM2 #判断数字 NUM1 是否大于等于数字 NUM2[[expr]]和 [ expr ](test expr 是[ expr ] 的另一种写法，效果相同)还接受如下操作符(从上到下优先级递减)：! expr #表示对表达式 expr 取反(expr) #表示提高 expr 的优先级expr1 -a expr2 #表示对两个表达式进行逻辑与操作，只能用于 [expr] 和 test expr 中expr1 &amp;&amp; expr2 #表示对两个表达式进行逻辑与操作，只能用于 [[expr]] 中expr1 -o expr2 #表示对两个表达式进行逻辑或操作，只能用于 [expr] 和 test expr 中expr1 || expr2 #表示对两个表达式进行逻辑或操作，只能用于 [[expr]] 中在使用操作符 == 和!= 判断字符串是否相等时，在 [[expr]] 中等号右边的 string2 可以被视为模式匹配 string1，规则和通配符匹配一致。([ expr ]不支持) [[expr]] 中比较两个字符串时还可以用操作符 =~，符号右边的 string2 可以被视为是正则表达式匹配 string1，如果匹配，返回真，否则返回假。 5、if list; then list; [elif list; then list;] … [ else list; ] fi条件分支命令。首先判断 if 后面的 list 的返回值，如果为 0，则执行 then 后面的 list；如果非 0，则继续判断 elif 后面的 list 的返回值，如果为 0，则……，若返回值均非 0，则最终执行 else 后的 list。fi 是条件分支的结束词。 注意这里的 list 均是命令，由于要判断返回值，通常使用上述条件表达式来进行判断 形如： 12345678910if [expr]then listelif [expr]then list...else listfi 甚至，许多人认为这样就是 if 语句的固定格式，其实 if 后面可以是任何 shell 命令，只要能够判断此命令的返回值。如： 123456[root@centos7 ~]# if bash;then echo true;else echo false;fi[root@centos7 ~]# #执行后没有任何输出[root@centos7 ~]# exitexittrue #由于执行了 bash 命令开启了一个新的 shell，所以执行 exit 之后 if 语句才获得返回值，并做了判断和输出[root@centos7 ~]# 脚本举例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/bash#条件表达式declare A#判断变量 A 是否 set[[-v A]] &amp;&amp; echo "var A is set" || echo "var A is unset"#判断变量 A 的值是否为空[! $A] &amp;&amp; echo false || echo truetest -z $A &amp;&amp; echo "var A is empty"#通配与正则A="1234567890abcdeABCDE"B='[0-9]*'C='[0-9]&#123;10&#125;\w+'[[$A = $B]] &amp;&amp; echo '变量 A 匹配通配符 [0-9]*' || echo '变量 A 不匹配通配符 [0-9]*'[$A == $B] &amp;&amp; echo '[ expr ] 中能够使用通配符' || echo '[ expr ] 中不能使用通配符'[[$A =~ $C]] &amp;&amp; echo '变量 A 匹配正则 [0-9]&#123;10&#125;\w+' || echo '变量 A 不匹配正则 [0-9]&#123;10&#125;\w+'#if 语句# 此例并没有什么特殊的意义，只为说明几点需要注意的地方：# 1、if 后面可以是任何能够判断返回值的命令# 2、直接执行复合命令 ((...)) 没有输出，要取得表达式的值必须通过数学扩展 $((...))# 3、复合命令 ((...)) 中表达式的值非 0，返回值才是 0number=1if if test -n $A then ((number+1)) else ((number-1)) fithen echo "数学表达式值非 0，返回值为 0"else echo "数学表达式值为 0，返回值非 0"fi# if 语句和控制操作符 &amp;&amp; || 连接的命令非常相似，但要注意它们之间细微的差别：# if 语句中 then 后面的命令不会影响 else 后的命令的执行# 但 &amp;&amp; 后的命令会影响 || 后的命令的执行echo '---------------'if [[-r file &amp;&amp; ! -d file]];then grep -q hello fileelse awk '/world/' filefiecho '---------------'# 上面的 if 语句无输出，但下面的命令有输出[-r file -a ! -d file] &amp;&amp; grep -q hello file || awk '/world/' file# 可以将控制操作符连接的命令写成这样来忽略 &amp;&amp; 后命令的影响 (使用了内置命令 true 来返回真):echo '---------------'[-r file -a ! -d file] &amp;&amp; (grep -q hello file;true) || awk '/world/' file 6、for name [[in [ word …] ];]do list;done7、for ((expr1;expr2;expr3));do list;donebash 中的 for 循环语句支持如上两种格式，在第一种格式中，先将 in 后面的 word 进行扩展，然后将得到的单词列表逐一赋值给变量 name，每一次赋值都执行一次 do 后面的 list，直到列表为空。如果 in word 被省略，则将位置变量逐一赋值给 name 并执行 list。第二种格式中，双圆括号内都是数学表达式，先计算 expr1，然后反复计算 expr2，直到其值为 0。每一次计算 expr2 得到非 0 值，执行 do 后面的 list 和第三个表达式 expr3。如果任何一个表达式省略，则表示其值为 1。for 语句的返回值是执行最后一个 list 的返回值。 脚本举例： 12345678910111213141516171819#!/bin/bash# word 举例for i in $&#123;a:=3&#125; $(head -1 /etc/passwd) $((a+=2))do echo -n "$i"doneecho $a# 省略 in worddeclare -a arrayfor numberdo array+=($number)doneecho $&#123;array[@]&#125;# 数学表达式格式for((i=0;i&lt;$&#123;#array[*]&#125;;i++))do echo -n "$&#123;array[$i]&#125;"|sed 'y/1234567890/abcdefghij/'done;echo 执行： 12345[root@centos7 temp]# ./test.sh "$(seq 10)" # 注意此处 "$(seq 10)" 将作为一个整体赋值给 $1，如果去掉双引号将会扩展成 10 个值并赋给 $1 $2 ... $&#123;10&#125;3 root:x:0:0:root:/root:/bin/bash 5 5 # 是否带双引号并不影响执行结果，只影响第二个 for 语句的循环次数。1 2 3 4 5 6 7 8 9 10a b c d e f g h i aj[root@centos7 temp]# 8、while list-1; do list-2; done9、until list-1; do list-2; donewhile 命令会重复执行 list-2，只要 list-1 的返回值为 0；until 命令会重复执行 list-2，只要 list-1 的返回值为非 0。while 和 until 命令的返回值是最后一次执行 list-2 的返回值。 break 和 continue 两个内置命令可以用于 for、while、until 循环中，分别表示跳出循环和停止本次循环开始下一次循环。 10、case word in [[(] pattern [ | pattern]…) list ;;] … esaccase 命令会将 word 扩展后的值和 in 后面的多个不同的 pattern 进行匹配 (通配符匹配)，如果匹配成功则执行相应的 list。 list 后使用操作符;; 时，表示如果执行了本次的 list，那么将不再进行下一次的匹配，case 命令结束； 使用操作符;&amp;，则表示执行完本次 list 后，再执行紧随其后的下一个 list(不判断是否匹配)； 使用操作符;;&amp;，则表示继续下一次的匹配，如果匹配成功，那么执行相应的 list。 case 命令的返回值是执行最后一个命令的返回值，当匹配均没有成功时，返回值为 0。 脚本举例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/bin/bash# whileunset i jwhile ((i++&lt;$(grep -c '^processor' /proc/cpuinfo)))do #每个后台运行的 yes 命令将占满一核 CPU yes &gt;/dev/null &amp;done# -------------------------------------------------# until# 获取 yes 进程 PID 数组PIDS=($(ps -eo pid,comm|grep -oP '\d+(?= yes$)'))# 逐个杀掉 yes 进程until ! (($&#123;#PIDS[*]&#125;-j++))do kill $&#123;PIDS[$j-1]&#125;done# -------------------------------------------------# caseuser_define_command &amp;&gt;/dev/nullcase $? in0) echo "执行成功" ;;1) echo "未知错误" ;;2) echo "误用 shell 命令" ;;126) echo "权限不够" ;;127) echo "未找到命令" ;;130) echo "CTRL+C 终止" ;;*) echo "其他错误" ;;esac# -------------------------------------------------#定义数组c=(1 2 3 4 5)#关于各种复合命令结合使用的例子：echo -e "$(for i in $&#123;c[@]&#125;do case $i in (1|2|3) printf "%d\n" $((i+1)) ;; (4|5) printf "%d\n" $((i-1)) ;; esacdone)" | while read NUMdo if [[$NUM -ge 4]];then printf "%s\n" "数字 $&#123;NUM&#125; 大于等于 4" else printf "%s\n" "数字 $&#123;NUM&#125; 小于 4" fidone 执行结果： 12345678910111213141516[root@centos7 temp]# ./test.sh./test.sh: 行 18: 18671 已终止 yes &gt; /dev/null./test.sh: 行 18: 18673 已终止 yes &gt; /dev/null./test.sh: 行 18: 18675 已终止 yes &gt; /dev/null./test.sh: 行 18: 18677 已终止 yes &gt; /dev/null./test.sh: 行 18: 18679 已终止 yes &gt; /dev/null./test.sh: 行 18: 18681 已终止 yes &gt; /dev/null./test.sh: 行 20: 18683 已终止 yes &gt; /dev/null./test.sh: 行 20: 18685 已终止 yes &gt; /dev/null未找到命令数字 2 小于 4数字 3 小于 4数字 4 大于等于 4数字 3 小于 4数字 4 大于等于 4[root@centos7 temp]# 11、select name [in word] ; do list ; doneselect 命令适用于交互式菜单选择场景。word 的扩展结果组成一系列可选项供用户选择，用户通过键入提示字符中可选项前的数字来选择特定项目，然后执行 list，完成后继续下一轮选择，需要使用内置命令 break 来跳出循环。 脚本举例： 1234567891011#!/bin/bashecho "系统信息："select item in "host_name" "user_name" "shell_name" "quit"do case $item in host*) hostname;; user*) echo $USER;; shell*) echo $SHELL;; quit) break;; esacdone 执行结果： 1234567891011121314[root@centos7 ~]# ./test.sh系统信息：1) host_name2) user_name3) shell_name4) quit#? 1centos7#? 2root#? 3/bin/bash#? 4[root@centos7 ~]# 协进程命令协进程命令是指由保留关键字 coproc 执行的命令 (bash4.0 版本以上)，其命令格式为： 1coproc [NAME] command [redirections] 命令 command 在子 shell 中异步执行，就像被控制操作符 &amp; 作用而放到了后台执行，同时建立起一个双向管道，连接该命令和当前 shell。 执行此命令，即创建了一个协进程，如果 NAME 省略 (command 为简单命令时必须省略，此时使用默认名 COPROC)，则称为匿名协进程，否则称为命名协进程。 此命令执行时，command 的标准输出和标准输入通过双向管道分别连接到当前 shell 的两个文件描述符，然后文件描述符又分别赋值给了数组元素 NAME[0] 和 NAME[1]。此双向管道的建立要早于命令 command 的其他重定向操作。被连接的文件描述符可以当成变量来使用。子 shell 的 pid 可以通过变量 NAME_PID 来获得。 关于协进程的例子，我们在下一篇给出。 函数定义bash 函数定义的格式有两种： 12name () compound-command [redirection]function name [()] compound-command [redirection] 这样定义了名为 name 的函数，使用保留关键字 function 定义函数时，括号可以省略。函数的代码块可以是任意一个上述的复合命令 (compound-command)。 脚本举例： 1234567891011121314151617181920212223242526272829303132333435#!/bin/bash#常用定义方法：func_1() &#123; #局部变量 local num=6 #嵌套执行函数 func_2 #函数的 return 值保存在特殊变量? 中 if [$? -gt 10];then echo "大于 10" else echo "小于等于 10" fi&#125;################func_2()&#123; # 内置命令 return 使函数退出，并使其的返回值为命令后的数字 # 如果 return 后没有参数，则返回函数中最后一个命令的返回值 return $((num+5))&#125;#执行。就如同执行一个简单命令。函数必须先定义后执行 (包括嵌套执行的函数)func_1################一般定义方法#函数名后面可以是任何复合命令：func_3() for NUMdo # 内置命令 shift 将会调整位置变量，每次执行都把前 n 个参数撤销，后面的参数前移。 # 如果 shift 后的数字省略，则表示撤销第一个参数 $1，其后参数前移 ($2 变为 $1....) shift echo -n "$((NUM+$#))"done#函数内部位置变量被重置为函数的参数func_3 `seq 10`;echo 执行结果： 1234[root@centos7 temp]# ./test.sh 大于 1010 10 10 10 10 10 10 10 10 10[root@centos7 temp]# 这些就是bash的所有命令语法。bash中任何复杂难懂的语句都是这些命令的变化组合。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redhat 7/CentOS 7 SSH 免密登录]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-ssh-authentication%2F</url>
      <content type="text"><![CDATA[先决条件3 台 CentOS 7 HOSTNAME IP ROLE server1 10.8.26.197 Master server2 10.8.26.196 Slave1 server3 10.8.26.195 Slave2 步骤1. 用 root 用户登录。每台服务器都生成公钥，再合并到 authorized_keys。 2. CentOS 默认没有启动 ssh 无密登录，去掉 /etc/ssh/sshd_config 其中 2 行的注释，每台服务器都要设置。 12RSAAuthentication yesPubkeyAuthentication yes 3. 每台服务器下都输入命令 ssh-keygen -t rsa，生成 key，一律不输入密码，直接回车，/root 就会生成 .ssh 文件夹。 4. 在 Master 服务器下，合并公钥到 authorized_keys 文件，进入 /root/.ssh 目录，通过 SSH 命令合并. 123# cat id_rsa.pub&gt;&gt; authorized_keys# ssh root@10.8.26.196 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys# ssh root@10.8.26.195 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys 5. 把 Master 服务器的 authorized_keys、known_hosts 复制到 Slave 服务器的 `/root/.ssh 目录 1234# scp authorized_keys root@server2:/root/.ssh/# scp authorized_keys root@server3:/root/.ssh/# scp known_hosts root@server2:/root/.ssh/# scp known_hosts root@server3:/root/.ssh/ 6. 完成，ssh root@10.8.26.196、ssh root@10.8.26.195 就不需要输入密码了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之文本流编辑 sed]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-sed%2F</url>
      <content type="text"><![CDATA[与 vim 不同，sed 是一种非交互式的文本编辑器，同时它又是面向字符流的，每行数据经过 sed 处理后输出。 1sed [OPTION]... [script] [file]... sed 的工作过程是这样的：首先，初始化两个数据缓冲区模式空间和保持空间；sed 读取一行输入 (来自标准输入或文件)，去掉结尾的换行符(\n) 后置于模式空间中，然后针对模式空间中的字符串开始执行‘sed 命令’，每个命令都可以有地址与之相关联，地址可以看成是条件，只有在条件成立时，相关的命令才被执行；所有可执行命令都处理完毕后，仍处于模式空间中的字符串会被追加一个换行符后打印输出；之后读取下一行输入做同样的处理，直到主动退出 (q) 或输入结束。 地址地址可以是如下的形式 number 表示行号 first~step 表示从 first(数字) 行开始，每隔 step(数字) 行 $ 表示最后一行 (注意当出现在正则表达式中时表示行尾) /regexp/ 表示匹配正则表达式 regexp(关于正则表达式，请参见这一篇) \%regexp% 表示匹配正则表达式 regexp，% 可以换成任意其他单个字符。(用于 regexp 包含斜线 / 的情况) /regexp/I 匹配正则表达式 regexp 时不区分大小写 /regexp/M 启用正则多行模式，使 $ 不止匹配行尾，还匹配 n 或 r 之前的位置；使 ^ 不止匹配行首，还匹配 n 或 r 之后的位置。此时可以用（`）匹配模式空间的开头位置，用（\’）匹配模式空间的结束位置。 还可以用逗号, 分隔两个地址来表示一个范围 表示从匹配第一个地址开始，直到匹配第二个地址或文件结尾为止。如果第二个地址是个正则表达式，则不会对第一个地址匹配行进行第二个地址的匹配；如果第二个地址是行号，但小于或等于第一个地址匹配行行号，则只会匹配一行 (第一个地址匹配行)。 0,/regexp/ 这种情况下，正则表达式 regexp 会在第一行就开始进行匹配。只有第二个地址是正则表达式时，第一个地址才能用 0。 addr1,+n 表示匹配地址 addr1 和其后的 n 行。 addr1,~n 表示从匹配地址 addr1 开始，直到 n 的倍数行为止。 如果没有给出地址，所有的行都会匹配；在地址或地址范围后追加字符! 表示对地址取反，所有不匹配的行才会被处理。 选项 -n 默认时每一行处理过的字符串都会被打印输出，此选项表示关闭此默认行为。只有被命令 p 作用的字符串才会被输出。 -f file 表示从 file 中读取 sed 命令 -i 表示原地修改。应用此选项时，sed 会创建一个临时文件，并将处理结果输出到此文件，处理完毕后，会将此临时文件覆盖至原文件。 -r 表示使用扩展的正则表达式 命令p 表示打印模式空间内容，通常配合选项 -n 一起使用 123456789101112$ seq 512345$ 只输出第二行到第四行$ seq 5|sed -n '2,4p'234$ d 删除模式空间内容，立即处理下一行输入。 1234567#删除最后一行$ seq 5|sed '$d'1234$ q 立即退出，不再处理任何命令和输入 (只接受单个地址) 12345$ seq 5|sed '/3/q'123$ n 如果没有使用选项 -n，输出模式空间中内容后，读取下一行输入并覆盖当前模式空间内容。如果没有更多的输入行，sed 会退出执行。 123456$ seq 9|sed -n 'n;p'2468$ 注意多个命令用分号分隔 s/regexp/replacement/flag 表示用 replacement 替换模式空间中匹配正则表达式 regexp 的部分。在这里符号 / 可以换成任意单个字符。 123$ echo "hello123world"|sed 's/[0-9]\+/,/' hello,world#注意这里 + 需要转义，如果使用选项 -r 则无需转义 在 replacement 中1、\n (n 为 1-9 中的一个数字)表示对正则表达式中分组 (…) 的引用； 1234$ echo "hello123world"|sed -r 's/[a-z]+([0-9]+)[a-z]+/\1/'123$ echo "hello123world"|sed -r 's/([a-z]+)[0-9]+([a-z]+)/\1,\2/'hello,world 2、&amp; 表示模式空间中所有匹配 regexp 的部分； 12$ echo "hello123world"|sed -r 's/[0-9]+/:&amp;:/'hello:123:world 3、\L 将后面的字符转化成小写直到 \U 或 \E 出现； 4、\l 将下一个字符转化为小写； 5、\U 将后面的字符转化成大写直到 \L 或 \E 出现； 6、\u 将下一个字符转化为大写； 7、\E 停止由 \L 或 \U 起始的大小写转化； 123$ echo "hello123world"|sed -r 's/^([a-z]+)[0-9]+([a-z]+)$/\U\1\E,\u\2/'HELLO,World$ flag1、n 数字 n 表示替换第 n 个匹配项 12345$ head -1 /etc/passwdroot:x:0:0:root:/root:/bin/bash#替换冒号分隔的第五部分为空$ head -1 /etc/passwd|sed 's/[^:]\+://5'root:x:0:0:/root:/bin/bash 2、g 表示全局替换 123456789$ echo "hello123world"|sed 's/./\U&amp;\E/'Hello123world$$ echo "hello123world"|sed 's/./\U&amp;\E/g'HELLO123WORLD$#当数字 n 和 g 同时使用时，表示从第 n 个匹配项开始替换一直到最后匹配项$ head -1 /etc/passwd|sed 's/[^:]\+://4g'root:x:0:/bin/bash/ 3、p 表示如果替换成功，则打印模式空间内容。 4、w file 表示如果替换成功，则输出模式空间内容至文件 file 中。 5、I 和 i 表示匹配 regexp 时不区分大小写。 123$ echo 'HELLO123world'|sed -r 's/[a-z]+//Ig'123$ 6、M 和 m 表示启用正则多行模式 (如前所述)。(讲命令 N 时再举例) y/source-chars/dest-chars/ 把 source-chars 中的字符替换为 dest-chars 中对应位置的字符，/ 可以换为其他任意单个字符，source-chars 和 dest-chars 中字符数量必须一致且不能用正则表达式。 123$ echo hello|sed 'y/el/LE/' hLEEo$ a text 表示输出模式空间内容后追加输出 text 内容 1234567$ seq 3|sed '1,2a hello'1hello2hello3$ i text 表示输出模式空间内容之前，先输出 text 内容 123456$ seq 3|sed '$ihello'12hello3$ c text 表示删除匹配地址或地址范围的模式空间内容，输出 text 内容。如果是单地址，则每个匹配行都输出，如果是地址范围，则只输出一次。 12345678910$ seq 5|sed '1,3chello'hello45$ seq 5|sed '/^[^3-4]/c hello'hellohello34hello = 表示打印当前输入行行号 1234567$ seq 100|sed -n '$='100$ seq 100|sed -n '/^10\|^20/='1020100$ 转义的 | 表示逻辑或 r file 表示读取 file 的内容，并在当前模式空间内容输出之后输出 12345678910111213141516171819202122232425262728293031323334353637383940$ cat filehello world$ seq 3|sed '1,2r file'1hello world2hello world3$```bash**w file 表示输出模式空间内容至 file 中****N 读入一行内容至模式空间后，再追加下一行内容至模式空间 (此时模式空间中内容形如 line1\nline2)，如果不存在下一行，sed 会退出。**```bash$ seq 10|sed -n 'N;s/\n/ /p'1 23 45 67 89 10$#s 命令的 m flag 举例$ seq 3|sed 'N;s/^2/xxx/'123$ seq 3|sed 'N;s/^2/xxx/m' 1xxx3$ seq 3|sed 'N;s/1$/xxx/'123$ seq 3|sed 'N;s/1$/xxx/M'xxx23 D 如果模式空间中没有新行 (如命令 N 产生的新行)，则和命令 d 起同样作用；如果包含新行，则会删除第一行内容，然后对模式空间中剩余内容重新开始一轮处理。(注意：D 后面的命令将会被忽略) 123456$ seq 5|sed 'N;D' 5$ seq 5|sed 'N;N;D' 345 P 打印模式空间中第一行内容 12345678910111213141516$ seq 10|sed -n 'N;P'13579$ seq 10|sed -n 'N;N;P'147#注意另一种写法输出中的不同$ seq 10|sed -n '1~3P'14710 g 用保持空间中的内容替换模式空间中的内容 1234$ seq 5|sed -n 'g;N;s/\n/xx/p'xx2xx4$ G 追加一个换行符到模式空间，然后再将保持空间中的内容追加至换行符之后。(此时模式空间中内容形如 PATTERN\nHOLD)* 123456$ seq 5|sed 'G;s/\n/xx/' 1xx2xx3xx4xx5xx h 用模式空间中的内容替换保持空间中的内容 (注意此时模式空间中的内容并没有被清除) 123456789101112$ seq 5|sed -n 'h;G;s/\n/xx/p'1xx12xx23xx34xx45xx5$ seq 5|sed -n 'h;G;G;s/\n/xx/gp'1xx1xx12xx2xx23xx3xx34xx4xx45xx5xx5 H 追加一个换行符到保持空间，然后再将模式空间中的内容追加至换行符之后。(此时保持空间中内容形如 HOLD\nPATTERN) 12345$ seq 3|sed -n 'H;G;s/\n/xx/gp'1xxxx12xxxx1xx23xxxx1xx2xx3$ x 交换模式空间和保持空间的内容 123456$ seq 9|sed -n '1!&#123;x;N&#125;;s/\n//p'3254769#处于 &#123;...&#125; 之中的是命令组 : label 为分支命令指定标签位置 (不允许地址匹配) b label 无条件跳转到 label 分支，如果省略了 label，则跳转到整条命令结尾 (即开始下一次读入) 123456789101112131415#如删除 xml 文件中注释部分 (&lt;!--...--&gt; 之间的部分是注释，可以多行)sed '/&lt;!--/&#123;:a;/--&gt;/!&#123;N;ba&#125;;d&#125;' server.xml#表示匹配 &lt;!-- 开始，在匹配到 --&gt; 之前一直执行 N，匹配到 --&gt;之后删除模式空间中内容#如在 nagios 的配置文件中，有许多 define host&#123;...&#125; 的字段，如下所示：define host&#123;use windows-serverhost_name serverAhostgroups 060202alias 060202contact_groups yuaddress 192.168.1.1&#125;#现在需要删除 ip 地址是 192.168.1.1 的段，可以这样：sed -i '/define host/&#123;:a;N;/&#125;/!ba;/192\.168\.1\.1/d&#125;' file#注意和前一个例子中的区别 t label 在一次输入后有成功执行的 s 替换命令才跳转到 label，如果省略了 label，则跳转到整条命令结尾 (即开始下一次读入) 12345678910111213#如行列转换$ seq 10|sed ':a;$!N;s/\n/,/;ta'1,2,3,4,5,6,7,8,9,10$#如将 MAC 地址 78A35114F798 改成带冒号的格式 78:A3:51:14:F7:98[root@centos7 temp]# echo '78A35114F798'|sed -r ':a;s/\B\w&#123;2&#125;\b/:&amp;/;ta'78:A3:51:14:F7:98[root@centos7 temp]##这里 \b 表示匹配单词边界，\B 表示匹配非单词边界的其他任意字符#当然也可以采用其他的方式实现：[root@centos7 temp]# echo '78A35114F798'|sed -r 's/..\B/&amp;:/g'78:A3:51:14:F7:98[root@centos7 temp]# T label 在一次输入后只要没有替换命令被成功执行就跳转到 label，如果省略了 label，则跳转到整条命令结尾 (即开始下一次读入) z 表示清除模式空间中内容，和 s/.*// 起相同的作用，但更有效。 更多例子1、删除匹配行的上一行和下一行1234567891011#例如输入数据为命令 seq 10 的输出 (当然也可以是任意其他文件内容)#要求删除匹配 5 那一行的前一行和后一行[root@centos7 temp]# seq 10|sed -n '$!N;/\n5/&#123;s/.*\n//p;N;d&#125;;P;D'123578910 2、合并奇偶数行123456#输入数据为命令 seq 11 的输出，要求分别将奇数和偶数分别放在同一行#输出第一行 `1 3 5 7 9 11`, 第二行 `2 4 6 8 10`$ seq 11|sed -nr '$!N;2!G;s/([^\n]+)\n((.+)\n)?(.+)\n(.+)/\4 \1\n\5 \3/;h;$p'1 3 5 7 9 112 4 6 8 10$ 3、合并多文件12345678910111213141516171819202122232425#文本 a.txt 的内容：01 12510101 400102 12310001 400203 12550101 400304 12610001 400405 12810001 400506 12310001 400607 12710001 400708 12310001 400809 12810101 400910 12510101 401011 12310001 401112 12610001 401213 12310001 4013#文本 b.txt 的内容：A 12410101 2006/02/15 2009/01/31 4002B 12310001 2006/08/31 2008/08/29 4001C 12610001 2008/05/23 2008/05/22 4002D 12810001 1992/12/10 1993/06/30 4001E 12660001 1992/05/11 1993/06/01 4005#要求输出 a.txt 内容中第二列和 b.txt 中第二列相同的行，并追加 b.txt 中对应的两个日期列。#形如：02 12310001 4002 2006/08/31 2008/08/29sed -rn '/^[01]/ba;H;:a;G;s/^((..)( .*)( [^\n]+)).*\3(( [^ ]*)&#123;2&#125;).*/\1\5/p' b.txt a.txt#当然如果使用 awk 来处理的话，解决思路更容易理解一些：awk 'NR==FNR&#123;a[$2]=$3FS$4;next&#125;&#123;if($2 in a)print $0,a[$2]&#125;' b.txt a.txt 为加深对 sed 各种命令特性的理解，请自行分析这三个例子。 各种命令的组合使用，再加上正则表达式的强大能力，使得sed可以处理所有能够计算的问题。但由于代码可读性不强，理解起来比较困难，通常使用sed作为一个文本编辑器，对文本做非交互的流式处理。理解上述各个命令的含义，熟练使用它们，就会发现sed的强大之处。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之进程与内存]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-process-and-memory%2F</url>
      <content type="text"><![CDATA[计算机存在的目的就是为了运行各种各样的程序，迄今我们介绍的绝大多数命令，都是为了完成某种计算而用编程语言编写的程序，它们以文件的形式保存在操作系统之中 (比如 /bin 下的各种命令)；但静态的程序并不能“自发的” 产生结果，只有在操作系统中为其指定输入数据并运行起来，才能得到输出结果。而操作系统中程序运行的最主要表现形式便是进程。 静态程序可以长久的存在，动态的进程具有有限的生命周期。每次程序运行的开始 (如键入一条命令后按下回车键)，操作系统都要为程序的运行准备各种资源，这些资源绝大多数都处于内存之中。为了限制多用户进程的权限，linux 还定义了两种进程运行时态：内核态和用户态；当进程想要请求系统服务时(比如操作一个物理设备)，必须通过系统调用(操作系统提供给用户空间的接口函数) 来实现，此时系统切换到内核态，代表程序执行该系统调用，执行完毕后系统切换回用户态，继续执行程序代码。本文介绍 linux 中关于进程与内存的管理命令 (更多的是查看命令) 1、uptime 系统运行时间1uptime [options] 单独执行此命令时，输出信息表示：当前时间，系统运行时长，登录用户个数，系统过去 1、5、15 分钟内的平均负载。 12$ uptime 10:46:38 up 58 days, 19:20, 3 users, load average: 0.00, 0.01, 0.05 2、ps 显示系统进程信息1ps [options] 单独运行 ps 命令时显示信息为：进程 ID 号 (PID)、终端 (TTY)、运行累积 CPU 时长 (TIME)、命令名 (CMD) 1234$ ps PID TTY TIME CMD 9503 pts/1 00:00:00 bash 9570 pts/1 00:00:00 ps 这里简要叙述一下关于进程、进程组、会话和终端的关系。linux 操作系统为了方便管理进程，将功能相近或存在父子、兄弟关系的进程归为一组，每个进程必定属于一个进程组，也只能属于一个进程组。一个进程除了有进程 ID 外，还有一个进程组 ID(PGID)；每个进程组都有一个进程组组长，它的 PID 和进程组 ID 相同。像一系列相关进程可以合并为进程组一样，一系列进程组也可以合并成一个会话 session。会话是由其中的进程建立的，该进程叫做会话的首进程 (session leader)。会话首进程的 PID 即为此会话的 SID(session ID)。每个会话都起始于用户登录，终止于用户退出。会话中的每个进程组称为一个工作(job)。会话可以有一个进程组成为会话的前台工作(foreground)，而其他的进程组是后台工作(background)。每个会话都关联到一个控制终端 control terminal，当会话终止时(用户退出终端)，系统会发送终止信号(SIGHUP) 给会话中的所有进程组，进程对此信号的默认处理方式为终止进程。 ps 接受三种格式的选项，带前缀符号 - 的 UNIX 格式的选项；不带前缀的 BSD 风格的选项；带两个 - 的 GNU 长格式选项。三种类型的选项可以自由组合，但可能会出现冲突。 选项 a(BSD) 表示显示所有和终端关联的进程信息，当配合选项 x(BSD) 一起使用时表示显示所有进程信息 (此时终端无关的进程 TTY 列显示为?)。 选项 -a(UNIX) 表示显示与终端关联的除了会话首进程之外的进程信息。选项 -e 表示所有进程。 1234567891011121314$ ps a PID TTY STAT TIME COMMAND 2528 tty1 Ss+ 0:00 -bash 9336 pts/0 Ss 0:00 -bash 9503 pts/1 Ss 0:00 -bash 9550 pts/2 Ss+ 0:00 -bash 9571 pts/0 S+ 0:00 man ps 9582 pts/0 S+ 0:00 less -s 9643 pts/1 R+ 0:00 ps a$ ps -a PID TTY TIME CMD 9571 pts/0 00:00:00 man 9582 pts/0 00:00:00 less 9644 pts/1 00:00:00 ps 如例子中所示，BSD 风格的选项还会显示进程的状态信息以及命令的参数。进程在运行的过程当中可能处于的状态包括： D 不可中断的睡眠状态 (通常在等待 IO) R 正在运行或可以运行 (在运行队列中) S 可中断的睡眠状态 (等待一个事件完成) T 暂停状态 t 跟踪状态 W 换页状态 (2.6 内核以后版本) X 死亡状态 (不可见) Z 僵死状态 BSD 风格的选项 STAT 列还可能包括以下字符 &lt; 高优先级进程 N 低优先级进程 L 锁定状态 s 会话首进程 l 多线程进程 进程处于前台进程组 选项 u 显示用户导向的进程信息 (如进程的发起用户，用户态占用 CPU 和 MEM 百分比等) 12345678910$ ps auUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 2528 0.0 0.0 115636 2384 tty1 Ss+ 9 月 30 0:00 -bashroot 9336 0.0 0.0 115596 2240 pts/0 Ss 08:44 0:00 -bashroot 9571 0.0 0.0 119196 1972 pts/0 S+ 10:59 0:00 man psroot 9582 0.0 0.0 110276 980 pts/0 S+ 10:59 0:00 less -sroot 9835 0.0 0.0 115636 2172 pts/1 Ss 13:48 0:00 -bashroot 9938 0.0 0.0 115512 2096 pts/2 Ss 14:49 0:00 -bashroot 9960 0.0 0.0 154068 5632 pts/2 S+ 14:50 0:00 vim others.shroot 9967 0.0 0.0 139496 1640 pts/1 R+ 14:59 0:00 ps au VSZ 表示占用的总的地址空间大小。它包括了没有映射到内存中的页面。 RSS 表示实际驻留 “在内存中” 的内存大小，不包括交换出去的内存。和 VSZ 的单位均为 KB 通常查看所有进程信息会使用命令 ps -ef 或 ps aux 选项 -o 或 o 表示指定输出格式 如显示所有 bash 进程的 pid，命令名，运行于哪颗逻辑 cpu： 12345$ ps -eo pid,comm,psr|grep bash 2528 bash 1 9336 bash 4 9835 bash 3 9938 bash 6 配合选项 –sort 可指定按某一列排序输出 12#表示按用户名排序ps -eo pid,user,args --sort user 还可以用 -o 指定许多其他信息，请查询相关手册。 3、kill 终止进程1kill [options] pid... 命令 kill 会发送特定的信号给指定的进程或进程组，如果没有指定信号，则发送 TERM 信号选项 -l 表示列出所有支持的信号： 123456789101112131415$ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX$ 可以使用选项 -s 指定要发送的信号 如在一个终端启动进程 sleep 300，在另一个终端查看并使用信号 SIGKILL 将其终止： 1234567891011$ sleep 300#此时会一直等待 sleep 执行完毕#在另一个终端中[root@centos7 temp]# ps -ef|grep [s]leeproot 10359 9835 0 12:05 pts/1 00:00:00 sleep 300#发送信号[root@centos7 temp]# kill -s SIGKILL 10359#原终端显示$ sleep 300已杀死$ 或者执行命令 kill -9 10359 是同样的效果。关于其他信号的作用，请自行搜索。 4、pgrep 和 pkill 搜索或者发送信号给进程12pgrep [options] patternpkill [options] pattern 这里的 pattern 是正则表达式，用来匹配进程名 如查看名称为 gunicorn 的所有进程 123456$ pgrep gunicorn1726817286172891729017293 选项 -l 显示进程名和 pid 123456$ pgrep -l gun17268 gunicorn17286 gunicorn17289 gunicorn17290 gunicorn17293 gunicorn 如终止所有 sleep 进程 1pkill sleep 如使 syslogd 重读它的配置文件 1pkill -HUP syslogd 5、top 显示进程信息top 命令实时动态的显示系统汇总信息和进程状态信息，它每隔 1s 刷新一次，按键盘 q 键退出。单独执行 top 命令时显示如下输出： 123456789101112131415top - 03:20:02 up 59 days, 17:30, 3 users, load average: 0.00, 0.01, 0.05Tasks: 184 total, 1 running, 183 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.1 us, 0.0 sy, 0.0 ni, 99.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 8010720 total, 5100308 free, 420652 used, 2489760 buff/cacheKiB Swap: 8257532 total, 8257532 free, 0 used. 6905944 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 193664 8708 2396 S 0.0 0.1 1:23.98 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.44 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.10 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 7 root rt 0 0 0 0 S 0.0 0.0 0:00.34 migration/0 8 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 9 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/0 10 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/1 下面分别对每行输出内容进行解释 (注：top 版本为 3.3.10，其他版本的输出第四行和第五行可能不同) 第一行显示信息和命令 uptime 的输出一致； 第二行显示任务汇总信息，状态即为进程可能状态中的四种； 第三行显示 cpu 负载信息，其中 us 表示用户态任务占用 CPU 时间百分比，sy 表示内核态任务占用 CPU 时间百分比，ni 表示改变过进程优先级的进程 (通过 nice 或 renice 命令) 占用 CPU 时间百分比，id 表示 CPU 空闲时间百分比，wa 表示等待输入输出的进程占用 CPU 时间百分比，hi 表示硬件中断花费时间，si 表示软件中断花费时间，st 表示虚拟机等待真实物理机 CPU 资源的时间 第四行显示内存信息，total 表示总内存，free 表示未分配内存，used 表示使用的内存 (值为 total-free-buff/cache 的结果)，buff/cache 表示缓存内存； 第五行显示交换分区使用量，其中 avail Mem 表示启动一个新程序时可以分配给它的最大内存，和第三行 free 列不同的地方在于，它会统计可以被回收的缓存分配器 (slab) 和页高速缓冲存储器 (page cache) 中的内存。(在一些较早的 top 实现中，并没有这一列的值) 接下来经过一个空行之后，显示的是进程相关信息，表头各列字段和 ps 命令的输出均有相对应的关系，其中 PR 表示优先级；NI 表示 nice 值 (后述)；VIRT 表示虚拟内存大小，对应 ps 命令中的 VSZ；RES 表示进程常驻内存大小，对应 ps 命令中的 RSS；SHR 表示共享内存大小；S 表示进程状态，对应 ps 命令的 STAT； linux 系统的进程状态中有一个优先级 (priority) 的概念，其值是一个动态变化的整数，范围是 0-139，此值越小，则优先级越高，那么它就越优先被 CPU 执行。如果 top 命令 PR 列显示为 rt，表示此进程为实时进程，它的优先级范围是 0-99，比其他的普通进程都要高。linux 中还有静态优先级的概念，用户可以通过使用命令 nice 和 renice 对进程设置或改变静态优先级，它可以看成是动态优先级的修正值，能够影响动态优先级的值。 PR 列显示的值为实际优先级减去实时进程最大优先级之后的值，3.10 内核非实时进程的默认值为 20，即：DEFAULT_PRIO = MAX_RT_PRIO + 20 = 120NI 列不为 0 时，表示进程被设置过静态优先级值，范围是 -20 到 19，它与当前优先级值的关系是：DEFAULT_PRIO = MAX_RT_PRIO + (nice) + 20如使用 nice 启动一个 sleep 进程： 1234567891011#当不使用选项 -n 指定时，默认值为 10$ nice -n -10 sleep 300#对于已存在的进程可以使用 renice 命令调整其静态优先级$$ ps -eo pri,ni,comm|grep sleep29 -10 sleep$$ top -bn1 |egrep 'COMMAND$|sleep$' PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND11967 root 10 -10 107892 616 528 S 0.0 0.0 0:00.00 sleep#注意这里 ps 和 top 优先级值显示的不同，ps 命令 pri 列的值 29 = MAX_PRIO(139) - MAX_RT_PRIO(100) + nice(-10)。它们实际的优先级值是相等的。 上例中使用了选项 -n 表示 top 刷新次数，-b 表示批处理模式运行 top，此模式会去掉输出中的控制字符，方便将输出交给其他程序处理。 选项 -o fieldname 按指定列排序输出，选项 -O 可以列出 -o 能够指定的列名 123#自行执行命令查看效果top -O |tr '\n' ' 'top -bn1 -o PR 下面简要介绍一些 top 中可以使用的交互命令： q 退出 top h 获得帮助信息 1 显示每个逻辑 cpu 的信息 k 终止一个进程 (会提示用户输入需要终止的 pid，以及需要发送什么样的信号) r 重新设置进程静态优先级 (相当于执行 renice) i 忽略闲置和僵死进程 H 显示线程信息 M 根据驻留内存大小排序 P 根据 CPU 使用百分比排序 W 将当前设置写入~/.toprc 文件中 6、free 显示系统内存使用情况1free [options] free 命令显示系统当前内存、swap(交换分区) 的使用情况，默认单位是 KB 12345#版本 3.3.10$ free total used free shared buff/cache availableMem: 8010720 423060 4540476 375580 3047184 6897052Swap: 8257532 0 8257532 显示信息和 top 命令输出中的对应值一致，其中 shared 表示内存文件系统 (tmpfs) 中使用内存的大小。前面讲述了 available 对应值所表示的含义，通常查看系统当前还有多少可用内存，看 available 的对应值就可以了。这里 available = free + 缓存 (可被回收部分)。但在较老版本的 free 中并没有这个值，它的输出可能是这样的： 1234 total used free shared buffers cachedMem: 8174384 4120488 4053896 0 229320 1041712-/+ buffers/cache: 2849456 5324928Swap: 16779884 0 16779884 说明： buffer(缓冲) 是为了提高内存和硬盘 (或其他 I/O 设备) 之间的数据交换的速度而设计的 cache(缓存) 是为了提高 cpu 和内存之间的数据交换速度而设计的 所以输出中 buffers 可简单理解为准备写入硬盘的缓冲数据；cached 可理解为从硬盘中读出的缓存数据 (页高速缓冲存储器)，缓存中可被回收部分来自 cached 和 slab(缓存分配器) Mem 行：used = total - free 此时的空闲内存 free 列并不能体现系统当前可用内存大小-/+ buffers/cache 行：used = total - free(Mem) - (buffers + cached)，这里的 free 列和前面所述的 available 关系为 available = free + 缓存 (可被回收部分) 所以当没有 available 列可查看时，并不能通过 free 命令查到或计算出真正可用内存，需要知道缓存部分的具体情况。 选项 -b、-k、-m、-g 分别表示指定各值的单位：bytes, KB, MB, 或者 GB 7、fuser 使用文件或套接字定位进程fuser 经常用来查看文件被哪些进程所使用 12$ fuser ./root: 2528c 11430c 11447c 例子表示显示有三个进程在使用当前目录，其中：2528c 前面数字表示进程 PID，后面的字符 c 表示当前目录 (即进程在此目录下工作)，还可能出现的字符有： e 表示进程正在运行执行文件 f 打开文件，默认输出时省略 F 写方式打开文件，默认时输出省略 r 根目录 m mmap 文件或共享库文件 选项 -k 表示发送信号 SIGKILL 给相关进程 (谨慎使用) 选项 -i 表示交互，在 kill 一个进程之前询问用户 选项 -l 列出支持的信号 选项 -SIGNAL 指定信号 8、lsof 列出打开文件在这一篇中我们简单描述了 bash 进程打开的前三个文件，并分别关联到文件描述符 0,1,2。对于其他进程打开的文件也是同样，系统为每个进程维护一个文件描述符表，该表的值都是从 0 开始的数字。单独执行 lsof 命令时会显示系统中所有进程打开的文件 123456789101112#命令版本为 4.87[root@centos7 temp]# lsof |headCOMMAND PID TID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root cwd DIR 253,0 4096 128 /systemd 1 root rtd DIR 253,0 4096 128 /systemd 1 root txt REG 253,0 1489960 6044 /usr/lib/systemd/systemdsystemd 1 root mem REG 253,0 20032 201329002 /usr/lib64/libuuid.so.1.3.0systemd 1 root mem REG 253,0 252704 201330338 /usr/lib64/libblkid.so.1.1.0systemd 1 root mem REG 253,0 90632 201328968 /usr/lib64/libz.so.1.2.7systemd 1 root mem REG 253,0 19888 201329137 /usr/lib64/libattr.so.1.1.0systemd 1 root mem REG 253,0 19520 201328509 /usr/lib64/libdl-2.17.sosystemd 1 root mem REG 253,0 153192 201328867 /usr/lib64/liblzma.so.5.0.99 每行一个打开的文件，表头各列意为： 1234567891011121314151617181920212223242526272829303132333435363738COMMAND 进程命令名前 9 个字符PID 进程 IDTID 任务 IDFD 1) 文件描述符号或者下面字符： cwd 当前工作目录 err FD 错误信息 ltx 共享库代码 mem 内存映射文件 mmap 内存映射设备 pd 父目录 rtd 根目录 txt 程序代码 2) 当是 FD(数字) 时，后面可能跟下面权限字符： r 读 w 写 u 读写 空格 权限未知且无锁定字符 - 权限未知但有锁定字符 3) 权限字符后可能有如下锁定字符： r 文件部分读锁 R 整个文件读锁 w 文件部分写锁 W 整个文件写锁 u 任意长度读写锁 U 未知类型锁 空格 无锁TYPE 类型，可能值为： DIR 目录 REG 普通文件 CHR 字符设备文件 BLK 块设备文件 FIFO 管道文件 unix UNIX 套接字文件 IPv4 IPv4 套接字文件 ....DEVICE 设备号SIZE/OFF 文件大小或偏移量 (bytes)NODE 文件 inode 号 选项 -n 表示不做 ip 到主机名的转换 选项 -c string 显示 COMMAND 列中包含指定字符的进程所有打开的文件 选项 -u username 显示所属 user 进程打开的文件 选项 -d FD 显示打开的文件描述符为 FD 的文件 12345678$ lsof -d 4COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root 4u a_inode 0,9 0 5755 [eventpoll]systemd-j 539 root 4u unix 0xffff880230168f00 0t0 10467 /run/systemd/journal/socketsystemd-u 549 root 4u unix 0xffff88003693d640 0t0 12826 /run/udev/controllvmetad 555 root 4wW REG 0,18 4 8539 /run/lvmetad.pidauditd 693 root 4w REG 253,0 701364 208737917 /var/log/audit/audit.log.... 选项 +d DIR 显示目录中被进程打开的文件 选项 +D DIR 递归显示目录中被进程打开的文件 1234$ lsof +d /root|head -3COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 2528 root cwd DIR 253,0 4096 201326721 /rootbash 12902 root cwd DIR 253,0 4096 201326721 /root 选项 -i 表示显示符合条件的进程打开的文件，格式为 [46][protocol][@hostname|hostaddr][:service|port] 123456789101112#查看 22 端口运行情况$ lsof -ni :22COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1358 root 3u IPv4 8979 0t0 TCP *:ssh (LISTEN)sshd 1358 root 4u IPv6 8981 0t0 TCP *:ssh (LISTEN)sshd 12900 root 3u IPv4 3509687 0t0 TCP 10.0.1.254:ssh-&gt;192.168.78.143:57325 (ESTABLISHED)#例子，smtp 为 /etc/services 文件中列出服务中的一种$ lsof -ni 4TCP@0.0.0.0:22,smtp COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1358 root 3u IPv4 8979 0t0 TCP *:ssh (LISTEN)master 2162 root 13u IPv4 16970 0t0 TCP 127.0.0.1:smtp (LISTEN)sshd 12900 root 3u IPv4 3509687 0t0 TCP 10.0.1.254:ssh-&gt;192.168.78.143:57325 (ESTABLISHED) 试想，如果删除了一个正在被其他进程打开的文件会怎样？实验来看看现象： 123456789101112131415161718192021222324#使用 more 命令查看一个文件$ more /root/.bash_history#在另一个终端使用 lsof 查看$ lsof|grep ^moremore 14470 root cwd DIR 253,0 4096 201326721 /rootmore 14470 root rtd DIR 253,0 4096 128 /more 14470 root txt REG 253,0 41096 134321844 /usr/bin/moremore 14470 root mem REG 253,0 106065056 134319094 /usr/lib/locale/locale-archivemore 14470 root mem REG 253,0 2107816 201328503 /usr/lib64/libc-2.17.somore 14470 root mem REG 253,0 174520 201328905 /usr/lib64/libtinfo.so.5.9more 14470 root mem REG 253,0 164440 225392061 /usr/lib64/ld-2.17.somore 14470 root mem REG 253,0 272001 67147302 /usr/share/locale/zh_CN/LC_MESSAGES/util-linux.momore 14470 root mem REG 253,0 26254 201328839 /usr/lib64/gconv/gconv-modules.cachemore 14470 root 0u CHR 136,1 0t0 4 /dev/pts/1more 14470 root 1u CHR 136,1 0t0 4 /dev/pts/1more 14470 root 2u CHR 136,1 0t0 4 /dev/pts/1more 14470 root 3r REG 253,0 17656 202386313 /root/.bash_history#删除这个文件$ rm -f /root/.bash_history#查看$ lsof -d 3|grep ^moremore 14470 root 3r REG 253,0 17656 202386313 /root/.bash_history (deleted)$#会发现文件列多出了 delete 的字样 linux 系统中 /proc 目录保存了系统所有进程相关的数据，里面的数字目录名即为 PID。我们进一步来看一下刚才的 more 进程的文件描述符 1234567$ cat /proc/14470/fd/3 &gt; /root/.bash_history.bak#此操作会将文件描述符 3 中的内容保存至 /root/.bash_history.bak#停止 more 进程并查看$ ls -l /root/.bash_history*-rw-r--r-- 1 root root 17656 11 月 30 07:47 /root/.bash_history.bak$ cat /root/.bash_history.bak#会发现原文件没有了，新文件保存了原文件的所有内容 结论就是，如果在删除文件的时候有进程正在打开该文件，那么该文件的内容还是可以通过进程的对应文件描述符恢复的。同时，如果删除了某文件，发现空间并没有释放，说明有进程正在打开该文件 (命令 lsof|grep delete 查看)，重新启动该进程之后，空间就会得到释放。 9、iostat 显示 CPU、I/O 统计信息12345678910111213$ iostatLinux 3.10.0-327.el7.x86_64 (centos7) 2016 年 11 月 30 日 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.12 0.00 0.03 0.00 0.00 99.85Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 0.23 0.79 3.05 4178309 16079082dm-0 0.22 0.57 2.94 3002207 15480498dm-1 0.00 0.00 0.00 1088 0dm-2 0.03 0.22 0.11 1146430 596232dm-3 0.06 0.01 1.91 28900 10079073dm-4 0.03 0.01 1.91 28644 10079073 显示信息中 cpu 部分在命令 top 的描述中都有相应的解释，I/O 部分是各个设备读写速率及总量信息，其中 tps 表示每秒多少次 I/O 请求 选项 -c 显示 CPU 信息 选项 -d 显示设备信息 选项 -x 显示更详细的信息 命令 iostat m n 数字 (m,n)，m 表示时间间隔，n 表示次数；此时 iostat 会每隔 m 秒打印一次，打印 n 次。 123456789101112131415161718192021222324$ iostat -c 1 3Linux 3.10.0-327.el7.x86_64 (centos7) 2016 年 11 月 30 日 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.12 0.00 0.03 0.00 0.00 99.85avg-cpu: %user %nice %system %iowait %steal %idle 0.12 0.00 0.00 0.00 0.00 99.88avg-cpu: %user %nice %system %iowait %steal %idle 0.12 0.00 0.12 0.00 0.00 99.75```bash也可以接设备名表示查看指定设备的 I/O 信息```bash$ iostat sdaLinux 3.10.0-327.el7.x86_64 (centos7) 2016 年 11 月 30 日 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.12 0.00 0.03 0.00 0.00 99.85Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 0.23 0.79 3.05 4178309 16084862 10、vmstat 显示虚拟内存统计信息1vmstat [options] [delay [count]] 同样也会显示一些 CPU 和 I/O 的信息 选项 -w 格式化输出 1234$ vmstat -wprocs -----------------------memory---------------------- ---swap-- -----io---- -system-- --------cpu-------- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 4517628 3184 3067904 0 0 0 1 1 0 0 0 100 0 0 其中 1234567891011121314151617181920212223procs r 表示可运行状态进程数量 b 表示不可中断睡眠状态进程数量memory swpd 虚拟内存使用量 free 空闲内存 buff buffer 缓冲中内存使用量 cache cache 缓存中内存使用量swap si 硬盘交换至内存量 so 内存交换至硬盘量io bi 从块设备中收到的块 (blocks) 数 bo 发送至块设备的块数system in 每秒中断次数，包括锁。 cs 每秒进程上下文切换次数。cpu (同命令 top) us 用户态任务占用 CPU 时间百分比 sy 内核态任务占用 CPU 时间百分比 id CPU 空闲时间百分比 wa 等待输入输出的进程占用 CPU 时间百分比 st 虚拟机等待真实物理机 CPU 资源的时间 选项 -m 显示 slab 信息 选项 -s 显示各种内存计数器及其信息 选项 -d 显示磁盘 I/O 信息 选项 -p device 显示设备分区详细 I/O 信息 同 iostat 一样也支持按频率打印次数 11、mpstat 显示 CPU 相关信息1mpstat [options] [interval [count]] 显示信息和 top 命令相似 1234567$ mpstat 1 2Linux 3.10.0-327.el7.x86_64 (centos7) 2016 年 11 月 30 日 _x86_64_ (8 CPU)09 时 18 分 19 秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle09 时 18 分 20 秒 all 0.12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.8809 时 18 分 21 秒 all 0.12 0.00 0.12 0.00 0.00 0.00 0.00 0.00 0.00 99.75平均时间: all 0.12 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 99.81 选项 -A 显示所有 CPU 及中断信息相当于执行 mpstat -I ALL -P ALL 选项 -I {SUM | CPU | SCPU | ALL} 显示中断信息 选项 -P {cpu [,...] | ON | ALL } 显示 CPU 信息 123456$ mpstat -P 3,5Linux 3.10.0-327.el7.x86_64 (centos7) 2016 年 11 月 30 日 _x86_64_ (8 CPU)09 时 29 分 03 秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle09 时 29 分 03 秒 3 0.15 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 99.8109 时 29 分 03 秒 5 0.11 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 99.86 本文简单介绍了linux中进程和内存的相关命令，进程和内存在计算机操作系统中非常重要，涉及到的内容也非常多，这里就不做展开了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之启动流程]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-boot-sequence%2F</url>
      <content type="text"><![CDATA[固件 (firmware) 是指设备最底层的，让设备得以运行的程序代码。简单理解就是：固定在硬件上的软件。计算机中的许多设备都拥有固件(如硬盘、鼠标、光驱、U 盘等)，在计算机启动过程中，最先读取的就是位于主板上的固件，这个固件当前有两种类型：传统的 BIOS 和新的通用性更强的 UEFI。 在上一篇中，我们提到另一种磁盘分区格式 GTP 也是 UEFI 标准的一部分。于是，当前计算机启动中，出现了两种不同的方式：BIOS/MBR 和 UEFI/GTP。 在 linux 操作系统的世界中，同样在经历着变革，系统初始化软件 sysvinit 正逐渐被 systemd 取代。本文将主要讲述传统的 BIOS/MBR–&gt;sysvinit 启动方式，同时，作为补充，也将简述 UEFI/GTP–&gt;systemd 的启动方式。 BIOS/MBR–&gt;sysvinit1、BIOS 阶段系统加电后会立即读取 BIOS 中内容并执行，BIOS 中程序的执行包括两个步骤： 加电自检 POST(power-on self test)，主要负责检测系统外围设备 (如 CPU、内存、显卡、键盘鼠标等) 是否正常。如果硬件出现问题，主板会发出不同含义的蜂鸣声，启动终止。如果没有问题，屏幕就会显示出 CPU、内存、硬盘等信息。 自检完成后，BIOS 会执行一段程序来枚举本地设备 (如光盘、U 盘、硬盘、网络等，可以在 BIOS 中设置枚举顺序) 寻找下一阶段的启动程序所在位置。BIOS 会将控制权交给启动顺序 (Boot Sequence) 中排在第一位的设备，此时，计算机读取该设备中的最前面的 512 个字节，如果这 512 个字节的最后两个字节是 0x55 和 0xAA(Magic Number)，表明这个设备可以用于启动；如果不是，表明该设备不能用于启动，控制权于是转交给启动顺序中的下一个设备。如上一篇所述，硬盘中的最前面的 512 字节即为主引导记录 MBR。 2、MBR 阶段前一篇中我们描述过 MBR 的结构，其中包括 446 字节的 Bootloader，64 字节的 DPT 和 2 字节的 Magic Number。Bootloader(引导加载程序) 中较常用的一种是 grub，grub 引导分为两个阶段 (有些 grub 还定义了 1.5 阶段)： BIOS 将 stage1 载入内存中的指定位置 (0x7C00) 并跳转执行，stage1 的内容即为 MBR 中起始的 446 字节；此阶段执行作用主要是将硬盘 0 磁头 0 磁道 2 扇区的内容载入到内存 0x8000 处并跳转执行。 由于 stage2 的代码 (较大) 存放在文件系统下的 /boot 分区中(或者 /boot 没有单独分区的 /etc/)，因此识别 stage2 文件需要文件系统环境(此时还只能直接读取硬盘指定位置的内容，并不能识别文件系统)。stage1.5 的作用就是为 stage2 提供文件系统环境，使系统能够找到位于文件系统中的 stage2 文件。 stage2 被载入内存并执行，它首先会解析 grub 的配置文件 menu.lst 即 /boot/grub/grub.conf，该文件中指定了系统内核文件所处的位置，如果没有找到该文件，就会执行一个 shell，等待用户手动指定内核文件的位置。此阶段的最终状态就是执行 boot 命令，将内核和 initrd 镜像载入内存，进而将控制权交给内核。 grub.conf 内容 (版本：GNU GRUB 0.97)： 123456789101112131415161718192021# grub.conf generated by anaconda## Note that you do not have to rerun grub after making changes to this file# NOTICE: You have a /boot partition. This means that# all kernel and initrd paths are relative to /boot/, eg.# root (hd0,0)# kernel /vmlinuz-version ro root=/dev/sda3# initrd /initrd-version.img#boot=/dev/sdadefault=0timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (2.6.18-407.el5) root (hd0,0) kernel /vmlinuz-2.6.18-407.el5 ro root=LABEL=/ rhgb quiet initrd /initrd-2.6.18-407.el5.imgtitle CentOS (2.6.18-398.el5) root (hd0,0) kernel /vmlinuz-2.6.18-398.el5 ro root=LABEL=/ rhgb quiet initrd /initrd-2.6.18-398.el5.img 文件中 # 开头的行是注释行，最重要的部分是两个 title 下面指定的内核位置及具体文件 (kernel 和 initrd 项) 3、内核阶段grub 的 stage2 将 initrd 文件加载到内存中，内核于是开始执行 initrd 中的 init 文件，此文件是一个脚本，主要作用是加载各种存储介质相关的设备驱动程序。当所需的驱动程序加载完成后，会创建一个根设备，然后将根文件系统 (rootfs) 以只读的方式挂载。这一步结束后，释放未使用的内存，转换到真正的根文件系统中运行程序 /sbin/init，启动系统 PID 为 1 的进程。此后系统的控制权就交给 /sbin/init 进程了。 4、init 阶段当 init 进程接管了系统的控制权之后，它首先会读取 /etc/inittab 文件，此文件描述了在特定的运行级别 (runlevel) 下，init 进程该如何初始化系统。 linux 中定义了 7 种运行级别： 0 表示关机 1 表示单用户模式 2 表示无网络的多用户模式 3 表示多用户模式 4 未使用 5 表示图形界面模式 6 表示重启 inittab 文件中指定了系统的默认运行级别，如 id:3:initdefault: 表示默认运行级别为 3(多用户模式)。 init 进程根据 inittab 文件，运行一系列指定的初始化脚本： /etc/rc.d/rc.sysinit 系统初始化脚本，它的作用包括设置主机名和默认网关、决定是否启用 SELinux、加载用户自定义模块、根据文件 /etc/sysctl.conf 设置内核参数、设置 raid 及 LVM 等硬盘功能、重新以读写方式挂载根文件系统等等 执行 /etc/rc.d/rc 文件，该文件确认由 inittab 指定的运行级别 N，并启动相应级别下的服务 (通过执行 /etc/rc.d/rcN.d 中的文件)，例如运行级别为 3 时，则先执行 /etc/rc.d/rc3.d 下以 K 开头的文件，然后执行以 S 开头的文件。这些文件都是指向 /etc/init.d 下的符号链接。以 K 开头的文件表示此运行级别下需要关闭的服务，以 S 开头的文件表示此运行级别下需要开启的服务。 在运行级别 2、3、4、5 中最后一个执行的文件均指向文件 /etc/rc.local, 用户可以在此文件中自定义启动内容。 之后根据 inittab 中设置，运行 6 个终端，以便用户登录系统，如果是运行级别 5，则还会执行 /etc/X11/prefdm -nodaemon 启动相应的桌面环境。 然后执行 /bin/login 程序用于接收和验证来自 mingetty 的用户名和密码。 至此整个系统即启动完毕了 UEFI/GTP–&gt;systemdUEFI 的出现是为了代替 BIOS，同样，GTP 和 systemd 也是为了弥补 MBR 和 sysvinit 的不足。和 BIOS 只负责 POST 和找到 MBR 不同，UEFI 将贯穿系统加电到关机的整个过程。粗略划分，UEFI 系统启动分为 4 个阶段： 1、UEFI 初始化阶段 SEC(安全验证)：接收并处理系统启动和重启信号，初始化临时存储区域，传递系统参数给下一阶段 (即 PEI)。 PEI(EFI 前期初始化)：为 DXE 准备执行环境，将需要传递到 DXE 的信息组成 HOB(Handoff Block) 列表，最终将控制权转交到 DXE 手中。 DXE(驱动执行环境)：根据 HOB 列表初始化系统服务，然后遍历固件中的所有 Driver，当驱动的依赖资源满足时，调度 Dirver 到执行队列执行，直到所有满足条件的 Dirver 都被加载。 2、操作系统加载器作为 UEFI 应用程序运行阶段 BDS(启动设备选择)：初始化控制台设备，加载必要的设备驱动，根据系统设置加载和执行启动项，用户选中某个启动项（或系统进入默认的启动项）后，OS Loader 启动，系统进入 TSL 阶段。 UEFI 中程序能够识别存储介质上的分区信息和文件系统 (如：fat32)，此时会将 /EFI/boot/grub2.efi(位于 GTP 格式硬盘的一个分区 ESP，安装时自动生成) 作为 UEFI 应用程序运行。 TSL(临时系统加载)：操作系统加载器 (OS Loader 也位于 ESP 分区) 执行的第一阶段，在这一阶段 OS Loader 作为一个 UEFI 应用程序运行，系统资源仍然由 UEFI 内核控制。当启动服务的 ExitBootServices()服务被调用后，系统进入 RT(Run Time)阶段。 3、操作系统运行阶段RT(运行时)：系统的控制权从 UEFI 内核转交到 OS Loader 手中，UEFI 占用的各种资源被回收到 OS Loader，仅有 UEFI 运行时服务保留给 OS Loader 和 OS 使用。随着 OS Loader 的执行，OS 最终取得对系统的控制权。 在 init 作为系统初始化程序时，服务是通过 /etc/rc.d/init.d 中的脚本来管理并且是顺序执行的，当使用 systemd 作为系统初始化程序后，这些脚本被服务单元替换，并尽可能的并行启动进程。 在 systemd 中，一个单元配置文件可以描述如下内容之一： 系统服务 (.service) 挂载点 (.mount) 套接字 (.sockets) 系统设备 (.device) 交换分区 (.swap) 文件路径 (.path) 启动目标 (.target) 由 systemd 管理的计时器 (.timer) …. systemd 为保持向下兼容性还保留了一些 init 命令和概念，但所对应的文件都是指向 systemd 对应命令或文件的符号链接： 12345678910[root@centos7 temp]# ls -l /sbin/initlrwxrwxrwx. 1 root root 22 1 月 15 2016 /sbin/init -&gt; ../lib/systemd/systemd[root@centos7 temp]# ls -l /usr/lib/systemd/system/runlevel*.targetlrwxrwxrwx. 1 root root 15 1 月 15 2016 /usr/lib/systemd/system/runlevel0.target -&gt; poweroff.targetlrwxrwxrwx. 1 root root 13 1 月 15 2016 /usr/lib/systemd/system/runlevel1.target -&gt; rescue.targetlrwxrwxrwx. 1 root root 17 1 月 15 2016 /usr/lib/systemd/system/runlevel2.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 17 1 月 15 2016 /usr/lib/systemd/system/runlevel3.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 17 1 月 15 2016 /usr/lib/systemd/system/runlevel4.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 16 1 月 15 2016 /usr/lib/systemd/system/runlevel5.target -&gt; graphical.targetlrwxrwxrwx. 1 root root 13 1 月 15 2016 /usr/lib/systemd/system/runlevel6.target -&gt; reboot.target systemd 启动后执行的第一个目标是 default.target，但实际上 default.target 是指向 graphical.target 的符号链接。 123456789101112131415161718[root@centos7 temp]# ls -l /usr/lib/systemd/system/default.targetlrwxrwxrwx. 1 root root 16 1 月 15 2016 /usr/lib/systemd/system/default.target -&gt; graphical.target[root@centos7 temp]# cat /usr/lib/systemd/system/graphical.target# This file is part of systemd.## systemd is free software; you can redistribute it and/or modify it# under the terms of the GNU Lesser General Public License as published by# the Free Software Foundation; either version 2.1 of the License, or# (at your option) any later version.[Unit]Description=Graphical InterfaceDocumentation=man:systemd.special(7)Requires=multi-user.targetWants=display-manager.serviceConflicts=rescue.service rescue.targetAfter=multi-user.target rescue.service rescue.target display-manager.serviceAllowIsolate=yes 其中 Requires 行指明了本单元的依赖关系 (其他各项意义可以通过命令 man systemd.unit 查看)，顺着此文件，可以找到需要执行的单元：multi-user.target、basic.target、sysinit.target、local-fs.target swap.target、local-fs-pre.target。 4、关机阶段AL(After-life)：当系统硬件或操作系统出现严重错误不能继续正常运行时，固件会尝试修复错误，这时系统进入 AL 期。UEFI 标准并没有定义此阶段的行为和规范。系统供应商可以自行定义。 相关命令init1、initinit 除了在系统初始化时起的重要作用外，还可以用来执行关机、重启、切换运行级别的作用： 123456#关机init 0#重启init 6#切换到单用户模式init 1 2、runlevel 显示运行级别123[root@centos7 temp]# runlevelN 3[root@centos7 temp]# 输出中 N 表示当前运行级别，如果系统启动后切换过运行级别，则输出类似于 3 5 表示之前运行级别为 3，现在的运行级别为 5。 3、halt reboot poweroff shutdown这几个命令的作用就是关机或重启系统，通常只使用 shutdown 就可以了： 12345678#立即关机shutdown -h now#在 11:50 分执行关机shutdown -h 11:50#如果要取消指定时间的关机，则在另一个终端中执行：shutdown -c#过 30 分钟之后重启系统，并且重启时不进行磁盘检测shutdown -fr +30 4、chkconfig 更新或查询服务的运行级别信息123456#列出服务 (还会列出 xinetd 管理的服务)chkconfig --list#增加一个服务chkconfig --add httpd#使服务在运行级别 2、3、5 时自启动chkconfig --level 235 httpd on 5、service 运行服务脚本 (服务脚本位于 /etc/init.d 内，service 本身也是脚本，位于 /sbin 内)123456789101112#列出所有服务状态service --status-all#列出单个服务状态service nginx status#启动服务service nginx start#停止服务service nginx stop#重启服务service nginx restart#重新加载配置文件service nginx reload systemdsystemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。 1、systemctl 控制 systemd 系统和管理服务1systemctl [OPTIONS...] COMMAND [NAME...] 如切换运行级别或开关机： 123456789101112#重启 (将执行 reboot.target)systemctl reboot#暂停 (将执行 suspend.target)systemctl suspend#休眠 (将执行 hibernate.target)systemctl hibernate#切换至救援模式 (单用户，将执行 rescue.target)systemctl rescue#列出运行级别systemctl get-default#切换到运行级别 5，即图形模式systemctl isolate graphical.target 系统服务单元相关： 12345678#列出正在运行的 Unitsystemctl list-units#列出所有的 Unitsystemctl list-units --all#列出所有加载失败的 Unitsystemctl list-units --failed#列出 Unit 时指定类型systemctl list-units --type=socket 系统和服务管理： 123456789101112131415161718#系统状态systemctl status#服务状态 (.service 可以省略)systemctl status nginx.service#启动服务systemctl start nginx#停止服务systemctl stop nginx#重启服务systemctl restart nginx#重新加载配置文件systemctl reload nginx#设置服务开机启动systemctl enable nginx#列出所有安装的服务systemctl list-unit-files#指定类型systemctl list-unit-files --type=target 还有许多其他选项，这里就不一一列举了。 2、systemd-analyze 查看启动用时12$ systemd-analyzeStartup finished in 730ms (kernel) + 1.904s (initrd) + 9.909s (userspace) = 12.544s 输出显示了系统启动过程中各部分耗时 123456789101112#各服务初始化用时$ systemd-analyze blame 5.424s NetworkManager-wait-online.service 1.830s dev-mapper-centos\x2droot.device 1.055s firewalld.service 980ms kdump.service 549ms network.service ....#输出各服务用时细节并写入文件 (该文件可以用浏览器或图片查看器打开)$ systemd-analyze plot &gt; file.svg#序列化输出各服务详细完整的状态信息 (输出内容很多，略)$ systemd-analyze dump 3、systemd-cgls 递归显示控制组 (Cgroups) 信息linux 内核从版本 2.6.24 开始，引入了一个叫做控制组 (control groups) 的特性，是用于限制、记录、隔离进程组（process groups）所使用的物理资源（如：cpu,memory,IO 等等）的机制。关于 Cgroups 的内容本文不再展开。 12345678910$ systemd-cgls├─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 21├─user.slice│ └─user-0.slice│ ├─session-182.scope│ │ ├─5165 sshd: root@pts/1 │ │ ├─5167 -bash│ │ ├─5409 systemd-cgls........ 4、systemd-cgtop 显示各控制组的使用量 (CPU, 内存, IO)显示效果类似命令 top 12345678910$ systemd-cgtopPath Tasks %CPU Memory Input/s Output/s/ 161 0.2 400.5M - -/system.slice/NetworkManager.service 1 - - - -/system.slice/auditd.service 1 - - - -/system.slice/crond.service 1 - - - -/system.slice/dbus.service 1 - - - -/system.slice/firewalld.service 1 - - - -.... 5、systemd-loginctl 控制 systemd 登录管理此命令是命令 loginctl 的符号链接 123456789101112131415161718192021222324252627282930#列出当前会话$ systemd-loginctl list-sessions SESSION UID USER SEAT 182 0 root 154 0 root 2 sessions listed.#列出当前登录用户$ loginctl list-users UID USER 0 root 1 users listed.#列出显示指定用户的信息$ loginctl show-user rootUID=0GID=0Name=rootTimestamp = 三 2016-12-21 08:38:54 CSTTimestampMonotonic=77015538361RuntimePath=/run/user/0Slice=user-0.sliceDisplay=154State=activeSessions=182 154IdleHint=noIdleSinceHint=0IdleSinceHintMonotonic=0Linger=no$ 6、timedatectl 系统时间和日期控制123456789101112131415161718192021$ timedatectl Local time: 三 2016-12-21 13:47:31 CST Universal time: 三 2016-12-21 05:47:31 UTC RTC time: 三 2016-12-21 05:47:31 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: n/aNTP synchronized: no RTC in local TZ: no DST active: n/a#设置时间$ timedatectl set-time "2012-10-30 18:17:16"#列出时区$ timedatectl list-timezonesAfrica/AbidjanAfrica/AccraAfrica/Addis_AbabaAfrica/AlgiersAfrica/Asmara....#设置时区$ timedatectl set-timezone America/New_York 7、hostnamectl 系统主机名控制123456789101112131415161718192021222324252627#状态$ hostnamectl status Static hostname: centos7 Icon name: computer-vm Chassis: vm Machine ID: 956ab824a02d489d85b079cb442d5442 Boot ID: 9016d7627d8148ecb7fb77afaa89aeab Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-327.el7.x86_64 Architecture: x86-64#设置主机名 (内核参数 /proc/sys/kernel/hostname 和文件 /etc/hostname 中都立即更新)$ hostnamectl set-hostname MYHOST#重新登录后主机名即变为 myhost(静态主机名)$ hostnamectl Static hostname: myhost Pretty hostname: MYHOST Icon name: computer-vm Chassis: vm Machine ID: 956ab824a02d489d85b079cb442d5442 Boot ID: 9016d7627d8148ecb7fb77afaa89aeab Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-327.el7.x86_64 Architecture: x86-64 以上 systemd 相关所有命令 (除 systemd-cgls 和 systemd-cgtop 外)，都可以使用选项 -H 指定远程基于 systemd 的主机 (使用 ssh 协议)： 1234567891011$ hostnamectl -H 10.0.1.252 Static hostname: idc-v-71252 Icon name: computer-vm Chassis: vm Machine ID: 956ab824a02d489d85b079cb442d5442 Boot ID: 9016d7627d8148ecb7fb77afaa89aeab Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 4.4.4-1.el7.elrepo.x86_64 Architecture: x86-64 systemd 功能强大，使用方便，但也比较复杂，体系庞大。本文只介绍一点相关命令，更多内容就不在此展开了。 本文简述了传统的BIOS和新的UEFI启动流程，介绍了init和systemd部分相关命令。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CGroup 介绍、应用实例及原理描述]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-cgroup%2F</url>
      <content type="text"><![CDATA[CGroup 技术被广泛用于 Linux 操作系统环境下的物理分割，是 Linux Container 技术的底层基础技术，是虚拟化技术的基础。本文首先介绍了 Cgroup 技术，然后通过在 CentOS 操作系统上部署、配置、运行一个实际多线程示例的方式让读者对物理限制 CPU 核的使用有一个大概的了解，接着通过讲解 CGroup 内部的设计原理来让读者有进一步的深入了解 CGroup 技术。 CGroup 介绍CGroup 是 Control Groups 的缩写，是 Linux 内核提供的一种可以限制、记录、隔离``进程组 (process groups)所使用的物理资源 (如 cpu、 memory、 i/o 等等) 的机制。2007 年进入 Linux 2.6.24 内核，CGroups 不是全新创造的，它将进程管理从 cpuset 中剥离出来，作者是 Google 的 Paul Menage。CGroups 也是 LXC 为实现虚拟化所使用的资源管理手段。 CGroup 功能及组成CGroup 是将任意进程进行分组化管理的 Linux 内核功能。CGroup 本身是提供将进程进行分组化管理的功能和接口的基础结构，I/O 或内存的分配控制等具体的资源管理功能是通过这个功能来实现的。这些具体的资源管理功能称为 CGroup 子系统或控制器。CGroup 子系统有控制内存的 Memory 控制器、控制进程调度的 CPU 控制器等。运行中的内核可以使用的 Cgroup 子系统由 /proc/cgroup 来确认。 CGroup 提供了一个 CGroup 虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用 CGroup，必须挂载 CGroup 文件系统。这时通过挂载选项指定使用哪个子系统。 CGroup 支持的文件种类 文件名 R/W 用途 Release_agent RW 删除分组时执行的命令，这个文件只存在于根分组 Notify_on_release RW 设置是否执行 release_agent。为 1 时执行 Tasks RW 属于分组的线程 TID 列表 Cgroup.procs R 属于分组的进程 PID 列表。仅包括多线程进程的线程 leader 的 TID，这点与 tasks 不同 Cgroup.event_control RW 监视状态变化和分组删除事件的配置文件 CGroup 相关概念解释 任务（task）：在 cgroups 中，任务就是系统的一个进程； 控制族群（control group）：控制族群就是一组按照某种标准划分的进程。Cgroups 中的资源控制都是以控制族群为单位实现。一个进程可以加入到某个控制族群，也从一个进程组迁移到另一个控制族群。一个进程组的进程可以使用 cgroups 以控制族群为单位分配的资源，同时受到 cgroups 以控制族群为单位设定的限制； 层级（hierarchy）：控制族群可以组织成 hierarchical 的形式，既一颗控制族群树。控制族群树上的子节点控制族群是父节点控制族群的孩子，继承父控制族群的特定的属性； 子系统（subsystem）：一个子系统就是一个资源控制器，比如 cpu 子系统就是控制 cpu 时间分配的一个控制器。子系统必须附加（attach）到一个层级上才能起作用，一个子系统附加到某个层级以后，这个层级上的所有控制族群都受到这个子系统的控制。 相互关系 每次在系统中创建新层级时，该系统中的所有任务都是那个层级的默认 cgroup（我们称之为 root cgroup，此 cgroup 在创建层级时自动创建，后面在该层级中创建的 cgroup 都是此 cgroup 的后代）的初始成员； 一个子系统最多只能附加到一个层级； 一个层级可以附加多个子系统； 一个任务可以是多个 cgroup 的成员，但是这些 cgroup 必须在不同的层级； 系统中的进程（任务）创建子进程（任务）时，该子任务自动成为其父进程所在 cgroup 的成员。然后可根据需要将该子任务移动到不同的 cgroup 中，但开始时它总是继承其父任务的 cgroup。 上图所示的 CGroup 层级关系显示，CPU 和 Memory 两个子系统有自己独立的层级系统，而又通过 Task Group 取得关联关系。 CGroup 特点 在 cgroups 中，任务就是系统的一个进程。 控制族群（control group）。控制族群就是一组按照某种标准划分的进程。Cgroups 中的资源控制都是以控制族群为单位实现。一个进程可以加入到某个控制族群，也从一个进程组迁移到另一个控制族群。一个进程组的进程可以使用 cgroups 以控制族群为单位分配的资源，同时受到 cgroups 以控制族群为单位设定的限制。 层级（hierarchy）。控制族群可以组织成 hierarchical 的形式，既一颗控制族群树。控制族群树上的子节点控制族群是父节点控制族群的孩子，继承父控制族群的特定的属性。 子系统（subsytem）。一个子系统就是一个资源控制器，比如 cpu 子系统就是控制 cpu 时间分配的一个控制器。子系统必须附加（attach）到一个层级上才能起作用，一个子系统附加到某个层级以后，这个层级上的所有控制族群都受到这个子系统的控制。 CGroup 应用架构 如上图所示 - CGroup 典型应用架构图，CGroup 技术可以 被用来在操作系统底层限制物理资源，起到 Container 的作用。图中每一个 JVM 进程对应一个 Container Cgroup 层级，通过 CGroup 提供的各类子系统，可以对每一个 JVM 进程对应的线程级别进行物理限制，这些限制包括 CPU、内存等等许多种类的资源。下一部分会具体对应用程序进行 CPU 资源隔离进行演示。 CGroup 部署及应用实例讲解 CGroup 设计原理前，我们先来做一个简单的实验。实验基于 Linux Centos v6.564 位版本，JDK 1.7。实验目的是运行一个占用 CPU 的 Java 程序，如果不用 CGroup 物理隔离 CPU 核，那程序会由操作系统层级自动挑选 CPU 核来运行程序。由于操作系统层面采用的是时间片轮询方式随机挑选 CPU 核作为运行容器，所以会在本机器上 24 个 CPU 核上随机执行。如果采用 CGroup 进行物理隔离，我们可以选择某些 CPU 核作为指定运行载体。 清单 1.Java 程序代码 123456789101112131415161718192021222324252627282930313233343536// 开启 4 个用户线程，其中 1 个线程大量占用 CPU 资源，其他 3 个线程则处于空闲状态public class HoldCPUMain &#123; public static class HoldCPUTask implements Runnable&#123; @Override public void run() &#123; // TODO Auto-generated method stub while(true)&#123; double a = Math.random()*Math.random();// 占用 CPU System.out.println(a); &#125; &#125; &#125; public static class LazyTask implements Runnable&#123; @Override public void run() &#123; // TODO Auto-generated method stub while(true)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;// 空闲线程 &#125; &#125; &#125; public static void main(String[] args)&#123; for(int i=0;i&lt;10;i++)&#123; new Thread(new HoldCPUTask()).start(); &#125; &#125;&#125; 清单 1 程序会启动 10 个线程，这 10 个线程都在做占用 CPU 的计算工作，它们可能会运行在 1 个 CPU 核上，也可能运行在多个核上，由操作系统决定。我们稍后会在 Linux 机器上通过命令在后台运行清单 1 程序。本实验需要对 CPU 资源进行限制，所以我们在 cpu_and_set 子系统上创建自己的层级 “zhoumingyao”。 清单 2. 创建层级 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@facenode4 cpu_and_set]# ls -rlt总用量 0-rw-r--r-- 1 root root 0 3 月 21 17:21 release_agent-rw-r--r-- 1 root root 0 3 月 21 17:21 notify_on_release-r--r--r-- 1 root root 0 3 月 21 17:21 cpu.stat-rw-r--r-- 1 root root 0 3 月 21 17:21 cpu.shares-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.sched_relax_domain_level-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.sched_load_balance-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.mems-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.memory_spread_slab-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.memory_spread_page-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.memory_pressure_enabled-r--r--r-- 1 root root 0 3 月 21 17:21 cpuset.memory_pressure-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.memory_migrate-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.mem_hardwall-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.mem_exclusive-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.cpus-rw-r--r-- 1 root root 0 3 月 21 17:21 cpuset.cpu_exclusive-rw-r--r-- 1 root root 0 3 月 21 17:21 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 3 月 21 17:21 cpu.rt_period_us-rw-r--r-- 1 root root 0 3 月 21 17:21 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 3 月 21 17:21 cpu.cfs_period_us-r--r--r-- 1 root root 0 3 月 21 17:21 cgroup.procsdrwxr-xr-x 2 root root 0 3 月 21 17:22 testdrwxr-xr-x 2 root root 0 3 月 23 16:36 test1-rw-r--r-- 1 root root 0 3 月 25 19:23 tasksdrwxr-xr-x 2 root root 0 3 月 31 19:32 singledrwxr-xr-x 2 root root 0 3 月 31 19:59 single1drwxr-xr-x 2 root root 0 3 月 31 19:59 single2drwxr-xr-x 2 root root 0 3 月 31 19:59 single3drwxr-xr-x 3 root root 0 4 月 3 17:34 aaaa[root@facenode4 cpu_and_set]# mkdir zhoumingyao[root@facenode4 cpu_and_set]# cd zhoumingyao[root@facenode4 zhoumingyao]# ls -rlt总用量 0-rw-r--r-- 1 root root 0 4 月 30 14:03 tasks-rw-r--r-- 1 root root 0 4 月 30 14:03 notify_on_release-r--r--r-- 1 root root 0 4 月 30 14:03 cpu.stat-rw-r--r-- 1 root root 0 4 月 30 14:03 cpu.shares-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.sched_relax_domain_level-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.sched_load_balance-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.mems-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.memory_spread_slab-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.memory_spread_page-r--r--r-- 1 root root 0 4 月 30 14:03 cpuset.memory_pressure-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.memory_migrate-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.mem_hardwall-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.mem_exclusive-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.cpus-rw-r--r-- 1 root root 0 4 月 30 14:03 cpuset.cpu_exclusive-rw-r--r-- 1 root root 0 4 月 30 14:03 cpu.rt_runtime_us-rw-r--r-- 1 root root 0 4 月 30 14:03 cpu.rt_period_us-rw-r--r-- 1 root root 0 4 月 30 14:03 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 4 月 30 14:03 cpu.cfs_period_us-r--r--r-- 1 root root 0 4 月 30 14:03 cgroup.procs 通过 mkdir 命令新建文件夹 zhoumingyao，由于已经预先加载 cpu_and_set 子系统成功，所以当文件夹创建完毕的同时，cpu_and_set 子系统对应的文件夹也会自动创建。 运行 Java 程序前，我们需要确认 cpu_and_set 子系统安装的目录，如清单 3 所示。 清单 3. 确认目录 123456789101112131415161718[root@facenode4 zhoumingyao]# lscgroupcpuacct:/devices:/freezer:/net_cls:/blkio:/memory:/memory:/test2cpuset,cpu:/cpuset,cpu:/zhoumingyaocpuset,cpu:/aaaacpuset,cpu:/aaaa/bbbbcpuset,cpu:/single3cpuset,cpu:/single2cpuset,cpu:/single1cpuset,cpu:/singlecpuset,cpu:/test1cpuset,cpu:/test 输出显示 cpuset_cpu 的目录是 cpuset,cpu:/zhoumingyao，由于本实验所采用的 Java 程序是多线程程序，所以需要使用 cgexec 命令来帮助启动，而不能如网络上有些材料所述，采用 java –jar 命令启动后，将 pid 进程号填入 tasks 文件即可的错误方式。清单 4 即采用 cgexec 命令启动 java 程序，需要使用到清单 3 定位到的 cpuset_cpu 目录地址。 清单 4. 运行 Java 程序 1[root@facenode4 zhoumingyao]# cgexec -g cpuset,cpu:/zhoumingyao java -jar test.jars 我们在 cpuset.cpus 文件中设置需要限制只有 0-10 这 11 个 CPU 核可以被用来运行上述清单 4 启动的 Java 多线程程序。当然 CGroup 还可以限制具体每个核的使用百分比，这里不再做过多的描述，请读者自行翻阅 CGroup 官方材料。 清单 5.cpu 核限制 12[root@facenode4 zhoumingyao]# cat cpuset.cpus0-10 接下来，通过 TOP 命令获得清单 4 启动的 Java 程序的所有相关线程 ID，将这些 ID 写入到 Tasks 文件。 清单 6. 设置线程 ID 123456789101112131415161718192021222324252627282930313233343536373839[root@facenode4 zhoumingyao]# cat tasks26562657265826592660266126622663266426652666266726682669267026712672267326742675267626772678267926802681268226832684268526862687268826892714271527162718 全部设置完毕后，我们可以通过 TOP 命令查看具体的每一颗 CPU 核上的运行情况，发现只有 0-10 这 11 颗 CPU 核上有计算资源被调用，可以进一步通过 TOP 命令确认全部都是清单 4 所启动的 Java 多线程程序的线程。 清单 7. 运行结果 12345678910111213141516171819202122232425262728top - 14:43:24 up 44 days, 59 min, 6 users, load average: 0.47, 0.40, 0.33Tasks: 715 total, 1 running, 714 sleeping, 0 stopped, 0 zombieCpu0 : 0.7%us, 0.3%sy, 0.0%ni, 99.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu1 : 1.0%us, 0.7%sy, 0.0%ni, 98.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu2 : 0.3%us, 0.3%sy, 0.0%ni, 99.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu3 : 1.0%us, 1.6%sy, 0.0%ni, 97.5%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu4 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu5 : 1.3%us, 1.9%sy, 0.0%ni, 96.8%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu6 : 3.8%us, 5.4%sy, 0.0%ni, 90.8%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu7 : 7.7%us, 9.9%sy, 0.0%ni, 82.4%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu8 : 4.8%us, 6.1%sy, 0.0%ni, 89.1%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu9 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu10 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu11 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu12 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu13 : 0.0%us, 0.0%sy, 0.0%ni, 72.8%id, 0.0%wa, 0.0%hi, 4.3%si, 0.0%stCpu14 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu15 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu16 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu17 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu18 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu19 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu20 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu21 : 0.3%us, 0.3%sy, 0.0%ni, 99.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu22 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu23 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 32829064k total, 5695012k used, 27134052k free, 533516k buffersSwap: 24777720k total, 0k used, 24777720k free, 3326940k cached 总体上来说，CGroup 的使用方式较为简单，目前主要的问题是网络上已有的中文材料缺少详细的配置步骤，一旦读者通过反复实验，掌握了配置方式，使用上应该不会有大的问题。 Cgroup 设计原理分析CGroups 的源代码较为清晰，我们可以从进程的角度出发来剖析 cgroups 相关数据结构之间的关系。在 Linux 中，管理进程的数据结构是 task_struct，其中与 cgroups 有关的代码如清单 8 所示： 清单 8.task_struct 代码 123456#ifdef CONFIG_CGROUPS/* Control Group info protected by css_set_lock */struct css_set *cgroups;/* cg_list protected by css_set_lock and tsk-&gt;alloc_lock */struct list_head cg_list;#endif 其中 cgroups 指针指向了一个 css_set 结构，而 css_set 存储了与进程有关的 cgroups 信息。cg_list 是一个嵌入的 list_head 结构，用于将连到同一个 css_set 的进程组织成一个链表。下面我们来看 css_set 的结构，代码如清单 9 所示： 清单 9.css_set 代码 12345678struct css_set &#123;atomic_t refcount;struct hlist_node hlist;struct list_head tasks;struct list_head cg_links;struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];struct rcu_head rcu_head;&#125;; 其中 refcount 是该 css_set 的引用数，因为一个 css_set 可以被多个进程公用，只要这些进程的 cgroups 信息相同，比如：在所有已创建的层级里面都在同一个 cgroup 里的进程。hlist 是嵌入的 hlist_node，用于把所有 css_set 组织成一个 hash 表，这样内核可以快速查找特定的 css_set。tasks 指向所有连到此 css_set 的进程连成的链表。cg_links 指向一个由 struct_cg_cgroup_link 连成的链表。 Subsys 是一个指针数组，存储一组指向 cgroup_subsys_state 的指针。一个 cgroup_subsys_state 就是进程与一个特定子系统相关的信息。通过这个指针数组，进程就可以获得相应的 cgroups 控制信息了。cgroup_subsys_state 结构如清单 10 所示： 清单 10.cgroup_subsys_state 代码 123456struct cgroup_subsys_state &#123;struct cgroup *cgroup;atomic_t refcnt;unsigned long flags;struct css_id *id;&#125;; cgroup 指针指向了一个 cgroup 结构，也就是进程属于的 cgroup。进程受到子系统的控制，实际上是通过加入到特定的 cgroup 实现的，因为 cgroup 在特定的层级上，而子系统又是附和到上面的。通过以上三个结构，进程就可以和 cgroup 连接起来了：task_struct-&gt;css_set-&gt;cgroup_subsys_state-&gt;cgroup。cgroup 结构如清单 11 所示： 清单 11.cgroup 代码 123456789101112131415161718struct cgroup &#123;unsigned long flags;atomic_t count;struct list_head sibling;struct list_head children;struct cgroup *parent;struct dentry *dentry;struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];struct cgroupfs_root *root;struct cgroup *top_cgroup;struct list_head css_sets;struct list_head release_list;struct list_head pidlists;struct mutex pidlist_mutex;struct rcu_head rcu_head;struct list_head event_list;spinlock_t event_list_lock;&#125;; sibling,children 和 parent 三个嵌入的 list_head 负责将统一层级的 cgroup 连接成一棵 cgroup 树。subsys 是一个指针数组，存储一组指向 cgroup_subsys_state 的指针。这组指针指向了此 cgroup 跟各个子系统相关的信息，这个跟 css_set 中的道理是一样的。 root 指向了一个 cgroupfs_root 的结构，就是 cgroup 所在的层级对应的结构体。这样一来，之前谈到的几个 cgroups 概念就全部联系起来了。 top_cgroup 指向了所在层级的根 cgroup，也就是创建层级时自动创建的那个 cgroup。 css_set 指向一个由 struct_cg_cgroup_link 连成的链表，跟 css_set 中 cg_links 一样。 下面分析一个 css_set 和 cgroup 之间的关系，cg_cgroup_link 的结构如清单 12 所示： 清单 12.cg_cgroup_link 代码 12345struct cg_cgroup_link &#123;struct list_head cgrp_link_list;struct cgroup *cgrp;struct list_head cg_link_list;struct css_set *cg; &#125;; cgrp_link_list 连入到 cgrouo-&gt;css_set 指向的链表，cgrp 则指向此 cg_cgroup_link 相关的 cgroup。 cg_link_list 则连入到 css_set-&gt;cg_lonks 指向的链表，cg 则指向此 cg_cgroup_link 相关的 css_set。 cgroup 和 css_set 是一个多对多的关系，必须添加一个中间结构来将两者联系起来，这就是 cg_cgroup_link 的作用。cg_cgroup_link 中的 cgrp 和 cg 就是此结构提的联合主键，而 cgrp_link_list 和 cg_link_list 分别连入到 cgroup 和 css_set 相应的链表，使得能从 cgroup 或 css_set 都可以进行遍历查询。 那为什么 cgroup 和 css_set 是多对多的关系呢？ 一个进程对应一个 css_set，一个 css_set 存储了一组进程 (有可能被多个进程共享，所以是一组) 跟各个子系统相关的信息，但是这些信息由可能不是从一个 cgroup 那里获得的，因为一个进程可以同时属于几个 cgroup，只要这些 cgroup 不在同一个层级。举个例子：我们创建一个层级 A，A 上面附加了 cpu 和 memory 两个子系统，进程 B 属于 A 的根 cgroup；然后我们再创建一个层级 C，C 上面附加了 ns 和 blkio 两个子系统，进程 B 同样属于 C 的根 cgroup；那么进程 B 对应的 cpu 和 memory 的信息是从 A 的根 cgroup 获得的，ns 和 blkio 信息则是从 C 的根 cgroup 获得的。因此，一个 css_set 存储的 cgroup_subsys_state 可以对应多个 cgroup。另一方面，cgroup 也存储了一组 cgroup_subsys_state，这一组 cgroup_subsys_state 则是 cgroup 从所在的层级附加的子系统获得的。一个 cgroup 中可以有多个进程，而这些进程的 css_set 不一定都相同，因为有些进程可能还加入了其他 cgroup。但是同一个 cgroup 中的进程与该 cgroup 关联的 cgroup_subsys_state 都受到该 cgroup 的管理 (cgroups 中进程控制是以 cgroup 为单位的) 的，所以一个 cgroup 也可以对应多个 css_set。 从前面的分析，我们可以看出从 task 到 cgroup 是很容易定位的，但是从 cgroup 获取此 cgroup 的所有的 task 就必须通过这个结构了。每个进程都回指向一个 css_set，而与这个 css_set 关联的所有进程都会链入到 css_set-&gt;tasks 链表，而 cgroup 又通过一个中间结构 cg_cgroup_link 来寻找所有与之关联的所有 css_set，从而可以得到与 cgroup 关联的所有进程。最后，我们看一下层级和子系统对应的结构体。层级对应的结构体是 cgroupfs_root 如清单 13 所示： 清单 13.cgroupfs_root 代码 12345678910111213struct cgroupfs_root &#123;struct super_block *sb;unsigned long subsys_bits;int hierarchy_id;unsigned long actual_subsys_bits;struct list_head subsys_list;struct cgroup top_cgroup;int number_of_cgroups;struct list_head root_list;unsigned long flags;char release_agent_path[PATH_MAX];char name[MAX_CGROUP_ROOT_NAMELEN];&#125;; sb 指向该层级关联的文件系统数据块。subsys_bits 和 actual_subsys_bits 分别指向将要附加到层级的子系统和现在实际附加到层级的子系统，在子系统附加到层级时使用。hierarchy_id 是该层级唯一的 id。top_cgroup 指向该层级的根 cgroup。number_of_cgroups 记录该层级 cgroup 的个数。root_list 是一个嵌入的 list_head，用于将系统所有的层级连成链表。子系统对应的结构体是 cgroup_subsys，代码如清单 14 所示。 清单 14. cgroup_subsys 代码 12345678910111213141516171819202122232425262728293031struct cgroup_subsys &#123;struct cgroup_subsys_state *(*create)(struct cgroup_subsys *ss,struct cgroup *cgrp);int (*pre_destroy)(struct cgroup_subsys *ss, struct cgroup *cgrp);void (*destroy)(struct cgroup_subsys *ss, struct cgroup *cgrp);int (*can_attach)(struct cgroup_subsys *ss, struct cgroup *cgrp, struct task_struct *tsk, bool threadgroup);void (*cancel_attach)(struct cgroup_subsys *ss,struct cgroup *cgrp, struct task_struct *tsk, bool threadgroup);void (*attach)(struct cgroup_subsys *ss, struct cgroup *cgrp,struct cgroup *old_cgrp, struct task_struct *tsk, bool threadgroup);void (*fork)(struct cgroup_subsys *ss, struct task_struct *task);void (*exit)(struct cgroup_subsys *ss, struct task_struct *task);int (*populate)(struct cgroup_subsys *ss, struct cgroup *cgrp);void (*post_clone)(struct cgroup_subsys *ss, struct cgroup *cgrp);void (*bind)(struct cgroup_subsys *ss, struct cgroup *root);int subsys_id;int active;int disabled;int early_init;bool use_id;#define MAX_CGROUP_TYPE_NAMELEN 32const char *name;struct mutex hierarchy_mutex;struct lock_class_key subsys_key;struct cgroupfs_root *root;struct list_head sibling;struct idr idr;spinlock_t id_lock;struct module *module;&#125;; cgroup_subsys 定义了一组操作，让各个子系统根据各自的需要去实现。这个相当于 C++ 中抽象基类，然后各个特定的子系统对应 cgroup_subsys 则是实现了相应操作的子类。类似的思想还被用在了 cgroup_subsys_state 中，cgroup_subsys_state 并未定义控制信息，而只是定义了各个子系统都需要的共同信息，比如该 cgroup_subsys_state 从属的 cgroup。然后各个子系统再根据各自的需要去定义自己的进程控制信息结构体，最后在各自的结构体中将 cgroup_subsys_state 包含进去，这样通过 Linux 内核的 container_of 等宏就可以通过 cgroup_subsys_state 来获取相应的结构体。 从基本层次顺序定义上来看，由 task_struct、css_set、cgroup_subsys_state、cgroup、cg_cgroup_link、cgroupfs_root、cgroup_subsys 等结构体组成的 CGroup 可以基本从进程级别反应之间的响应关系。后续文章会针对文件系统、各子系统做进一步的分析。 结束语就象大多数开源技术一样，CGroup 不是全新创造的，它将进程管理从 cpuset 中剥离出来。通过物理限制的方式为进程间资源控制提供了简单的实现方式，为 Linux Container 技术、虚拟化技术的发展奠定了技术基础，本文的目标是让初学者可以通过自己动手的方式简单地理解技术，将起步门槛放低。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之定时任务]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-cron%2F</url>
      <content type="text"><![CDATA[在计算机的使用过程中，经常会有一些计划中的任务需要在将来的某个时间执行，linux 中提供了一些方法来设定定时任务。 1、at命令 at 从文件或标准输入中读取命令并在将来的一个时间执行，只执行一次。at 的正常执行需要有守护进程 atd(关于 systemctl 请看这一篇)： 12345678#安装 atyum install -y at 或 apt-get install at -y#启动守护进程service atd start 或 systemctl start atd#查看是否开机启动chkconfig --list|grep atd 或 systemctl list-unit-files|grep atd#设置开机启动chkconfig --level 235 atd on 或 systemctl enable atd 如果不使用管道 | 或指定选项 -f 的话，at 的执行将会是交互式的，需要在 at 的提示符下输入命令： 12345[root@centos7 temp]# at now +2 minutes #执行 at 并指定执行时刻为现在时间的后两分钟at&gt; echo hello world &gt; /root/temp/file #手动输入命令并回车at&gt; &lt;EOT&gt; #ctrl+d 结束输入job 9 at Thu Dec 22 14:05:00 2016 #显示任务号及执行时间[root@centos7 temp]# 选项 -l 或命令 atq 查询任务 12[root@centos7 temp]# atq9 Thu Dec 22 14:05:00 2016 a root 到达时间后任务被执行，生成一个新文件 file 并保存 echo 的输出内容 12345[root@centos7 temp]# ls -l file-rw-r--r-- 1 root root 12 12 月 22 14:05 file[root@centos7 temp]# cat filehello world[root@centos7 temp]# at 指定时间的方法很丰富，可以是 hh:mm 小时: 分钟 (当天，如果时间已过，则在第二天执行) midnight(深夜),noon(中午),teatime(下午茶时间，下午 4 点),today,tomorrow 等 12 小时计时制，时间后加 am(上午) 或 pm(下午) 指定具体执行日期 mm/dd/yy(月 /日 /年) 或 dd.mm.yy(日. 月. 年) 相对计时法 now + n units，now 是现在时刻，n 为数字，units 是单位 (minutes、hours、days、weeks) 如明天下午 2 点 20 分执行创建一个目录 1234[root@centos7 temp]# at 02:20pm tomorrowat&gt; mkdir /root/temp/Xat&gt; &lt;EOT&gt;job 11 at Fri Dec 23 14:20:00 2016 选项 -d 或命令 atrm 表示删除任务 123[root@centos7 temp]# at -d 11 #删除 11 号任务 (上例)[root@centos7 temp]# atq[root@centos7 temp]# 可以使用管道 | 或选项 -f 让 at 从标准输入或文件中获得任务 123456[root@centos7 temp]# cat test.txtecho hello world &gt; /root/temp/file[root@centos7 temp]# at -f test.txt 5pm +2 daysjob 12 at Sat Dec 24 17:00:00 2016[root@centos7 temp]# cat test.txt|at 16:20 12/23/16job 13 at Fri Dec 23 16:20:00 2016 atd 通过两个文件 /etc/at.allow 和 /etc/at.deny 来决定系统中哪些用户可以使用 at 设置定时任务，它首先检查 /etc/at.allow，如果文件存在，则只有文件中列出的用户 (每行一个用户名)，才能使用 at；如果不存在，则检查文件 /etc/at.deny，不在此文件中的所有用户都可以使用 at。如果 /etc/at.deny 是空文件，则表示系统中所有用户都可以使用 at；如果 /etc/at.deny 文件也不存在，则只有超级用户(root) 才能使用 at。 2、crontab命令 crontab 用来设置、移除、列出服务 crond 表格，crond 服务的作用类似 atd，区别的地方在于 crond 可以设置任务多次执行。相对来说比 atd 更常用。 同样需要启动服务 crond 12[root@centos7 temp]# ps -ef|grep [c]rondroot 733 1 0 12 月 20 ? 00:00:00 /usr/sbin/crond -n 系统中每个用户都可以拥有自己的 cron table，同 atd 类似，crond 也有两个文件 /etc/cron.allow 和 /etc/cron.deny 用来限制用户使用 cron，规则也和 atd 的两个文件相同。 选项 -l 表示列出当前用户的 cron 表项选项 -u 表示指定用户 123[root@centos7 ~]# crontab -l -u learnerno crontab for learner[root@centos7 ~]# 选项 -e 表示编辑用户的 cron table。编辑时系统会选定默认编辑器，在笔者的环境中是 vi 通过直接编辑文件 /etc/crontab 可以设置系统级别的 cron table。 使用 crontab -e 的方式编辑时，会在 /tmp 下面生成一个临时文件，保存后 crond 会将内容写入到 /var/spool/cron 下面一个和用户名同名的文件中，crond 会在保存时做语法检查。这也是推荐的设置定时任务的用法。 语法： 1* * * * * command 每一行表示一个任务，以符号 # 开头的行表示注释，不生效。每个生效行都形如上面所示，一行被分为 6 部分，其中： 第一部分表示分钟 (0-59)，* 表示每分钟 第二部分表示小时 (0-23)，* 表示每小时 第三部分表示日 (1-31)， * 表示每天 第四部分表示月 (1-12)， * 表示每月 第五部分表示周几 (0-6,0 表示周日)，* 表示一周中每天 第六部分表示要执行的任务 关于时间设置的前五部分中，除了 * 表示当前部分的任意时间外，还支持另外三个符号 /、,、- 分别表示每隔、时间点 A 和时间点 B、时间点 A 到时间点 B。 如每隔 3 分钟测试 10.0.1.252 的连通性，并将结果追加输出到 /root/252.log 中 12[root@centos7 ~]# crontab -e*/3 * * * * /usr/bin/ping -c1 10.0.1.252 &amp;&gt;&gt; /root/252.log 保存后会有 crontab: installing new crontab 字样出现。注意六个部分都不能为空，命令最好写绝对路径，编辑普通用户的定时任务时，要注意命令的执行权限。 如一月份到五月份，每周 2 和周 5 凌晨 2:30 执行备份任务 130 2 * 1-5 2,5 /bin/bash /root/temp/backup.sh 这里将备份任务写入到脚本 /root/temp/backup.sh 中执行 如 3-6 月和 9-12 月，每周一到周五 12 点到 14 点，每 2 分钟执行一次刷新任务 1*/2 12-14 * 3-6,9-12 1-5 /bin/bash /root/temp/refresh.sh 混合使用日期时间及特殊符号，可以组合出大多数想要的时间。 查看定时任务 1234[root@centos7 ~]# crontab -l*/3 * * * * /usr/bin/ping -c1 10.0.1.252 &amp;&gt;&gt; /root/252.log30 2 * 1-5 2,5 /bin/bash /root/temp/backup.sh*/2 12-14 * 3-6,9-12 1-5 /bin/bash /root/temp/refresh.sh 选项 -r 表示删除定时任务 123[root@centos7 ~]# crontab -r[root@centos7 ~]# crontab -lno crontab for root 使用 crontab 时经常会遇到的一个问题是，在命令行下能够正常执行的命令或脚本，设置了定时任务时却不能正常执行。造成这种情况的原因一般是因为 crond 为命令或脚本设置了与登录 shell 不同的环境变量 12345678[root@centos7 ~]# head -3 /etc/crontabSHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root[root@centos7 ~]#[root@centos7 ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@centos7 ~]# 这里 crond 的 PATH 和 shell 中的值不同，PATH 环境变量定义了 shell 执行命令时搜索命令的路径。关于环境变量更多的内容，将在 shell 编程的文章里详细说明。 对于系统级别的定时任务，这些任务更加重要，大部分 linux 系统在 /etc 中包含了一系列与 cron 有关的子目录：/etc/cron.{hourly,daily,weekly,monthly}，目录中的文件定义了每小时、每天、每周、每月需要运行的脚本，运行这些任务的精确时间在文件 /etc/crontab 中指定。如： 12345678910SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/# run-parts01 * * * * root run-parts /etc/cron.hourly02 4 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly 对于 24 小时开机的服务器来说，这些任务的定期运行，保证了服务器的稳定性。但注意到这些任务的执行一般都在凌晨，对于经常需要关机的 linux 计算机 (如笔记本) 来说，很可能在需要运行 cron 的时候处于关机状态，cron 得不到运行，时间长了会导致系统变慢。对于这样的系统，linux 引入了另一个工具 anacron 来负责执行系统定时任务。 anacron 的目的并不是完全替代 cron，是作为 cron 的一个补充。anacron 的任务定义在文件 /etc/anacrontab 中： 12345678910111213141516# /etc/anacrontab: configuration file for anacron# See anacron(8) and anacrontab(5) for details.SHELL=/bin/shPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# the maximal random delay added to the base delay of the jobsRANDOM_DELAY=45# the jobs will be started during the following hours onlySTART_HOURS_RANGE=3-22#period in days delay in minutes job-identifier command1 5 cron.daily nice run-parts /etc/cron.daily7 25 cron.weekly nice run-parts /etc/cron.weekly@monthly 45 cron.monthly nice run-parts /etc/cron.monthly 与 cron 是作为守护进程运行的不同，anacron 是作为普通进程运行并终止的。对于定义的每个任务，anacron 在系统启动后将会检查应当运行的任务，判断上一次运行到现在的时间是否超过了预定天数 (/etc/anacrontab 中任务行第一列)，如果大于预定天数，则会延迟一个时间(/etc/anacrontab 中任务行第二列) 之后运行该任务。这样就保证了任务的执行。关于 anacron 的更多内容，请查阅相关文档。 3、systemd.timercrond 和 atd 服务基于分钟的，意思是说它们每分钟醒来一次检查是否有任务需要执行。如果有任务的执行需要精确到秒，crond 和 atd 是无能为力的。在基于 systemd 的系统上，可以通过计时器 systemd.timer 来实现精确到秒的计划任务。 上一篇文章中我们提到了 systemd 中服务单元的概念，在这里我们需要用到其中的两种：.service 和. timer。其中. service 负责配置需要运行的任务，.timer 负责配置执行时间。 我们先看一个例子： 创建任务脚本 123[root@centos7 temp]# cat /root/temp/ping252.sh#!/bin/bashping -c1 10.0.1.252 &amp;&gt;&gt; /root/temp/252.log 配置服务. service 123456789[root@centos7 temp]# cd /usr/lib/systemd/system[root@centos7 system]# cat ping252.service[Unit]Description=ping 252[Service]Type=simpleExecStart=/root/temp/ping252.sh[root@centos7 system]# 配置计时器. timer 123456789101112131415[root@centos7 temp]# cd /usr/lib/systemd/system[root@centos7 system]# cat ping252.timer[Unit]Description=ping 252 every 30s[Timer]# Time to wait after enable this unitOnActiveSec=60# Time between running each consecutive timeOnUnitActiveSec=30Unit=ping252.service[Install]WantedBy=multi-user.target[root@centos7 system]# 启用计时器 123[root@centos7 system]# systemctl enable ping252.timerCreated symlink from /etc/systemd/system/multi-user.target.wants/ping252.timer to /usr/lib/systemd/system/ping252.timer.[root@centos7 system]# systemctl start ping252.timer 查看 12345678910111213141516171819#计时器[root@centos7 system]# systemctl status ping252.timer● ping252.timer - ping 252 every 30s Loaded: loaded (/usr/lib/systemd/system/ping252.timer; enabled; vendor preset: disabled) Active: active (waiting) since 五 2016-12-23 14:27:26 CST; 3min 42s ago12 月 23 14:27:26 centos7 systemd[1]: Started ping 252 every 30s.12 月 23 14:27:26 centos7 systemd[1]: Starting ping 252 every 30s.#服务[root@centos7 system]# systemctl status ping252● ping252.service - ping 252 Loaded: loaded (/usr/lib/systemd/system/ping252.service; static; vendor preset: disabled) Active: active (running) since 五 2016-12-23 14:35:38 CST; 2ms ago Main PID: 11494 (ping252.sh) CGroup: /system.slice/ping252.service └─11494 /bin/bash /root/temp/ping252.sh12 月 23 14:35:38 centos7 systemd[1]: Started ping 252.12 月 23 14:35:38 centos7 systemd[1]: Starting ping 252... 停用 1234[root@centos7 system]# systemctl disable ping252.timerRemoved symlink /etc/systemd/system/multi-user.target.wants/ping252.timer.[root@centos7 system]# systemctl stop ping252.timer[root@centos7 system]# 计时器启用 1 分钟之后看到 /root/temp/252.log 文件的生成，之后每隔 30 秒都有内容写入。systemd 的服务单元配置文件中被不同的标签分隔成不同的配置区块，其中： [Unit] 标签下指定了不依赖于特定类型的通用配置信息，比如例子中两个文件都指定了一个选项 Description = 表示描述信息。 [Install] 标签下保存了本单元的安装信息，其中 WantedBy = 表示当使用 systemctl enable 命令启用该单元时，会在指定的目标的. wants / 或. requires / 下创建对应的符号链接 (如上例)。这么做的结果是：当指定的目标启动时本单元也会被启动。 除了这两个所有配置文件都可以设置的标签外 (其余选项可以通过命令 man 5 systemd.unit 查看)，每个服务单元还有一个特定单元类型的标签，比如我们例子中. service 文件中的[Service] 和. timer 文件中的[Timer]。 [Service] 标签下 Type = 后的值指明了执行方式，设置为 simple 并配合 ExecStart = 表明指定的程序 (我们例子中的脚本) 将不会 fork()而启动；如果设置为 oneshot 表明只执行一次(类似 at)，如果需要让 systemd 在服务进程退出之后仍然认为该服务处于激活状态，则还需要设置 RemainAfterExit=yes。其余选项请用命令 man 5 systemd.service 查看 [Timer] 标签中可以指定多种单调定时器，所谓 “单调时间” 的意思是从开机那一刻 (零点) 起， 只要系统正在运行，该时间就不断的单调均匀递增(但在系统休眠时此时间保持不变)，永远不会往后退，并且与时区也没有关系。 即使在系统运行的过程中，用户向前 / 向后修改系统时间，也不会对 “单调时间” 产生任何影响。包括： 12345OnActiveSec= 表示相对于本单元被启用的时间点OnBootSec= 表示相对于机器被启动的时间点OnStartupSec= 表示相对于 systemd 被首次启动的时间点OnUnitActiveSec= 表示相对于匹配单元 (本标签下 Unit = 指定的单元) 最后一次被启动的时间点OnUnitInactiveSec= 表示相对于匹配单元 (本标签下 Unit = 指定的单元) 最后一次被停止的时间点 我们的例子中使用了其中的两个 OnActiveSec=60 和 OnUnitActiveSec=30 指定本单元在启用之后 60 秒调用 Unit = 后的单元，并在此单元被启用后每隔 30 秒再次启用它，达到了定时周期性的执行的目的。 这些定时器后指定的时间单位可以是：us(微秒), ms(毫秒), s(秒), m(分), h(时), d(天), w(周), month(月), y(年)。如果省略了单位，则表示使用默认单位‘秒’。可以写成 5h 30min 表示之后的 5 小时 30 分钟。 [Timer] 标签下还可以设置基于挂钟时间 (wall clock) 的日历定时器 OnCalendar=，所谓 “挂钟时间” 是指真实世界中墙上挂钟的时间， 在操作系统中实际上就是系统时间，这个时间是操作系统在启动时从主板的时钟芯片中读取的。由于这个时间是可以手动修改的，所以，这个时间既不一定是单调递增的、也不一定是均匀递增的。其时间格式可以是： 1234567Thu,Fri 2012-*-1,5 11:12:13 #表示 2012 年任意月份的 1 日和 5 日，如果是星期四或星期五，则在时间 11:12:13 执行*-*-* *:*:00 #表示每分钟*-*-* 00:00:00 #表示每天*-01,07-01 00:00:00 #表示每半年*:0/15 #表示每 15 分钟12,14,13:20,10,30 #表示 12/13/14 点的 10 分、20 分、30 分Mon,Fri *-01/2-01,03 *:30:45 #表示任意年份奇数月份的 1 日和 3 日，如果是周一或周五，则在每小时的 30 分 45 秒执行 单调定时器和日历定时器的其他内容可以通过命令 man 7 systemd.time 查询 Unit= 后指明了与此计时器相关联的服务单元 (我们例子中的 ping252.service)。服务单元中的大部分设置选项允许指定多次，不相冲突的情况下将均生效，如. timer 中可以设置多个 Unit 表示这些服务单元共用一个计时器。 另外 [Timer] 标签下还可以设置选项 Persistent=，它只对 OnCalendar = 指令定义的日历定时器有意义。如果设为 yes(默认值为 no)，则表示将匹配单元的上次触发时间永久保存在磁盘上。 这样，当定时器单元再次被启动时， 如果匹配单元本应该在定时器单元停止期间至少被启动一次， 那么将立即启动匹配单元。 这样就不会因为关机而错过必须执行的任务。(类似于 anacron 的功能)关于定时器的更多选项可以通过 man systemd.timer 查看 使用 systemd.timer 设置定时任务可以代替 atd 和 crond 的所有功能，另外 systemd 还接管了许多其他服务，这些内容超出了本篇的范围，在以后的文章中如果涉及到相关的内容，会有相应的介绍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之磁盘与文件系统]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-disk-and-fs%2F</url>
      <content type="text"><![CDATA[本篇讲述磁盘管理相关的命令。计算机中需要持久化存储的数据一般是保存在硬盘等辅助存储器中。硬盘一般容量较大，为了便于管理和使用，可以将硬盘分成一到多个逻辑磁盘，称为分区；为使分区中的文件组织成操作系统能够处理的形式，需要对分区进行格式化 (创建文件系统)；在 linux 中，对于格式化后的分区，还必须经过挂载(可简单理解为将分区关联至 linux 目录树中某个已知目录) 之后才能使用。 1、df 显示文件系统磁盘空间使用量12345678910[root@centos7 temp]# df -h文件系统 容量 已用 可用 已用 % 挂载点/dev/mapper/centos-root 49G 18G 31G 36% /devtmpfs 3.9G 0 3.9G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 367M 3.5G 10% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 497M 125M 373M 26% /boot/dev/mapper/centos-home 24G 4.0G 20G 17% /hometmpfs 783M 0 783M 0% /run/user/0 选项 -h 作用是转换数字的显示单位 (默认为 KB)。显示信息文件系统列下面带 tmpfs 字样的是虚拟内存文件系统 (此处不做展开)。文件系统 /dev/mapper/centos-root 的挂载点是 /(根目录)，即通常所说的根分区 (或根文件系统)；/dev/sda1(boot 分区) 中保存了内核映像和一些启动时需要的辅助文件；另外，还对用户家目录单独做了分区(/dev/mapper/centos-home)。在 linux 中还可以做一个特殊的分区：swap 分区 (交换分区)。作用是：当系统的物理内存不够用时，会将物理内存中一部分暂时不使用的数据交换至 swap 分区中，当需要使用这些数据时，再从 swap 空间交换回内存空间。swap 在功能上突破了物理内存的限制，使程序可以操纵大于实际物理内存的空间。但由于硬盘的速度远远低于内存，使 swap 只能作为物理内存的辅助。通常 swap 空间的大小是实际物理内存大小的 1 到 2 倍。使用命令 free 可以查看 swap 空间的大小。 选项 -i 显示 inode 信息 12345678910[root@centos7 tmp]# df -i文件系统 Inode 已用 (I) 可用 (I) 已用 (I)% 挂载点/dev/mapper/centos-root 50425856 78822 50347034 1% /devtmpfs 998721 391 998330 1% /devtmpfs 1001340 1 1001339 1% /dev/shmtmpfs 1001340 490 1000850 1% /runtmpfs 1001340 13 1001327 1% /sys/fs/cgroup/dev/sda1 512000 330 511670 1% /boot/dev/mapper/centos-home 24621056 190391 24430665 1% /hometmpfs 1001340 1 1001339 1% /run/user/0 这里显示的数字是该文件系统中 inode 数量的使用情况。 2、fdisk 磁盘分区工具1fdisk [options] [device...] 选项 -l 表示列出分区表 12345678910111213[root@centos7 tmp]# fdisk -l /dev/sda磁盘 /dev/sda：85.9 GB, 85899345920 字节，167772160 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小 (逻辑 / 物理)：512 字节 / 512 字节I/O 大小 (最小 / 最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0x0001abbc 设备 Boot Start End Blocks Id System/dev/sda1 * 2048 1026047 512000 83 Linux/dev/sda2 1026048 167772159 83373056 8e Linux LVM[root@centos7 tmp]# 当前机械硬盘中包含一到多个固定在主轴 (spindle) 上的盘片(platter)，盘片由硬质磁性合金材料构成。每张盘片有上下两个表面，每个表面都包含数量巨大的扇区(sector)，扇区是大小为 512 byte 的区块，这些区块均匀的分布于盘片的同心圆上，这些同心圆被称为磁道(track)。上千个磁道的宽度相当于人类头发的直径。 硬盘中使用固定于磁臂 (disk arm) 顶端的磁头 (disk head 上下两面均有) 读写盘面中的数据。硬盘不工作时，磁头停留在启停区(盘片上靠近主轴的区域)；启停区外是数据区，盘片最外围磁道称为 0 磁道；硬盘启动后，盘片会围绕主轴高速旋转，盘片旋转产生的气流相当强，足以使磁头托起，并与盘面保持一个微小的距离(大概相当于人类头发直径的千分之一)。磁臂摆动，可以将磁头移动至任意磁道上方。 单一磁道示意图： 当前硬盘转速大概在 7200 转 / 分钟到 15000 转 / 分钟左右。假设硬盘转速是 10000 转 / 分钟，则意味着，转一圈需要的时间是 6ms。 所有盘面上的同一磁道构成一个圆柱，通常称做柱面 (Cylinder)，系统将数据存储到磁盘上时，按柱面、磁头、扇区的方式进行，即最上方 0 磁头最外围 0 磁道第一个扇区开始写入，写满一个磁道之后，接着在同一柱面的下一个磁头继续写入。同一个柱面都写满之后才推进到内层的下一个柱面。 fdisk 命令中 device 通常是 /dev/hda、/dev/hdb….(IDE 接口类型的硬盘设备名) 或 /dev/sda、/dev/sdb….(SCSI 接口类型硬盘设备名)，表示整个硬盘，如果硬盘被分区，则在设备名后追加一个数字表示此设备的第几个分区。如上例中的 /dev/sda1 和 /dev/sda2 硬盘磁头存取数据是以扇区 (512bytes) 为单位的 (上例中 Start 和 End 列)，但操作系统存取数据是以块(Block) 为单位的 (注意：这里说的 Block 的大小不同于 fdisk 命令输出中的 Blocks 列，fdisk 命令输出中 Blocks 列的大小为 1024 bytes)；扇区是硬件级别的，Block 是文件系统级别的，也就是说在创建文件系统(格式化) 的时候才决定一个 block 的大小、数量。一个块的大小是一个扇区大小 2 的 n 次方倍，本例文件系统 Block 的默认大小为 4096 bytes(格式化时可以指定为其他值)。 我们在 252 这台机器上新添加三块硬盘 (每块 200GB) 123456789[root@idc-v-71252 ~]# ls -l /dev/sd[a-d]*brw-rw---- 1 root disk 8, 0 12 月 13 09:49 /dev/sdabrw-rw---- 1 root disk 8, 1 12 月 13 09:49 /dev/sda1brw-rw---- 1 root disk 8, 2 12 月 13 09:49 /dev/sda2brw-rw---- 1 root disk 8, 16 12 月 13 09:49 /dev/sdbbrw-rw---- 1 root disk 8, 32 12 月 13 09:49 /dev/sdcbrw-rw---- 1 root disk 8, 48 12 月 13 09:49 /dev/sdd#这里看到除了原有被分过区的 sda 外，多出了设备 sdb、sdc、sdd#这里的第五列由逗号分隔的两个数字组成，它们是内核用来识别具体设备的标识号。 下面使用 fdisk 命令对新磁盘进行分区 12345678910[root@idc-v-71252 ~]# fdisk /dev/sdb欢迎使用 fdisk (util-linux 2.23.2)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。Device does not contain a recognized partition table使用磁盘标识符 0xc41dfd92 创建新的 DOS 磁盘标签。命令 (输入 m 获取帮助)： 在提示符后输入 m 获取帮助信息（列出了在提示符后可使用的命令及其解释） 12345678910111213141516171819202122命令 (输入 m 获取帮助)：m命令操作 a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition g create a new empty GPT partition table G create an IRIX (SGI) partition table l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only)命令 (输入 m 获取帮助)： 命令 n 表示创建一个新分区 12345命令 (输入 m 获取帮助)：nPartition type: p primary (0 primary, 0 extended, 4 free) e extendedSelect (default p): 此处可选项有两个，p 表示主分区 (primary)，e 表示扩展分区 (extended)，默认为主分区。 每块硬盘分区后，位于 0 磁头 0 柱面 1 扇区的是一个特殊区域，称为 MBR(Main Boot Record 主引导记录区)，其中前 446 字节是 Bootloader(引导加载程序)，之后的 64 字节是 DPT(Disk Partition Table 硬盘分区表)，最后两个字节的 Magic Number(硬盘有效标志)。 DPT 中记录了此块硬盘有哪些分区，由于其大小的限制，使得分区表只能包含 4 条记录，可以是一到四个主分区或一个扩展分区和一到三个主分区。其中扩展分区可以再分区，称为逻辑分区。 我们选择默认的主分区： 123456789Select (default p):Using default response p分区号 (1-4，默认 1)：起始 扇区 (2048-419430399，默认为 2048)：将使用默认值 2048Last 扇区, + 扇区 or +size&#123;K,M,G&#125; (2048-419430399，默认为 419430399)：+100G分区 1 已设置为 Linux 类型，大小设为 100 GiB命令 (输入 m 获取帮助)： 每一步骤都有相应提示，可以被使用的扇区从 2048 号开始 (前面的扇区包括 MBR 用做其他用途)，分区结束扇区的指定可以是扇区号，也可以是 + size 这样的格式。这里我们指定分区大小为 100G 使用 p 命令打印分区信息： 12345678910111213命令 (输入 m 获取帮助)：p磁盘 /dev/sdb：214.7 GB, 214748364800 字节，419430400 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小 (逻辑 / 物理)：512 字节 / 512 字节I/O 大小 (最小 / 最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xc41dfd92 设备 Boot Start End Blocks Id System/dev/sdb1 2048 209717247 104857600 83 Linux命令 (输入 m 获取帮助)： 注意这里的显示的不同，Boot 列如果有 * 标志，表示此分区为 boot 分区。Id 列表示分区类型，可以使用命令 l 列出所有支持的类型，其中 82 表示 linux swap，83 表示 linux 默认分区类型，8e 表示 linux lvm(后述)。 然后我们将信息保存： 123456命令 (输入 m 获取帮助)：wThe partition table has been altered!Calling ioctl() to re-read partition table.正在同步磁盘。[root@idc-v-71252 ~]# 3、mkfs 创建文件系统选项 -t 可以指定文件系统类型 (包括 ext3 ext4 btrfs xfs reiserfs 等) 123456789101112131415161718192021222324[root@idc-v-71252 ~]# mkfs -t ext4 /dev/sdb1 #或者直接执行 mkfs.ext4 /dev/sdb1mke2fs 1.42.9 (28-Dec-2013)文件系统标签 =OS type: Linux块大小 = 4096 (log=2)分块大小 = 4096 (log=2)Stride=0 blocks, Stripe width=0 blocks6553600 inodes, 26214400 blocks1310720 blocks (5.00%) reserved for the super user第一个数据块 = 0Maximum filesystem blocks=2174746624800 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872Allocating group tables: 完成 正在写入 inode 表: 完成 Creating journal (32768 blocks): 完成Writing superblocks and filesystem accounting information: 完成 [root@idc-v-71252 ~]# 这样，我们就把刚刚分的区格式化成了 ext4 文件系统，输出信息中显示了 inode 和 block 数量等信息。 4、mount 挂载文件系统将格式化好的文件系统挂载至 /root/temp/tmp 12345678910111213[root@idc-v-71252 tmp]# mount /dev/sdb1 /root/temp/tmp[root@idc-v-71252 tmp]# df -h文件系统 容量 已用 可用 已用 % 挂载点/dev/mapper/centos-root 49G 14G 35G 28% /devtmpfs 3.9G 0 3.9G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 8.5M 3.9G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 497M 170M 328M 35% /boot/dev/mapper/centos-home 24G 16G 7.6G 68% /hometmpfs 799M 0 799M 0% /run/user/0/dev/sdb1 99G 61M 94G 1% /root/temp/tmp[root@idc-v-71252 tmp]# 可以看到新分区已经可以使用了，在格式化时，系统会将磁盘上一定空间 (此处是 5%) 保留做其他用途，可以使用命令 dumpe2fs /dev/sdb1 2&gt;/dev/null|grep ‘Reserved block count’查看保留块数量。 这样挂载的分区只是临时有效，当系统重启时，并不会自动挂载该分区。如需要永久生效，可以将分区信息写入分区配置文件 /etc/fstab 12345678910111213[root@idc-v-71252 ~]# cat /etc/fstab## /etc/fstab# Created by anaconda on Fri Jan 15 00:59:59 2016## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root / xfs defaults 0 0UUID=10205c20-bd44-4991-8c84-7b38db63a581 /boot xfs defaults 0 0/dev/mapper/centos-home /home xfs defaults 0 0/dev/mapper/centos-swap swap swap defaults 0 0 此文件中记录了原有分区及其挂载信息，# 开头的行为注释行，其余行被分为 6 列： 第一列表示文件系统 第二列是挂载点 第三列为文件系统类型 第四列为选项 第五列表示是否对该文件系统使用 dump 工具备份，0 表示不备份 第六列表示是否使用 fsck 工具对该文件系统做定时检查，0 表示不检查 在文件中追加如下信息后，系统重启时新分区也会被自动挂载： 1/dev/sdb1 /root/temp/tmp ext4 defaults 0 0 在使用 mount 命令挂载时，可以使用选项 -o options 指定挂载选项 (/etc/fstab 中第四列) 如对已挂载的新分区重新以只读方式挂载： 12345[root@idc-v-71252 home]# mount -o remount,ro /dev/sdb1[root@idc-v-71252 home]# cd /root/temp/tmp[root@idc-v-71252 tmp]# touch 1touch: 无法创建 "1": 只读文件系统[root@idc-v-71252 tmp]# 此时再在目录 /root/temp/tmp 中创建文件时显示报错：只读文件系统 12345[root@idc-v-71252 tmp]# mount -o remount,rw /dev/sdb1[root@idc-v-71252 tmp]# touch 2[root@idc-v-71252 tmp]# ls2 lost+found[root@idc-v-71252 tmp]# 重新以读写方式挂载后可以创建文件 配置文件中的 defaults 指的是选项：rw, suid, dev, exec, auto, nouser, 和 async. 它们的意思请查看 mount 的 man 手册 选项 -a 表示读取配置文件中所有记录并重新挂载 选项 -B 或 –bind 可以使一个目录挂载至另一个目录 123456789[root@idc-v-71252 tmp]# ls -l /opt/总用量 0[root@idc-v-71252 tmp]#[root@idc-v-71252 tmp]# mount --bind /root/temp/tmp /opt[root@idc-v-71252 tmp]# ls /opt -l总用量 16-rw-r--r-- 1 root root 0 12 月 13 14:44 2drwx------ 2 root root 16384 12 月 13 12:54 lost+found[root@idc-v-71252 tmp]# 这样挂载的目录使用 df 命令并不能查看到，可以使用 mount 命令查看 123[root@idc-v-71252 tmp]# mount | grep /dev/sdb1/dev/sdb1 on /root/temp/tmp type ext4 (rw,relatime,data=ordered)/dev/sdb1 on /opt type ext4 (rw,relatime,data=ordered) 选项 -t 表示指定文件系统类型，如挂载光盘： 12345[root@centos7 tmp]# mount -t iso9660 /dev/cdrom /mntmount: /dev/sr0 写保护，将以只读方式挂载[root@centos7 tmp]##或者挂载 NFS 文件系统 (x.x.x.x 是 NFS 服务器 IP 地址)mount -t nfs x.x.x.x:/src_dir /path/to/local/dest_dir 5、umount 卸载文件系统卸载时既可以指定设备名也可以指定挂载点，当文件系统内有进程正在使用某文件时，卸载会报错： 12345[root@idc-v-71252 ~]# umount /root/temp/tmpumount: /root/temp/tmp：目标忙。 (有些情况下通过 lsof(8) 或 fuser(1) 可以 找到有关使用该设备的进程的有用信息)[root@idc-v-71252 ~]# 此时可使用 lsof 或 fuser 找出进程 (见这里)，停止该进程之后再卸载即可。 如果是卸载光盘还可以用 eject 命令 1[root@centos7 tmp]# eject 6、fsck 检查并修复文件系统可以使用 fsck 命令检查分区是否正常，需要在卸载的状态检查 12345[root@idc-v-71252 temp]# umount /dev/sdb1[root@idc-v-71252 temp]# fsck /dev/sdb1fsck，来自 util-linux 2.23.2e2fsck 1.42.9 (28-Dec-2013)/dev/sdb1: clean, 12/6553600 files, 459544/26214400 blocks 直接执行命令时，如果检测到受损，会有交互式提示询问是否进行修复坏块 选项 -a 表示不询问直接修复 选项 -y 表示总是对交互式询问输入 yes 7、mkswap 创建 swap 分区linux 的 swap 分区可以用磁盘分区做，也可以用文件做，当前系统的 swap 使用的是分区。下面举一个使用文件创建 swap 分区的例子首先使用命令 dd 生成一个大小为 8G 的文件 123456789101112[root@idc-v-71252 tmp]# dd if=/dev/zero of=swapfile bs=1024K count=8192记录了 8192+0 的读入记录了 8192+0 的写出8589934592 字节 (8.6 GB) 已复制，35.1683 秒，244 MB / 秒[root@idc-v-71252 tmp]##命令会在当前目录下创建一个文件 swapfile#if 表示指定读取的文件或设备#of 表示指定写入的文件或设备#bs 表示一次读出或写入的大小#count 表示读出或写入次数[root@idc-v-71252 tmp]# du -sh swapfile8.0G swapfile 创建 swap 分区 123[root@idc-v-71252 tmp]# mkswap swapfile正在设置交换空间版本 1，大小 = 8388604 KiB无标签，UUID=84fbe922-9444-482b-aa55-631ce72161c0 8、swapon/swapoff 启用 / 停用 swap 文件或设备123456789101112[root@idc-v-71252 tmp]# swapon swapfileswapon: /root/temp/tmp/swapfile：不安全的权限 0644，建议使用 0600。[root@idc-v-71252 tmp]# free -m total used free shared buff/cache availableMem: 7983 115 53 8 7813 7794Swap: 16255 0 16255#此处看到 swap 分区已被扩大[root@idc-v-71252 tmp]# swapoff swapfile[root@idc-v-71252 tmp]# free -m total used free shared buff/cache availableMem: 7983 109 59 8 7813 7800Swap: 8063 0 8063 9、parted 磁盘分区工具前面所述的 MBR 中的分区表不支持大于 2TB 以上的分区，为了解决这一限制和 MBR 的其它不足，出现了 GTP(全局唯一标识分区表 GUID Partition Table)，是一种磁盘的分区表的结构布局的标准，属于 UEFI(统一可扩展固件接口) 标准的一部分。需要使用命令 parted 划分支持 GTP 的分区 (可兼容 MBR 分区)。 直接使用命令 parted 时会进入交互界面 12345[root@idc-v-71252 ~]# parted /dev/sdbGNU Parted 3.1使用 /dev/sdbWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) 可以在提示符后输入 help 显示可用命令列表 (命令可简写) 命令 print(简写 p) 表示打印分区表 1234567891011(parted) p Model: VMware Virtual disk (scsi)Disk /dev/sdb: 215GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags:Number Start End Size Type File system 标志 1 1049kB 107GB 107GB primary ext4(parted) 命令 quit 表示退出交互界面 选项 -s 表示非交互模式，此时命令写在后面 123456789101112131415161718[root@idc-v-71252 ~]# parted -s /dev/sdb printModel: VMware Virtual disk (scsi)Disk /dev/sdb: 215GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags:Number Start End Size Type File system 标志 1 1049kB 107GB 107GB primary ext4[root@idc-v-71252 ~]# fdisk -l /dev/sdb1磁盘 /dev/sdb1：107.4 GB, 107374182400 字节，209715200 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小 (逻辑 / 物理)：512 字节 / 512 字节I/O 大小 (最小 / 最佳)：512 字节 / 512 字节[root@idc-v-71252 ~]# Partition Table 后的 msdos 表示为 MBR 分区，之所以两个命令中 sdb1 分区大小显示为 107G 而不是 100G 是因为在进行计算时使用 1000 bytes 作为 1KB 计数。 不能在已经做 MBR 分区的硬盘上做 GTP 分区，重做会导致原有分区被格式化。 这里在新磁盘 /dev/sdc 上做 GTP 分区： 12345[root@idc-v-71252 ~]# parted /dev/sdcGNU Parted 3.1使用 /dev/sdcWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) 注意交互模式与 fdisk 命令不同，parted 的命令一旦按回车确认，命令就马上执行，对磁盘的更改就立即生效。 命令 mklabel 指定分区格式 (msdos 或 gtp)，如果格式未知，使用 print 命令时会报错：错误: /dev/sdc: unrecognised disk label 1(parted) mklabel gpt 命令 mkpart 表示创建新分区，后面接分区类型 (主分区还是扩展分区)、文件系统类型 (ext4 等，可省略)、起始点、结束点。 mkpart primary 0KB 100GB123456789101112131415警告: You requested a partition from 0.00B to 100GB (sectors 0..195312500).The closest location we can manage is 17.4kB to 100GB (sectors 34..195312500).Is this still acceptable to you?是 / Yes / 否 / No? yes 警告: The resulting partition is not properly aligned for best performance.忽略 / Ignore / 放弃 / Cancel? ignore (parted) p Model: VMware Virtual disk (scsi)Disk /dev/sdc: 215GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name 标志 1 17.4kB 100GB 100GB primary 命令 rm 表示删除分区，后面接分区号 1234567891011(parted) rm 1 (parted) p Model: VMware Virtual disk (scsi)Disk /dev/sdc: 215GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name 标志(parted) 下面使用非交互模式继续 1234567891011[root@idc-v-71252 ~]# parted -s /dev/sdc mkpart primary ext4 18KB 100GB警告: The resulting partition is not properly aligned for best performance.[root@idc-v-71252 ~]# parted -s /dev/sdc printModel: VMware Virtual disk (scsi)Disk /dev/sdc: 215GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name 标志 1 17.9kB 100GB 100GB primary 这里使用 1000 bytes 作为 1KB 计数格式化并挂载 (部分输出略) 12345678[root@idc-v-71252 temp]# mkfs.ext4 /dev/sdc1[root@idc-v-71252 temp]# mount /dev/sdc1 /root/temp/tmp_1[root@idc-v-71252 temp]# df -h|grep ^/dev/dev/mapper/centos-root 49G 22G 27G 44% //dev/sda1 497M 170M 328M 35% /boot/dev/mapper/centos-home 24G 16G 7.6G 68% /home/dev/sdb1 99G 61M 94G 1% /root/temp/tmp/dev/sdc1 92G 61M 87G 1% /root/temp/tmp_1 最后再用 parted 做一个 MBR 扩展分区，命令如下： 123parted -s /dev/sdd mklabel msdosparted -s /dev/sdd mkpart extended 100GB 100%parted -s /dev/sdd mkpart logical 100GB 200GB 结果显示为： 1234567891011121314151617181920212223[root@idc-v-71252 temp]# parted -s /dev/sdd printModel: VMware Virtual disk (scsi)Disk /dev/sdd: 215GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags:Number Start End Size Type File system 标志 1 100GB 215GB 115GB extended lba 5 100GB 200GB 100GB logical[root@idc-v-71252 temp]# fdisk -l /dev/sdd磁盘 /dev/sdd：214.7 GB, 214748364800 字节，419430400 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小 (逻辑 / 物理)：512 字节 / 512 字节I/O 大小 (最小 / 最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0x0006d495 设备 Boot Start End Blocks Id System/dev/sdd1 195311616 419430399 112059392 f W95 Ext'd (LBA)/dev/sdd5 195313664 390625279 97655808 83 Linux 格式化及挂载 (省略部分输出) 123456789[root@idc-v-71252 temp]# mkfs.ext4 /dev/sdd5[root@idc-v-71252 temp]# mount /dev/sdd5 /root/temp/tmp_2[root@idc-v-71252 temp]# df -h|grep ^/dev/dev/mapper/centos-root 49G 22G 27G 44% //dev/sda1 497M 170M 328M 35% /boot/dev/mapper/centos-home 24G 16G 7.6G 68% /home/dev/sdb1 99G 61M 94G 1% /root/temp/tmp/dev/sdc1 92G 61M 87G 1% /root/temp/tmp_1/dev/sdd5 92G 61M 87G 1% /root/temp/tmp_2 这些新分区都可以写入配置文件 /etc/fstab 中实现重启后自动挂载 LVM 逻辑卷管理LVM 是 linux 环境下对磁盘分区进行管理的一种机制，能够使系统管理员更方便的为应用与用户分配存储空间。 术语 物理存储介质 (The physical media)：指的是系统的存储设备，如上面制作的分区 /dev/sdb1、/dev/sdc1、/dev/sdd5 物理卷 (PV: Physical Volume)：相当于物理存储介质，但添加了与 LVM 相关的管理参数。 卷组 (VG: Volume Group)：由一个或多个物理卷组成。 逻辑卷 (LV: Logical Volume)：在卷组的基础上划分的逻辑分区 (文件系统)。 PE(physical extent)：每一个物理卷被划分为称为 PE 的基本单元，具有唯一编号的 PE 是可以被 LVM 寻址的最小单元。PE 的大小是可配置的，默认为 4MB。 LE(logical extent)：逻辑卷也被划分为被称为 LE 的可被寻址的基本单位。在同一个卷组中，LE 的大小和 PE 是相同的，并且一一对应。 步骤1、创建分区 可以使用 fdisk 或 parted 进行分区，和前面举例中的区别仅仅是分区类型要选 8e。这里将三块新硬盘的剩余空间做成 LVM 分区，parted 方式 (仅举一例，其余略)： 12parted -s /dev/sdb mkpart primary 107GB 100%parted -s /dev/sdb toggle 2 lvm #表示将第二个分区定义为 lvm 类型 (8e) 2、创建 PV 123456789101112[root@idc-v-71252 ~]# pvcreate /dev/sd[bcd]2 Physical volume "/dev/sdb2" successfully created Physical volume "/dev/sdc2" successfully created Physical volume "/dev/sdd2" successfully created[root@idc-v-71252 ~]##查看[root@idc-v-71252 ~]# pvscan PV /dev/sda2 VG centos lvm2 [79.51 GiB / 64.00 MiB free] PV /dev/sdb2 lvm2 [100.00 GiB] PV /dev/sdc2 lvm2 [106.87 GiB] PV /dev/sdd2 lvm2 [93.13 GiB] Total: 4 [379.50 GiB] / in use: 1 [79.51 GiB] / in no VG: 3 [300.00 GiB] 3、创建 VG 12345678[root@idc-v-71252 ~]# vgcreate -s 8M test_lvm /dev/sd[bcd]2 Volume group "test_lvm" successfully created#这里使用选项 -s 指定 PE 大小为 8M，卷组起名为 test_lvm#查看[root@idc-v-71252 ~]# vgscan Reading all physical volumes. This may take a while... Found volume group "centos" using metadata type lvm2 Found volume group "test_lvm" using metadata type lvm2 4、创建 LV 1234567891011[root@idc-v-71252 ~]# lvcreate -n test_1 -L 50G test_lvm Logical volume "test_1" created.[root@idc-v-71252 ~]##选项 -n 指定 LV 名为 test_1，-L 指定大小，也可以用选项 -l 指定 LE 的数量#查看[root@idc-v-71252 ~]# lvscan ACTIVE '/dev/centos/swap' [7.88 GiB] inherit ACTIVE '/dev/centos/home' [23.48 GiB] inherit ACTIVE '/dev/centos/root' [48.09 GiB] inherit ACTIVE '/dev/test_lvm/test_1' [50.00 GiB] inherit[root@idc-v-71252 ~]# 5、格式化及挂载 123456789101112131415161718192021#在这里进行格式化，第一步分区之后并不需要格式化。#这里我们格式化成 xfs 格式[root@idc-v-71252 ~]# mkfs.xfs /dev/test_lvm/test_1meta-data=/dev/test_lvm/test_1 isize=256 agcount=4, agsize=3276800 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0data = bsize=4096 blocks=13107200, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=0log =internal log bsize=4096 blocks=6400, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0[root@idc-v-71252 ~]# mount /dev/test_lvm/test_1 /root/temp/test_1[root@idc-v-71252 ~]# df -h|grep ^/dev/dev/mapper/centos-root 49G 22G 27G 44% //dev/sda1 497M 170M 328M 35% /boot/dev/mapper/centos-home 24G 16G 7.6G 68% /home/dev/sdb1 99G 61M 94G 1% /root/temp/tmp/dev/sdc1 92G 61M 87G 1% /root/temp/tmp_1/dev/sdd5 92G 61M 87G 1% /root/temp/tmp_2/dev/mapper/test_lvm-test_1 50G 33M 50G 1% /root/temp/test_1 这里文件系统之所以显示为 /dev/mapper/…. 是因为内核利用 Mapper Device 机制将设备做了映射： 1234[root@idc-v-71252 ~]# ls -l /dev/mapper/test_lvm-test_1lrwxrwxrwx 1 root root 7 12 月 14 09:58 /dev/mapper/test_lvm-test_1 -&gt; ../dm-3[root@idc-v-71252 ~]# ls -l /dev/test_lvm/test_1lrwxrwxrwx 1 root root 7 12 月 14 09:58 /dev/test_lvm/test_1 -&gt; ../dm-3 实际上 /dev/test_lvm/test_1 和 /dev/mapper/test_lvm-test_1 指向了同一个设备 /dev/dm-3(在配置文件 /etc/fstab 中写任意一种都可以)，这里就不对映射机制做详细展开了。 命令前面举例中说到了几个创建和查看命令，除此之外，LVM 还有一系列的命令，它们都以 pv/vg/lv 开头，所起的作用大多是增加、删除、扩充、缩减、查看、改变等等。 创建命令1pvcreate vgcreate lvcreate 查看命令分三类，显示信息侧重或详细程度不同：123pvs pvscan pvdisplayvgs vgscan vgdisplaylvs lvscan lvdisplay 改变属性 (分别改变本层次上对象的属性)1pvchange vgchange lvchange 扩容1vgextend lvextend 扩容 LV 举例 (注意内核可能不支持对某些文件系统的在线扩容，此时需要先将文件系统卸载)： 123456789101112131415161718192021[root@idc-v-71252 dev]# lvextend -L +10G /dev/test_lvm/test_1 Size of logical volume test_lvm/test_1 changed from 50.00 GiB (6400 extents) to 60.00 GiB (7680 extents). Logical volume test_1 successfully resized.[root@idc-v-71252 ~]# df -h /dev/mapper/test_lvm-test_1文件系统 容量 已用 可用 已用 % 挂载点/dev/mapper/test_lvm-test_1 50G 33M 50G 1% /root/temp/test_1#此时扩容还没有生效，使用 xfs_growfs 对 xfs 文件系统进行在线扩容[root@idc-v-71252 dev]# xfs_growfs /dev/test_lvm/test_1meta-data=/dev/mapper/test_lvm-test_1 isize=256 agcount=4, agsize=3276800 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0data = bsize=4096 blocks=13107200, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=0log =internal bsize=4096 blocks=6400, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0data blocks changed from 13107200 to 15728640[root@idc-v-71252 ~]# df -h /dev/mapper/test_lvm-test_1文件系统 容量 已用 可用 已用 % 挂载点/dev/mapper/test_lvm-test_1 60G 33M 60G 1% /root/temp/test_1 ext 系列的文件系统扩容时需要使用命令 resize2fs 进行在线扩容 缩减 (慎用)1vgreduce lvreduce 改名 1vgrename lvrename 还有一些其他命令这里就不再列出了，关于它们的用法请查看相关手册 本文简要介绍了磁盘和LVM相关的管理命令，另外，还有一个介于物理磁盘和磁盘分区的中间层：RAID(独立冗余磁盘阵列)，它提供磁盘级别的数据冗余能力。当前服务器上一般都有RAID卡(硬件)，关于它的设置以及原理就不在此叙述了，请搜索相关文档。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之文件搜索及其它]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-file-search%2F</url>
      <content type="text"><![CDATA[1、linux 中包含大量的文件，对于文件查找，linux 提供了 find 命令。find 是一个非常有效的工具，它可以遍历目标目录甚至整个文件系统来查找某些文件或目录： 1find [path...] [expression] 其中 expression 包括三种：options、tests 和 actions。多个表达式之间被操作符分隔，当操作符被省略时，表示使用了默认操作符 -and。 当表达式中不包含任何 actions 时，默认使用 -print，也就是打印出搜索到的所有文件，用换行分隔。 其实可以将三种表达式均视为选项，表示对搜索的某种限制 (如 -maxdepth 表示搜索路径的最大深度)、或对找到的目标文件的某种测试 (如 -readable 判断是否可读)、或对结果采取的某种动作 (如 -print)。 选项 -name pattern 搜索文件名： 123456789101112[root@centos7 temp]# find /root/* -name "file?" /root/file1/root/temp/file1/root/temp/file2/root/temp/file3/root/temp/file4/root/temp/file5/root/temp/file6/root/temp/file7/root/temp/file8/root/temp/file9[root@centos7 temp]# 此例中搜索目录 /root 下所有文件，找出匹配 file? 的文件名，同时由于没有指定 action，所以使用默认的 -print 将结果打印出来。find 命令中，搜索路径和某些文件名的表示可以使用 shell 通配符 (见上一篇)，但为了避免混淆，处于选项后的通配符需要被引号引起来。 选项 -maxdepth n 指定搜索路径的最大深度： 123[root@centos7 ~]# find /root -maxdepth 1 -name "file?" #注意表达式之间的隐含操作符 -and/root/file1[root@centos7 ~]# 本例中指定最大深度为 1，表示只搜索 /root 目录，而不进入任何它的子目录去搜索。和此选项相对应，-mindepth 表示指定搜索路径的最小深度。 选项 -user name 按照文件属主来查找文件： 1234[root@centos7 ~]# find /root/temp -name "file?" -user learner/root/temp/file1/root/temp/file2[root@centos7 ~]# 或者类似选项 -uid n 表示按文件属主的 uid，-gid n 表示按文件所属组的 gid，-group name 表示按文件所属组。 选项 -mtime n 文件上次内容被修改距离现在 n*24 小时： 123456789101112131415[root@centos7 temp]# ls -lt file1?-rw-r--r-- 1 root root 64 10 月 27 15:06 file11-rw-r--r-- 1 root root 132 10 月 27 13:28 file10-rw-r--r-- 1 root root 22 10 月 26 21:31 file12-rw-r--r-- 1 root root 137 10 月 12 16:42 file13[root@centos7 temp]# find . -name "file1?" -mtime +5 #五天前./file13[root@centos7 temp]#[root@centos7 temp]# find . -name "file1?" -mtime -5 #五天内./file10./file11[root@centos7 temp]#[root@centos7 temp]# find . -name "file1?" -mtime 5 #刚好五天./file12[root@centos7 temp]# 本例中使用了命令 ls 的选项 -t 对文件的时间进行排序，最近被修改的文件在前。选项 -mtime n 中 n 可以表示成： +n 表示大于 n -n 表示小于 n n 表示等于 n 还有其他时间 (如 atime,ctime) 的比较，用法相同。 选项 -newer file 表示搜索到的文件比指定的 file 要‘新’(上次内容被修改离现在时间更短)： 1234[root@centos7 temp]# find . -name "file1?" -newer file12./file10./file11[root@centos7 temp]# 选项 -path pattern 文件名匹配 pattern(通配符): 1234[root@centos7 temp]# find . -name "file1?" -path "./file1[13]"./file11./file13[root@centos7 temp]# 注意 pattern 匹配时不会对 / 和. 进行特殊处理。 通常 -path 会配合选项 -prune 使用，表示对某目录的排除： 12345678910111213[root@centos7 temp]# find . -name "file*"./file10./file12./file11./tmp/file./file13[root@centos7 temp]#[root@centos7 temp]# find . -path "./tmp" -prune -o -name "file*" -print./file10./file12./file11./file13[root@centos7 temp]# 这里的 -o 表示或者，它和之前所说的 -and 都是操作符。表示表达式之间的逻辑关系。本例中可以理解为：如果目录匹配./tmp 则执行 -prune 跳过该目录，否则匹配 -name 指定的文件并执行 -print。 除这两个操作符外，操作符! 或 -not 表示逻辑非，操作符 (…) 和数学运算中的括号类似，表示提高优先级： 12345678910111213[root@centos7 temp]# find . ! -path "./tmp*" -name "file*" ./file10./file12./file11./file13[root@centos7 temp]##排除多个目录：[root@centos7 temp]# find . \( -path "./tmp" -o -path "./abcd" \) -prune -o -name "file*" -print./file10./file12./file11./file13[root@centos7 temp]# 注意这里的 (…) 操作符需要被转义(为避免被 shell 解释为其他含义)，在符号前加上反斜线’\’。(关于 shell 中的转义或引用我们会在讲 bash 编程时详述) 选项 -type x 表示搜索类型为 x 的文件，其中 x 的可能值包括 b、c、d、p、f、l、s。它们和命令 ls 显示的文件类型一致 (见基础命令介绍一)，f 代表普通文件。 123456[root@centos7 temp]# ln -s file13 file14[root@centos7 temp]# ls -l file14lrwxrwxrwx 1 root root 6 11 月 1 12:29 file14 -&gt; file13[root@centos7 temp]# find . -type l./file14[root@centos7 temp]# 选项 -perm mode 表示搜索特定权限的文件： 123456789[root@centos7 temp]# chmod 777 file14[root@centos7 temp]# ls -l file1[3-4]-rwxrwxrwx 1 root root 137 10 月 12 16:42 file13lrwxrwxrwx 1 root root 6 11 月 1 12:29 file14 -&gt; file13[root@centos7 temp]#[root@centos7 temp]# find . -perm 777./file13./file14[root@centos7 temp]# 或表示成： 1234[root@centos7 temp]# find . -perm -g=rwx #表示文件所属组的权限是可读、可写、可执行。./file13./file14[root@centos7 temp]# 选项 -size n 表示搜索文件大小 1234[root@centos7 temp]# find . -path "./*" -size +100c./file10./file13[root@centos7 temp]# 此例中 + 100c 表示当前目录下大于 100 bytes 的文件，n 和前面表示时间的方式类似 (+n,-n,n)，n 后面的字符还包括： 1234b 单位为 512 bytes 的块 (n 后面没有后缀时的默认单位)k 1024 bytesM 1048576 bytesG 1073741824 bytes 选项 -print0 类似 -print 输出文件名，但不用任何字符分隔它们。当文件名中包含特殊字符时使用。可以配合带选项 -0 的命令 xargs 一起使用 (后述)。 选项 -exec command ; 表示要执行的命令 -exec 后可以跟任意 shell 命令来对搜索到的文件做进一步的处理，在 command 和分号之间都被视为 command 的参数，其中用 {} 代表被搜索到的文件。分号需要被转义。如对搜索到的文件执行命令 ls -l： 12345678[root@centos7 temp]# find . -name "file*" -exec ls -l &#123;&#125; \;-rw-r--r-- 1 root root 132 10 月 27 13:28 ./file10-rw-r--r-- 1 root root 22 10 月 26 21:31 ./file12-rw-r--r-- 1 root root 64 10 月 27 15:06 ./file11-rw-r--r-- 1 root root 67 10 月 31 17:50 ./tmp/file-rw-r--r-- 1 root root 0 11 月 1 12:05 ./abcd/file15-rwxrwxrwx 1 root root 137 10 月 12 16:42 ./file13lrwxrwxrwx 1 root root 6 11 月 1 12:29 ./file14 -&gt; file13 -exec 选项后的命令是在启动 find 所在的目录内执行的，并且对于每个搜索到的文件，该命令都执行一次，而不是把所有文件列在命令后面只执行一次。举例说明下其中的区别： 12345678910#命令 echo 只执行一次[root@centos7 temp]# echo ./file11 ./file12 ./file13./file11 ./file12 ./file13#命令 echo 执行了三次[root@centos7 temp]# find . -name "file1[1-3]" -exec echo &#123;&#125; \;./file12./file11./file13[root@centos7 temp]# 当使用格式 -exec command {} + 时表示每个文件都被追加到命令后面，这样，命令就只被执行一次了： 123[root@centos7 temp]# find . -name "file1[1-3]" -exec echo &#123;&#125; +./file12 ./file11 ./file13[root@centos7 temp]# 但有时会出现问题： 123[root@centos7 temp]# find . -name "file1[1-3]" -exec mv &#123;&#125; abcd/ +find: 遗漏 “-exec” 的参数[root@centos7 temp]# 因为这里文件被追加于目录 abcd / 的后面，导致报错。 同时，使用格式 -exec command {} + 还可能会造成被追加的文件数过多，超出了操作系统对命令行长度的限制。 使用 -exec 可能会有安全漏洞，通常使用管道和另一个命令 xargs 来代替 -exec 执行命令。 2、xargs 从标准输入中获得命令的参数并执行xargs 从标准输入中获得由空格分隔的项目，并执行命令 (默认为 /bin/echo) 选项 -0 将忽略项目的分隔符，配合 find 的选项 -print0，处理带特殊符号的文件。 123456[root@centos7 temp]# find . -name "file*" -print0 | xargs -0 ls -l-rw-r--r-- 1 root root 132 10 月 27 13:28 ./file10-rw-r--r-- 1 root root 64 10 月 27 15:06 ./file11-rw-r--r-- 1 root root 22 10 月 26 21:31 ./file12-rwxrwxrwx 1 root root 137 10 月 12 16:42 ./file13-rw-r--r-- 1 root root 0 11 月 1 14:45 ./file 14 #注意此文件名中包含空格 当不用时： 1234[root@centos7 temp]# find . -name "file*" | xargs lsls: 无法访问./file: 没有那个文件或目录ls: 无法访问 14: 没有那个文件或目录./file10 ./file11 ./file12 ./file13 选项 -I string 为输入项目指定替代字符串： 12345[root@centos7 temp]# ls abcd/[root@centos7 temp]# find . -name "file*" | xargs -I&#123;&#125; mv &#123;&#125; abcd/[root@centos7 temp]# ls abcd/file10 file11 file12 file13[root@centos7 temp]# 这里的意思是说使用 -I 后面的字符串去代替输入项目，这样就可以把它们作为整体放到命令的任意位置来执行了。也避免了 -exec command {} + 的错误。 选项 -d 指定输入项目的分隔符： 12345[root@centos7 temp]# head -n1 /etc/passwdroot:x:0:0:root:/root:/bin/bash[root@centos7 temp]# head -n1 /etc/passwd|xargs -d ":" echo -nroot x 0 0 root /root /bin/bash[root@centos7 temp]# 选项 -P 指定最大进程数，默认进程数为 1，多个进程并发执行。 3、date 打印或设置系统时间1date [OPTION]... [+FORMAT] 当没有任何参数时表示显示当前时间： 123[root@centos7 temp]# date2016 年 11 月 01 日 星期二 15:30:46 CST[root@centos7 temp]# 选项 -d string 按描述字符串显示时间 (例子中字符串表示距离 1970-01-01 零点的秒数)： 12[root@centos7 temp]# date --date='@2147483647'2038 年 01 月 19 日 星期二 11:14:07 CST 或者： 12[root@centos7 temp]# date -d@21474836472038 年 01 月 19 日 星期二 11:14:07 CST -d 后面的字符串还可以是： 12[root@centos7 temp]# date -d "-1 day"2016 年 10 月 31 日 星期一 16:11:27 CST 表示昨天 又如明年表示为： 12[root@centos7 temp]# date -d "1 year"2017 年 11 月 01 日 星期三 16:12:27 CST 选项 -s 设置系统时间： 1234[root@centos7 temp]# date -s "2016-11-01 15:49"2016 年 11 月 01 日 星期二 15:49:00 CST[root@centos7 temp]# date2016 年 11 月 01 日 星期二 15:49:03 CST 由于 linux 系统启动时将读取 CMOS 来获得时间，系统会每隔一段时间将系统时间写入 CMOS，为避免更改时间后系统的立即重启造成时间没有被写入 CMOS，通常设置完时间后会使用命令 clock -w 将系统时间写入到 CMOS 中。 date 命令中由 FORMAT 来控制输出格式，加号 + 在格式之前表示格式开始： 123[root@centos7 temp]# date "+%Y-%m-%d %H:%M:%S"2016-11-01 16:00:45[root@centos7 temp]# 本例中格式被双引号引起来以避免被 shell 误解，其中： 123456%Y 表示年%m 表示月%d 表示天%H 表示小时%M 表示分钟%S 表示秒 还可以指定很多其他格式如只输出当前时间： 123[root@centos7 temp]# date "+%T"16:03:50[root@centos7 temp]# 如输出距离 1970-01-01 零点到现在时间的秒数： 123[root@centos7 temp]# date +%s1477987540[root@centos7 temp]# 如输出今天星期几： 123[root@centos7 temp]# date +%A星期二[root@centos7 temp]# 其他格式请自行 man 4、gzip 压缩或解压文件1gzip [OPTION]... [FILE]... 当命令后直接跟文件时，表示压缩该文件： 123456789101112[root@centos7 temp]# ls -l file1*-rw-r--r-- 1 root root 132 10 月 27 13:28 file10-rw-r--r-- 1 root root 64 10 月 27 15:06 file11-rw-r--r-- 1 root root 22 10 月 26 21:31 file12-rw-r--r-- 1 root root 137 10 月 12 16:42 file13[root@centos7 temp]#[root@centos7 temp]# gzip file10 file11 file12 file13[root@centos7 temp]# ls -l file1* -rw-r--r-- 1 root root 75 10 月 27 13:28 file10.gz-rw-r--r-- 1 root root 49 10 月 27 15:06 file11.gz-rw-r--r-- 1 root root 44 10 月 26 21:31 file12.gz-rw-r--r-- 1 root root 109 10 月 12 16:42 file13.gz 压缩后的文件以. gz 结尾，gzip 是不保留源文件的 选项 -d 表示解压缩 123456[root@centos7 temp]# gzip -d *.gz[root@centos7 temp]# ls -l file1*-rw-r--r-- 1 root root 132 10 月 27 13:28 file10-rw-r--r-- 1 root root 64 10 月 27 15:06 file11-rw-r--r-- 1 root root 22 10 月 26 21:31 file12-rw-r--r-- 1 root root 137 10 月 12 16:42 file13 选项 -r 可以递归地进入目录并压缩里面的文件 选项 -n 指定压缩级别，n 为从 1-9 的数字。1 为最快压缩，但压缩比最小; 9 的压缩速度最慢，但压缩比最大。默认时 n 为 6。 1[root@centos7 temp]# gzip -r9 ./tmp 当 gzip 后没有文件或文件为 - 时，将从标准输入读取并压缩： 123[root@centos7 temp]# echo "hello world" | gzip &gt;hello.gz[root@centos7 temp]# ls -l *.gz-rw-r--r-- 1 root root 32 11 月 1 16:40 hello.gz 注意例子中 gzip 的输出被重定向到文件 hello.gz 中，如果对此文件进行解压，将会生成文件 hello。如果被重定向的文件后缀不是. gz，文件名在被改成. gz 后缀之前将不能被解压。 5、zcat 将压缩的文件内容输出到标准输出123[root@centos7 temp]# zcat hello.gzhello world[root@centos7 temp]# zcat 读取被 gzip 压缩的文件，只需文件格式正确，不需要文件名具有. gz 的后缀。 6、bzip2 压缩解压文件1bzip2 [OPTION]... [FILE]... 命令 bzip2 和 gzip 类似都是压缩命令，只是使用的压缩算法不一样，通常 bzip2 的压缩比较高。本命令默认同样不保留源文件，默认文件名后缀为. bz2： 123[root@centos7 temp]# bzip2 file11[root@centos7 temp]# ls -l file11.bz2-rw-r--r-- 1 root root 61 10 月 27 15:06 file11.bz2 选项 -k 可使源文件保留： 1234[root@centos7 temp]# bzip2 -k file10[root@centos7 temp]# ls -l file10*-rw-r--r-- 1 root root 132 10 月 27 13:28 file10-rw-r--r-- 1 root root 96 10 月 27 13:28 file10.bz2 选项 -d 表示解压 (若存在源文件则报错)： 12345[root@centos7 temp]# bzip2 -d file10.bz2bzip2: Output file file10 already exists.[root@centos7 temp]# bzip2 -d file11.bz2[root@centos7 temp]# ls -l file11-rw-r--r-- 1 root root 64 10 月 27 15:06 file11 选项 -f 表示强制覆盖源文件： 123[root@centos7 temp]# bzip2 -d -f file10.bz2[root@centos7 temp]# ls -l file10*-rw-r--r-- 1 root root 132 10 月 27 13:28 file10 选项 -n 和 gzip 用法一致，表示压缩比。 7、tar 打包压缩文件1tar [OPTION...] [FILE]... 命令 gzip 和 bzip2 均不支持压缩目录 (虽然 gzip 可以用选项 -r 到目录内去压缩，但仍无法压缩目录)，用 tar 命令可以将目录归档，然后利用压缩命令进行压缩： 1234567[root@centos7 temp]# tar -cf tmp.tar tmp/[root@centos7 temp]# ls -l总用量 18256drwxr-xr-x 2 root root 6 11 月 1 16:23 abcd-rwxr-xr-x 1 root root 12 10 月 28 17:24 test.shdrwxr-xr-x 2 root root 425984 11 月 1 17:08 tmp-rw-r--r-- 1 root root 18001920 11 月 1 17:17 tmp.tar 例子中选项 -c 表示创建打包文件，-f tmp.tar 表示指定打包文件名为 tmp.tar，后面跟被打包目录名 tmp/。 选项 -t 列出归档内容 选项 -v 详细地列出处理的文件 12345678[root@centos7 temp]# ls -l abcd.tar-rw-r--r-- 1 root root 10240 11 月 2 08:58 abcd.tar[root@centos7 temp]# tar -tvf abcd.tardrwxr-xr-x root/root 0 2016-11-02 08:57 abcd/-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file10-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file11-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file12-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file13 选项 -u 更新归档文件 (update)。 12345678910[root@centos7 temp]# touch abcd/file15[root@centos7 temp]# tar uvf abcd.tar abcdabcd/file15[root@centos7 temp]# tar tvf abcd.tardrwxr-xr-x root/root 0 2016-11-02 08:57 abcd/-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file10-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file11-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file12-rw-r--r-- root/root 6 2016-11-02 08:57 abcd/file13-rw-r--r-- root/root 0 2016-11-02 09:07 abcd/file15 选项 -x 对归档文件进行提取操作。(解包) 1234567891011[root@centos7 temp]# rm -rf abcd/[root@centos7 temp]# tar -xvf abcd.tarabcd/abcd/file10abcd/file11abcd/file12abcd/file13abcd/file15[root@centos7 temp]# ls abcd #这里是按两次 tab 键的补全结果abcd/ abcd.tar [root@centos7 temp]# 选项 -O 解压文件至标准输出 123456789[root@centos7 temp]# tar -xf abcd.tar -Ohellohellohellohello[root@centos7 temp]# #注意这里输出了每个归档文件的内容[root@centos7 temp]# tar -xf abcd.tar -O | xargs echohello hello hello hello[root@centos7 temp]# 选项 -p 保留文件权限 (用于解包时)。 选项 -j、-J、-z 用于压缩。 其中 -j 使用命令 bzip2，-J 使用命令 xz，-z 使用命令 gzip 分别将归档文件进行压缩解压处理 (命令 tar 后的选项可以省略 -)： 123456789[root@centos7 temp]# tar zcf tmp.tar.gz tmp[root@centos7 temp]# tar jcf tmp.tar.bz2 tmp[root@centos7 temp]# tar Jcf tmp.tar.xz tmp[root@centos7 temp]# du -sh tmp*70M tmp28K tmp.tar.bz2180K tmp.tar.gz40K tmp.tar.xz[root@centos7 temp]# 本例中分别使用三种压缩格式进行压缩，可以看到使用命令 bzip2 的压缩比最高，命令 gzip 的压缩比最低。在执行压缩文件时，压缩时间也是我们考量的一个重要因素。默认时，使用 gzip 最快，xz 最慢。 对于这三种格式的压缩文件进行解压，只需将选项中 -c 换成 -x 即可。 选项 -X FILE 排除匹配文件 FILE 中所列模式的文件： 123456789[root@centos7 abcd]# cat filefile10file13[root@centos7 abcd]# tar -X file -cf file.tar file*[root@centos7 abcd]# tar -tvf file.tar-rw-r--r-- root/root 14 2016-11-02 10:10 file-rw-r--r-- root/root 6 2016-11-02 10:02 file11-rw-r--r-- root/root 6 2016-11-02 10:02 file12-rw-r--r-- root/root 0 2016-11-02 09:07 file15 注意文件 FILE 中支持通配符匹配： 12345678[root@centos7 abcd]# cat filefile1[2-3][root@centos7 abcd]# tar -X file -cf file.tar file*[root@centos7 abcd]# tar -tvf file.tar-rw-r--r-- root/root 11 2016-11-02 10:20 file-rw-r--r-- root/root 6 2016-11-02 10:02 file10-rw-r--r-- root/root 6 2016-11-02 10:02 file11-rw-r--r-- root/root 0 2016-11-02 09:07 file15 选项 -C DIR 改变至目录 DIR(用于解包时)： 12345678910[root@centos7 temp]# tar zxf tmp.tar.gz -C abcd[root@centos7 temp]# ls -l abcd/总用量 688-rw-r--r-- 1 root root 11 11 月 2 10:20 file-rw-r--r-- 1 root root 6 11 月 2 10:02 file10-rw-r--r-- 1 root root 6 11 月 2 10:02 file11-rw-r--r-- 1 root root 6 11 月 2 10:02 file12-rw-r--r-- 1 root root 6 11 月 2 10:02 file13-rw-r--r-- 1 root root 0 11 月 2 09:07 file15drwxr-xr-x 2 root root 425984 11 月 1 17:08 tmp 只解压指定文件： 1234567[root@centos7 temp]# tar zxvf tmp.tar.gz -C abcd/ file1[23]file12file13[root@centos7 temp]# ls -l abcd总用量 12-rw-r--r-- 1 root root 6 11 月 16 15:26 file12-rw-r--r-- 1 root root 6 11 月 16 15:26 file13 注意这里解压时，指定文件不能在选项 -C 之前 如不想解压压缩包，但想查看压缩包中某个文件的内容时，可以使用如下技巧： 123[root@centos7 temp]# tar zxf tmp.tar.gz file -OBLOG ADDRESS IS "https://segmentfault.com/learnning"[root@centos7 temp]# 本文讲述了linux中关于文件搜索和归档压缩等相关的命令及部分选项用法，都是在系统管理过程中经常要使用的。需熟练使用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Firewalld 防火墙]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-firewall%2F</url>
      <content type="text"><![CDATA[Firewalld 服务是红帽 RHEL7 系统中默认的防火墙管理工具，特点是拥有运行时配置与永久配置选项且能够支持动态更新以及 “zone” 的区域功能概念，使用图形化工具 firewall-config 或文本管理工具 firewall-cmd，下面实验中会讲到~ 区域概念与作用防火墙的网络区域定义了网络连接的可信等级，我们可以根据不同场景来调用不同的 firewalld 区域，区域规则有： 区域 默认规则策略 trusted 允许所有的数据包。 home 拒绝流入的数据包，除非与输出流量数据包相关或是 ssh,mdns,ipp-client,samba-client 与 dhcpv6-client 服务则允许。 internal 等同于 home 区域 work 拒绝流入的数据包，除非与输出流量数据包相关或是 ssh,ipp-client 与 dhcpv6-client 服务则允许。 public 拒绝流入的数据包，除非与输出流量数据包相关或是 ssh,dhcpv6-client 服务则允许。 external 拒绝流入的数据包，除非与输出流量数据包相关或是 ssh 服务则允许。 dmz 拒绝流入的数据包，除非与输出流量数据包相关或是 ssh 服务则允许。 block 拒绝流入的数据包，除非与输出流量数据包相关。 drop 拒绝流入的数据包，除非与输出流量数据包相关。 简单来讲就是为用户预先准备了几套规则集合，我们可以根据场景的不同选择合适的规矩集合，而默认区域是 public。 字符管理工具如果想要更高效的配置妥当防火墙，那么就一定要学习字符管理工具 firewall-cmd 命令, 命令参数有： 参数 作用 –get-default-zone 查询默认的区域名称。 –set-default-zone=&lt;区域名称&gt; 设置默认的区域，永久生效。 –get-zones 显示可用的区域。 –get-services 显示预先定义的服务。 –get-active-zones 显示当前正在使用的区域与网卡名称。 –add-source= 将来源于此 IP 或子网的流量导向指定的区域。 –remove-source= 不再将此 IP 或子网的流量导向某个指定区域。 –add-interface=&lt;网卡名称&gt; 将来自于该网卡的所有流量都导向某个指定区域。 –change-interface=&lt;网卡名称&gt; 将某个网卡与区域做关联。 –list-all 显示当前区域的网卡配置参数，资源，端口以及服务等信息。 –list-all-zones 显示所有区域的网卡配置参数，资源，端口以及服务等信息。 –add-service=&lt;服务名&gt; 设置默认区域允许该服务的流量。 –add-port=&lt;端口号/协议&gt; 允许默认区域允许该端口的流量。 –remove-service=&lt;服务名&gt; 设置默认区域不再允许该服务的流量。 –remove-port=&lt;端口号/协议&gt; 允许默认区域不再允许该端口的流量。 –reload 让 “永久生效” 的配置规则立即生效，覆盖当前的。 特别需要注意的是 firewalld 服务有两份规则策略配置记录，必需要能够区分： RunTime: 当前正在生效的。 Permanent: 永久生效的。 当下面实验修改的是永久生效的策略记录时，必须执行 “--reload“ 参数后才能立即生效，否则要重启后再生效。 查看当前的区域：12$ firewall-cmd --get-default-zonepublic 查询 eno16777728 网卡的区域：12$ firewall-cmd --get-zone-of-interface=eno16777728public 在 public 中分别查询 ssh 与 http 服务是否被允许：1234$ firewall-cmd --zone=public --query-service=sshyes$ firewall-cmd --zone=public --query-service=httpno 设置默认规则为 dmz：1$ firewall-cmd --set-default-zone=dmz 让 “永久生效” 的配置文件立即生效：12$ firewall-cmd --reloadsuccess 启动/关闭应急状况模式，阻断所有网络连接：应急状况模式启动后会禁止所有的网络连接，一切服务的请求也都会被拒绝，当心，请慎用。 1234$ firewall-cmd --panic-onsuccess$ firewall-cmd --panic-offsuccess 如果您已经能够完全理解上面练习中 firewall-cmd 命令的参数作用，不妨来尝试完成下面的模拟训练吧： 模拟训练 A: 允许 https 服务流量通过 public 区域，要求立即生效且永久有效：方法一: 分别设置当前生效与永久有效的规则记录：12$ firewall-cmd --zone=public --add-service=https$ firewall-cmd --permanent --zone=public --add-service=https 方法二: 设置永久生效的规则记录后读取记录：12$ firewall-cmd --permanent --zone=public --add-service=https$ firewall-cmd --reload 模拟训练 B: 不再允许 http 服务流量通过 public 区域，要求立即生效且永久生效：12$ firewall-cmd --permanent --zone=public --remove-service=http success 使用参数 “–reload” 让永久生效的配置文件立即生效： 12$ firewall-cmd --reloadsuccess 模拟训练 C: 允许 8080 与 8081 端口流量通过 public 区域，立即生效且永久生效：12$ firewall-cmd --permanent --zone=public --add-port=8080-8081/tcp$ firewall-cmd --reload 模拟训练 D: 查看模拟实验 C 中要求加入的端口操作是否成功：1234$ firewall-cmd --zone=public --list-ports8080-8081/tcp$ firewall-cmd --permanent --zone=public --list-ports8080-8081/tcp 模拟实验 E: 将 eno16777728 网卡的区域修改为 external，重启后生效：1234$ firewall-cmd --permanent --zone=external --change-interface=eno16777728success$ firewall-cmd --get-zone-of-interface=eno16777728public 端口转发功能可以将原本到某端口的数据包转发到其他端口:1firewall-cmd --permanent --zone=&lt;区域&gt; --add-forward-port=port=&lt;源端口号&gt;:proto=&lt;协议&gt;:toport=&lt;目标端口号&gt;:toaddr=&lt;目标 IP 地址&gt; 将访问 192.168.10.10 主机 888 端口的请求转发至 22 端口：12$ firewall-cmd --permanent --zone=public --add-forward-port=port=888:proto=tcp:toport=22:toaddr=192.168.10.10success 使用客户机的 ssh 命令访问 192.168.10.10 主机的 888 端口：1234567$ ssh -p 888 192.168.10.10The authenticity of host '[192.168.10.10]:888 ([192.168.10.10]:888)' can't be established.ECDSA key fingerprint is b8:25:88:89:5c:05:b6:dd:ef:76:63:ff:1a:54:02:1a.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '[192.168.10.10]:888' (ECDSA) to the list of known hosts.root@192.168.10.10's password:Last login: Sun Jul 19 21:43:48 2015 from 192.168.10.10 再次提示: 请读者们再仔细琢磨下立即生效与重启后依然生效的差别，千万不要修改错了。 模拟实验 F: 设置富规则，拒绝 192.168.10.0/24 网段的用户访问 ssh 服务：firewalld 服务的富规则用于对服务、端口、协议进行更详细的配置，规则的优先级最高。 12$ firewall-cmd --permanent --zone=public --add-rich-rule="rule family="ipv4"source address="192.168.10.0/24"service name="ssh"reject"success 图形化工具 firewall-config…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之文本过滤 grep]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-grep%2F</url>
      <content type="text"><![CDATA[在 linux 中经常需要对文本或输出内容进行过滤，最常用的过滤命令是 grep 1grep [OPTIONS] PATTERN [FILE...] grep 按行检索输入的每一行，如果输入行包含模式 PATTERN，则输出这一行。这里的 PATTERN 是正则表达式 (参考前一篇，本文将结合 grep 一同举例)。 输出文件 /etc/passwd 中包含 root 的行： 123$ grep root /etc/passwdroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 或者从标准输入获得： 123$ cat /etc/passwd | grep rootroot:x:0:0:root:/root:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologin 需要注意的地方是：当 grep 的输入既来自文件也来自标准输入时，grep 将忽略标准输入的内容不做处理，除非使用符号 - 来代表标准输入： 12345$ cat /etc/passwd | grep root /etc/passwd -/etc/passwd:root:x:0:0:root:/root:/bin/bash/etc/passwd:operator:x:11:0:operator:/root:/sbin/nologin(标准输入):root:x:0:0:root:/root:/bin/bash(标准输入):operator:x:11:0:operator:/root:/sbin/nologin 此时，grep 会标明哪些结果来自于文件哪些来自于标准输入。 输出文件 /etc/passwd 和文件 / etc/group 中以 root 开头的行： 123$ grep "^root" /etc/passwd /etc/group/etc/passwd:root:x:0:0:root:/root:/bin/bash/etc/group:root:x:0: 输出文件 /etc/passwd 中以 / bin/bash 结尾的行： 123$ grep "/bin/bash$" /etc/passwdroot:x:0:0:root:/root:/bin/bashlearner:x:1000:1000::/home/learner:/bin/bash 注意以上两个例子中 PATTERN 被双引号引用起来以防止被 shell 解析。 输出文件 / etc/passwd 中不以 a-s 中任何一个字母开头的行： 123$ grep "^[^a-s]" /etc/passwdtss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologintcpdump:x:72:72::/:/sbin/nologin 这里需要理解两个 ^ 间不同的含义，第一个 ^ 表示行首，第二个在 [] 内部的首个字符 ^ 表示取反。 输出文件 / etc/passwd 中字符 0 连续出现 3 次及以上的行 (注意转义字符’\’)： 12$ grep "0\&#123;3,\&#125;" /etc/passwdlearner:x:1000:1000::/home/learner:/bin/bash 如输出文件 /etc/passwd 中以字符 r 或 l 开头的行： 1234$ grep "^[r,l]" /etc/passwdroot:x:0:0:root:/root:/bin/bashlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinlearner:x:1000:1000::/home/learner:/bin/bash 选项 -i 使 grep 在匹配模式时忽略大小写： 123$ grep -i abcd fileABCDfunction abcd() &#123; 选项 -o 表示只输出匹配的字符，而不是整行： 123$ grep -oi abcd fileABCDabcd 选项 -c 统计匹配的行数： 12$ grep -oic abcd file2 选项 -v 表示取反匹配，如输出 / etc/passwd 中不以 / sbin/nologin 结尾的行： 123456$ grep -v "/sbin/nologin$" /etc/passwdroot:x:0:0:root:/root:/bin/bashsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltlearner:x:1000:1000::/home/learner:/bin/bash 选项 -f FILE 表示以文件 FILE 中的每一行作为模式匹配： 123456$ cat testabcdABCD$ grep -f test fileABCDfunction abcd() &#123; 选项 -x 表示整行匹配： 12$ grep -xf test fileABCD 选项 -w 表示匹配整个单词： 12345$ grep here fileherethere$ grep -w here filehere 选项 -h 表示当多个文件时不输出文件名： 123$ cat /etc/passwd|grep ^root - /etc/passwd -hroot:x:0:0:root:/root:/bin/bashroot:x:0:0:root:/root:/bin/bash 选项 -n 表示显示行号： 1234$ grep -n "^[r,l]" /etc/passwd1:root:x:0:0:root:/root:/bin/bash5:lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin24:learner:x:1000:1000::/home/learner:/bin/bash 选项 -A N、-B N、-C N 表示输出匹配行和其’周围行’ 123456789101112131415-A N 表示输出匹配行和其之后 (after) 的 N 行-B N 表示输出匹配行和其之前 (before) 的 N 行-C N 表示输出匹配行和其之前之后各 N 行$ grep -A 2 ^operator /etc/passwdoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologin$ grep -B2 ^operator /etc/passwd halt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologin$ grep -C1 ^operator /etc/passwd mail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologin 选项 -F 视 PATTERN 为它的字面意思匹配 (忽略字符的特殊含义)，等同于执行命令 fgrep： 1$ grep -F ^root /etc/passwd 命令无输出 选项 -E 可以使用扩展的正则表达式，如同执行 egrep 命令： 123$ egrep "^root|^learner" /etc/passwdroot:x:0:0:root:/root:/bin/bashlearner:x:1000:1000::/home/learner:/bin/bash 使用扩展正则表达式意味着不需要转义就能表示字符的特殊含义，包括?,+,{,|,(和)。 选项 -P 表示使用 perl 的正则表达式进行匹配如： 12$ echo "helloworld123456"| grep -oP "\d+"123456 perl 正则中 “\d” 表示数字，+ 表示匹配一到多次 (同 vim)。 选项 -a 将二进制文件当成文本文件处理： 12$ grep -a online /usr/bin/ls%s online help: &lt;%s&gt; 选项 --exclude=GLOB 和 --include=GLOB 分别表示排除和包含匹配 GLOB 的文件，GLOB 表示通配符 (find 及 xargs 用法见基础命令介绍三)： 12$ find . -type f | xargs grep --exclude=*.txt --include=test* bash./test.sh:#!/bin/bash grep 强大的过滤能力来自于各种选项以及正则表达式的配合，在今后的文章中还有更多的例子。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之网络传输与安全]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-network-transmission-and-security%2F</url>
      <content type="text"><![CDATA[本篇接着介绍网络相关命令 1、wget 文件下载工具1wget [option]... [URL]... wget 是一个非交互的下载器，支持 HTTP, HTTPS 和 FTP 协议，也可以使用代理。所谓’非交互’意思是说，可以在一个系统中启动一个 wget 下载任务，然后退出系统，wget 会在完成下载 (或出现异常) 之后才退出，不需要用户参与。 1234567891011[root@centos7 temp]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-6/v6.0.47/bin/apache-tomcat-6.0.47.tar.gz--2016-11-15 12:16:24-- http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-6/v6.0.47/bin/apache-tomcat-6.0.47.tar.gz正在解析主机 mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 166.111.206.63, 2402:f000:1:416:166:111:206:63正在连接 mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|166.111.206.63|:80... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：7084545 (6.8M) [application/octet-stream]正在保存至: “apache-tomcat-6.0.47.tar.gz”100%[===========================================================&gt;] 7,084,545 2.28MB/s 用时 3.0s 2016-11-15 12:16:27 (2.28 MB/s) - 已保存 “apache-tomcat-6.0.47.tar.gz” [7084545/7084545]) 命令的执行会经过域名解析、建立连接、发送请求、保存文件等过程，wget 还会显示下载进度条，包括下载百分比、大小、速度、用时。下载完成后显示完成时间、保存文件名、下载大小 / 总大小。 选项 -q 表示禁止输出 选项 -b 表示后台执行 选项 -r 表示递归下载 选项 -o logfile 表示将输出保存到文件 logfile 中 选项 -i file 表示从 file 中读取 URL 并进行下载 选项 -O file 表示下载文件保存至 file 选项 -c 断点续传，当下载一个大文件时使用此选项，如果碰到网络故障，可以从已经下载的部分开始继续下载未完成的部分。 选项 –limit-rate=amount 下载限速，amount 可以以 k,m 等为后缀表示速率为 KB/s 和 MB/s。 选项 –user-agent 指定用户代理 选项 –user 和选项 –password 指定用户和密码 选项 –load-cookies file 和选项 –save-cookies file 分别表示使用和保存文件中的 cookies。 选项 –accept list 和选项 –reject list 表示接受或排除 list 中所列文件。list 中用逗号分隔每个文件名的后缀。注意如果 list 中包含 shell 通配符 (* ? […])，将作为一个模式匹配，而不是文件后缀名。 2、curl 网络数据传输工具1curl [options] [URL...] curl 同样也可以做为文件下载工具，和 wget 相比，curl 支持更多的协议，在指定下载 URL 时支持序列或集合。但 curl 不支持递归下载。 curl 的 URL 可以表示成如下格式： 1234567891011#可以将几个个字符串放到大括号里用逗号分隔来表示多个 URLhttp://site.&#123;one,two,three&#125;.com#可以将字母数字序列放在 [] 中表示多个文件或 URL(和 shell 通配符类似但并不相同)ftp://ftp.numericals.com/file[1-100].txtftp://ftp.numericals.com/file[001-100].txtftp://ftp.letters.com/file[a-z].txt#还能用冒号: n 表示在序列中每隔 n 个取一个值http://www.numericals.com/file[1-100:10].txthttp://www.letters.com/file[a-z:2].txt#不支持大括号和中括号的嵌套，但可以在一条 URL 中分开同时使用它们http://any.org/archive[1996-1999]/vol[1-4]/part&#123;a,b,c&#125;.html 选项 -C offset 表示从断点 (offset) 的位置继续传输，其中 offset 是个数字，单位为 bytes。使用 - C - 时，curl 会自动在给定的文件中找出断点。 选项 -o file 表示下载文件保存至 file(注意 wget 使用的是 - O) 选项 -O 表示保存为文件的原始名字 选项 -s 忽略下载进度显示 选项 –limit-rate speed 指定下载速度，默认单位为 bytes/s，可以使用 k/K,m/M,g/G 后缀。 还可以指定许多其他下载相关的选项，这里不再一一介绍。 当 curl 没有其他选项时，会将页面内容输出至标准输出。 选项 -I 表示只获得 HTTP 头信息： 123456789101112[root@centos7 ~]# curl -I www.baidu.comHTTP/1.1 200 OKServer: bfe/1.0.8.18Date: Tue, 15 Nov 2016 07:20:50 GMTContent-Type: text/htmlContent-Length: 277Last-Modified: Mon, 13 Jun 2016 02:50:02 GMTConnection: Keep-AliveETag: "575e1f5a-115"Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transformPragma: no-cacheAccept-Ranges: bytes 选项 -w format 按格式输出。 12345678#如获得 HTTP 状态码：[root@centos7 ~]# curl -s -w "%&#123;http_code&#125;\n" www.baidu.com -o /dev/null200[root@centos7 ~]##如获得服务器端 IP 地址：[root@centos7 ~]# curl -s -w "%&#123;remote_ip&#125;\n" www.baidu.com -o /dev/null 61.135.169.125[root@centos7 ~]# 选项 -X METHOD 指定 http 请求方法 选项 -L 当指定的 URL 被重定向时 (http 状态码为 3xx)，使用 - L 会使 curl 重新发送请求至新地址。 选项 -d 指定发送数据 这些选项在操作一个远程 http API 时会很有用 12345678910#删除 peer2curl -L -XDELETE http://127.0.0.1:2380/v2/admin/machines/peer2#用 PUT 方法发送给指定 URL 数据curl -L http://127.0.0.1:2379/v2/keys/message -XPUT -d 'value="Hello world"'#指定数据可以是 JSON 格式的字符串curl -L http://127.0.0.1:2380/v2/admin/config -XPUT -d '&#123;"activeSize":3,"removeDelay":1800,"syncInterval":5&#125;'选项 -T file 表示上传文件 filecurl -T test.sql ftp://name:password@ip:port/demo/curtain/bbstudy_files/#注意这里是如何指定 ftp 用户、密码、IP、端口的；也可以使用选项 - u user:password 指定用户和密码 3、rsync 文件传输工具rsync 的初衷是为了取代 scp，作为一个更快速，功能更强的文件传输工具。它使用 “rsync” 算法，可以实现每次只传输两个文件的不同部分(即增量备份)。 123456789101112rsync [OPTION...] SRC... [DEST]#类似于 cp，本地传输。当目的 (DEST) 省略时，会以 `ls -l` 的风格列出源文件列表[root@centos7 temp]# rsync .drwxr-xr-x 102 2016/11/16 09:47:10 .-rw-r--r-- 0 2016/11/10 22:02:25 b.txt-rw-r--r-- 0 2016/11/10 22:02:25 c.txt-rw-r--r-- 0 2016/11/10 22:02:25 d.txt-rw-r--r-- 0 2016/11/10 22:02:25 e.txt-rw-r--r-- 0 2016/11/10 22:02:25 f.txt-rw-r--r-- 1979 2016/11/08 15:49:31 file-rw-r--r-- 10 2016/11/07 18:01:33 test-rwxr-xr-x 24 2016/11/04 09:03:18 test.sh rsync 在本地和远程之间传输文件有两种工作模式，一种是利用 ssh 加密传输，类似于 scp；一种是守护进程 (daemon) 模式，使用命令 rsync –daemon 启动，作为 rsync 服务器为客户端服务。 12345678#通过 sshrsync [OPTION...] [USER@]HOST:SRC... [DEST]rsync [OPTION...] SRC... [USER@]HOST:DEST#通过 daemonrsync [OPTION...] [USER@]HOST::SRC... [DEST]rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]rsync [OPTION...] SRC... [USER@]HOST::DESTrsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 选项 -r 表示递归 选项 -v 表示显示详细信息 选项 -a 表示保持文件所有属性并且递归地传输文件 如使用 ssh 将本地 /root/temp 目录及其内容同步至 10.0.1.253 的 /root/temp： 12345678910111213141516#注意源和目的主机都需要有 rsync 命令[root@centos7 temp]# rsync -av . root@10.0.1.253:/root/tempsending incremental file listcreated directory /root/temp./b.txtc.txtd.txte.txtf.txtfiletesttest.shsent 2468 bytes received 167 bytes 5270.00 bytes/sectotal size is 2013 speedup is 0.76 命令的执行开始会在源端 (此例中的本机：发送端) 创建文件列表(file list)，在创建的过程中会将文件列表发送至目的端(此例中的 10.0.1.253：接收端)。发送完成之后，接收端对文件列表进行计算处理，保留接收端不存在的或变化的文件，创建新文件列表，然后发送回源端；发送端收到新文件列表后开始进行传输。 返回结果中显示了发送的文件以及一些汇总信息。 如执行完上述命令后更新其中一个文件，然后再次执行同步： 1234567[root@centos7 temp]# echo "hello world" &gt;&gt; d.txt[root@centos7 temp]# rsync -av . root@10.0.1.253:/root/tempsending incremental file listd.txtsent 193 bytes received 31 bytes 448.00 bytes/sectotal size is 2025 speedup is 9.04 这次只有变化了的文件才被传输。 选项 –delete 会将接收端存在但发送端不存在的文件删除： 12345678[root@centos7 temp]# rm -f test[root@centos7 temp]# rsync -av --delete . root@10.0.1.253:/root/tempsending incremental file list./deleting test #这里删除了接收端的 test 文件sent 132 bytes received 15 bytes 98.00 bytes/sectotal size is 2015 speedup is 13.71 选项 –exclude=PATTERN 排除符合模式 PATTERN 的文件不传输 (同 tar 命令，例子见这里) 选项 –exclude-from=FILE 排除符合文件 FILE 内模式 (一行一个 PATTERN) 的文件不传输 选项 –include=PATTERN 和 –include-from=FILE 同理，表示包含某模式的文件才被传输 选项 -z 表示将文件压缩之后再传输。(即使使用此选项，有些文件默认时也不会被压缩，如某些 gz jpg mp4 avi zip 等结尾的文件) 默认时，rsync 会将部分传输的文件 (如连接被中断导致文件没有传输完) 删除。 选项 –partial 会保留这些部分传输的文件 选项 –progress 会打印出每个文件传输的状态信息，类似于： 1782448 63% 110.64kB/s 0:00:04 #这里文件已被传输了 63% 选项 -P 等同于选项 –partial 和 –progress。 当使用 daemon 模式时，服务端使用默认配置文件 /etc/rsyncd.conf 和密码文件 /etc/rsyncd.secrets(可选)。(如不存在可手动创建)配置文件的格式： 123456/etc/rsyncd.conf 的内容由两部分组成，模块 (modules) 和参数(parameters)；模块以中括号包含模块名 (`[modul]`) 为开头一直到下一个模块开头之前。模块包含形如 "name = value" 的多个参数。文件中以符号 #开头的行是注释行，起描述性作用，没有实际效果。文件是基于行的。意思是说每一行表示一条注释或者模块开头或者一个参数，多个参数的话，只有第一个起作用。在第一个模块之前的参数会作为全局参数，作为默认值适用于每个模块。 举例说明如下： 12345678910111213141516171819202122[root@idc-v-71253 temp]# cat /etc/rsyncd.conf # /etc/rsyncd: configuration file for rsync daemon mode# 注释行# global parametersuid = nobody #指定传输文件时守护进程应该具有的 uidgid = nobody #指定传输文件时守护进程应该具有的 giduse chroot = true #在传输之前会 chroot 到该模块 path 参数所指定的目录max connections = 4 #最大并发连接数量pid file = /var/run/rsyncd.pid #指定 rsync 的 pid 文件timeout = 900 #指定超时时间，单位是秒read only = false #允许客户端上载文件到服务端 (默认为 true, 禁止上传)。dont compress = *.gz *.tgz *.zip *.z *.bz2 #指定特定后缀名的文件在传输之前不被压缩#modules[temp] #模块 path = /home/temp #服务端该模块可用目录，每个模块都必须指定此参数 comment = test for command rsync(daemon) #描述字符串[cvs] path = /data/cvs comment = CVS repository (requires authentication) auth users = tridge, susan #允许连接到此模块的用户，这里的用户和系统用户没关系。 secrets file = /etc/rsyncd.secrets #前面参数 “auth users” 所使用的密码文件 我们在 10.0.1.253 这台机器上的配置文件中写入了上述内容，然后把它作为 rsync 服务端启动起来： 123456789101112[root@idc-v-71253 temp]# rsync --daemon[root@idc-v-71253 temp]# ls -l /var/run/rsyncd.pid-rw-r--r-- 1 root root 6 11 月 16 14:03 /var/run/rsyncd.pid#这里看到新创建的 pid 文件[root@idc-v-71253 log]# cat /var/run/rsyncd.pid29623#默认守护进程模式的 rsync 服务端会通过系统的 syslog(一个系统服务) 记录日志，保存于 /var/log/messages 中[root@idc-v-71253 log]# tail -1 /var/log/messagesNov 16 14:03:44 idc-v-71253 rsyncd[29623]: rsyncd version 3.0.9 starting, listening on port 873#这里看到 rsyncd 已经启动了，监听端口 873[root@idc-v-71253 log]# chown -R nobody.nobody /root/temp#改变模块中 path 所指定的目录的权限以使它和全局参数 uid，gid 一致 然后，我们就可以使用 rsync 服务器来传输文件了。注意服务端防火墙允许对 TCP 873 端口的连接，本文后面有对防火墙的描述。如在 10.0.1.254 上拉取 (pull)： 1234567891011121314151617181920212223[root@centos7 temp]# lsb.txt c.txt d.txt e.txt file f.txt test.sh[root@centos7 temp]# rm -rf *[root@centos7 temp]# rsync -avP --delete 10.0.1.253::temp ./ #注意书写格式与使用 ssh 时的不同receiving incremental file list./b.txt 13 100% 12.70kB/s 0:00:00 (xfer#1, to-check=6/8)c.txt 0 100% 0.00kB/s 0:00:00 (xfer#2, to-check=5/8)d.txt 12 100% 11.72kB/s 0:00:00 (xfer#3, to-check=4/8)e.txt 0 100% 0.00kB/s 0:00:00 (xfer#4, to-check=3/8)f.txt 0 100% 0.00kB/s 0:00:00 (xfer#5, to-check=2/8)file 1979 100% 1.89MB/s 0:00:00 (xfer#6, to-check=1/8)test.sh 24 100% 23.44kB/s 0:00:00 (xfer#7, to-check=0/8)sent 162 bytes received 2476 bytes 5276.00 bytes/sectotal size is 2028 speedup is 0.77 或者推送 (push)： 123456789101112[root@centos7 temp]# echo 'BLOG ADDRESS IS"https://segmentfault.com/learnning"' &gt;&gt; c.txt [root@centos7 temp]# rm -f file[root@centos7 temp]# rsync -avP --delete . rsync://10.0.1.253/temp #注意格式sending incremental file list./deleting filec.txt 58 100% 0.00kB/s 0:00:00 (xfer#1, to-check=4/7)sent 235 bytes received 30 bytes 530.00 bytes/sectotal size is 107 speedup is 0.40[root@centos7 temp]# 根据配置文件，当同步 cvs 模块时需要对用户进行认证 在服务器端 (10.0.1.253)： 123456#编辑密码文件写入所示内容[root@idc-v-71253 cvs]# vim /etc/rsyncd.secretstridge:123456susan:654321#还需要改变文件权限[root@idc-v-71253 cvs]# chmod 600 /etc/rsyncd.secrets 在客户端 (10.0.1.254)： 12345[root@centos7 temp]# touch /etc/tridge.pass[root@centos7 temp]# echo 123456 &gt; /etc/tridge.pass[root@centos7 temp]# touch /etc/susan.pass[root@centos7 temp]# echo 654321 &gt; /etc/susan.pass[root@centos7 temp]# chmod 600 /etc/tridge.pass /etc/susan.pass 客户端同步时需要使用选项 –password-file 指定所用密码文件 PULL： 1234567891011121314[root@centos7 temp]# rsync -avP --delete --password-file=/etc/tridge.pass rsync://tridge@10.0.1.253/cvs /data/cvs #注意格式receiving incremental file listA/a.txt 20 100% 19.53kB/s 0:00:00 (xfer#1, to-check=675/703)A/b.txt 20 100% 6.51kB/s 0:00:00 (xfer#2, to-check=674/703).... #省略部分输出Z/y.txt 78 100% 1.27kB/s 0:00:00 (xfer#675, to-check=1/703)Z/z.txt 78 100% 1.27kB/s 0:00:00 (xfer#676, to-check=0/703)sent 16981 bytes received 71532 bytes 1416.21 bytes/sectotal size is 34632 speedup is 0.39 PUSH： 123456789101112131415161718192021222324[root@centos7 temp]# echo "baby on the way..." | tee -a /data/cvs/A/*baby on the way...[root@centos7 temp]# rm -rf /data/cvs/B[root@centos7 temp]# rsync -avP --delete --password-file=/etc/susan.pass /data/cvs/ susan@10.0.1.253::cvssending incremental file list./deleting B/z.txtdeleting B/y.txtdeleting B/x.txt....deleting B/a.txtdeleting B/A/a.txt 55 100% 0.00kB/s 0:00:00 (xfer#1, to-check=675/703)A/b.txt 55 100% 53.71kB/s 0:00:00 (xfer#2, to-check=674/703)....A/y.txt 55 100% 53.71kB/s 0:00:00 (xfer#25, to-check=651/703)A/z.txt 55 100% 53.71kB/s 0:00:00 (xfer#26, to-check=650/703)sent 10331 bytes received 684 bytes 22030.00 bytes/sectotal size is 35542 speedup is 3.23 要注意上例中源目录的书写，在 rsync 中如果源目录不以 / 结尾，意味着将在目的目录下创建子目录，如： 12rsync -avz foo:src/bar /data/tmp#此时会将源目录 src/bar 内所有的内容传送至目标 /data/tmp/bar 内 可以在源目录结尾增加 / 来阻止这一行为： 12rsync -avz foo:src/bar/ /data/tmp#此时会将源目录 src/bar 内所有的内容传送至目标 / data/tmp 内，不会创建子目录 bar 配置文件中还可以设置其他参数如设置监听端口、指定日志文件、指定允许客户端列表等等，可使用命令 man rsyncd.conf 自行查看。 4、iptables 防火墙设置 (注：基于 linux2.6 内核)iptables 通过定义一系列的规则利用内核的 netfilter 对每个网络包进行过滤。用户可以定义多种规则，实现对系统的防护。首先我们先看一下一个网络数据包是怎样在系统中流转的，再来说明 netfilter 在哪些位置起作用： 1234567891011121314151617#入站1）数据包从网络到达网卡，网卡接收帧，放入网卡 buffer 中，并向系统发送中断请求。2）cpu 调用网卡驱动程序中相应的中断处理函数，将 buffer 中的数据读入内存。3）链路层对帧进行 CRC 校验，正常则将其放入自己的队列，置软中断标志位。4）进程调度器看到了标志位，调度相应进程，该进程将包从队列取出，与相应协议匹配，一般为 ip 协议，再将包传递给该协议接收函数。5）网络层对包进行错误检测，没错的话，进行路由选择。6）此时的路由操作将包分为两类，一类是本地包，继续交给传输层处理；一类是转发包，将会到达出站的第 5 步，路由选择之后。7）传输层收到报文段后将进行校验，校验通过后查找相应端口关联 socket，数据被放入相应 socket 接收队列8）socket 唤醒拥有该 socket 的进程，进程从系统调用 read 中返回，将数据拷贝到自己的 buffer。然后进行相应的处理。#出站1）应用程序调用系统调用，将数据发送给 socket。2）socket 检查数据类型，调用相应的 send 函数。3）send 函数检查 socket 状态、协议类型，传给传输层。4）传输层为这些数据创建数据结构，加入协议头部，比如端口号、检验和，传给网络层。5）ip（网络层协议）添加 ip 头，对包进行路由选择，然后将包传给链路层。6）链路层将包组装成帧，发送至至网卡的 send 队列。7）网卡将帧组织成二进制比特流发送至物理媒体上 (网线)。 netfilter 在 5 个位置放置了关卡 PREROUTING (入站网络层错误检测之后，路由选择之前) INPUT (入站路由选择后，交给传输层处理之前) FORWARD (入站路由选择后，进行转发之前；然后到达 POSTROUTING) OUTPUT (出站路由选择之前) POSTROUTING (出站路由选择之后) 这 5 个位置即对应了 iptables 的 5 个规则链, 如图所示： 对于如何处理数据包，iptables 还定义了如下 4 张不同功能的表： raw 决定数据包是否被状态跟踪机制处理，可以作用的位置：OUTPUT、PREROUTING mangle 修改数据包的服务类型、TTL、并且可以配置路由实现 QOS，可以作用的位置：PREROUTING、POSTROUTING、INPUT、OUTPUT、FORWARD nat 用于网络地址转换，可以作用的位置：PREROUTING、POSTROUTING、OUTPUT filter 过滤数据包，可以作用的位置：INPUT、FORWARD、OUTPUT 同一位置的不同表处理的优先级为 raw-&gt;mangle-&gt;nat-&gt;filter，但各表的使用频度正好相反，filter 表最常用 (也是 iptables 不使用选项 -t 指定表时的默认表)，raw 表极少使用。 12345678#语法iptables [-t table] COMMAND chain rule-specification-t table 指定表，省略时表示 filter 表COMMAND 定义如何对规则进行管理chain 指定规则生效的位置 (规则链)rule-specification = [matches...] [target] 特定规则，包括匹配和目标match = -m matchname [per-match-options] 匹配target = -j targetname [per-target-options] 目标 netfilter 在处理数据包时，会对照 iptables 指定的规则从上至下逐条进行匹配，如果符合某一条规则，就按这条规则的 ACTION 进行处理，这个表 (table) 后面的所有规则都将不会再对此包起作用。如果本表中所有的规则都没有匹配上，则进行默认的策略处理。(注意：同样的表可以作用于不同的链&lt;位置&gt;，不同的位置又可以有多张表。在定义规则或跟踪数据包在防火墙内的流动时，一定要清楚的知道当前数据包在哪个位置、进入了哪张表、匹配到表中相应规则链的哪条语句) COMMAND 选项： 1234567891011-A 追加规则 (尾部)。-D 删除规则 (后面可以是规则描述或者规则号 &lt; 第几条&gt;)-I 插入规则 (可以指定在第几条之后插入)-R 替换规则-L 列出规则-F 清除规则-Z 清空匹配统计-N 创建自定义链-X 删除自定义链 (链必须为空且没有其它链指向此链)-P 指定链默认策略-E 重命名链 规则选项： 12345678910-p 指定协议-s 指定源 (可以是 ip 地址，ip 网段，主机名)-d 指定目的 (同 -s)-j target 跳转到目标，目标可以是：用户自定义链；特殊内建目标 (DROP,ACCEPT 等)；扩展 (EXTENSIONS)-g chain 使数据包到指定自定义链中处理，完成后继续在上一次由 -j 跳转到本链的位置处继续处理-i name 指定入站网卡名-o name 指定出站网卡名-v 显示详细信息-n 数字化输出 (域名等显示为 IP)--line-numbers 显示行号 target 12345ACCEPT 表示允许包通过DROP 表示丢弃该包RETURN 表示停止执行当前链后续规则，返回到调用链中QUEUE 将数据包移交到用户空间EXTENSIONS 包含两种，一种是 target 扩展，表示对数据包做某种处理；一种是使用选项 -m 构成的匹配扩展，表示指定某种匹配方式。 target 扩展 DNAT 对数据包进行目的地址转换，接受选项 –to-destination(只能用于 nat 表，PREROUTING 和 OUTPUT 链) SNAT 对数据包进行源地址转换，接受选项 –to-source(只能用于 nat 表，POSTROUTING 和 INPUT 链)如 1234#将目的地址为 221.226.x.x，目的端口为 80 的数据包做 DNAT，使目的地址为 192.168.5.16，目的端口为 80iptables -t nat -A PREROUTING -p tcp -i eth1 -d 221.226.x.x --dport 80 -j DNAT --to-destination 192.168.5.16:80#将源地址为 192.168.5.16，源端口为 80 的数据包做 SNAT，使源地址变为 221.226.x.xiptables -t nat -A POSTROUTING -p tcp -o eth1 -s 192.168.5.16 --sport 80 -j SNAT --to-source 221.226.x.x LOG 对匹配包进行日志记录 REJECT 同 DROP 一样丢弃包，但返回错误信息。(只能用于 INPUT、FORWARD 和 OUTPUT 链) REDIRECT 重定向匹配包 (只能用于 nat 表，PREROUTING 和 OUTPUT 链) 12#将目标端口 8888 的重定向至本机 443 端口iptables -t nat -A PREROUTING -p tcp --dport 8888 -j REDIRECT --to 443 匹配扩展 icmp 匹配 icmp 协议，接受选项 –icmp-type 指定 icmp 类型 1iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT tcp 匹配 tcp 协议 udp 匹配 udp 协议 connlimit 连接限制 12#限制每个 C 段 IP http 最大并发连接数为 16iptables -p tcp --syn --dport 80 -m connlimit --connlimit-above 16 --connlimit-mask 24 -j REJECT limit 限速 1234567#创建自定义链 SYNFLOODiptables -N SYNFLOOD#没有超过限定值的话返回iptables -A SYNFLOOD -m limit --limit 10/s --limit-burst 20 -j RETURN#超过限定值, 就视为 SYNFLOOD 攻击, 记录日志并丢弃iptables -A SYNFLOOD -m limit --limit 1/s --limit-burst 10 -j LOG --log-level=1 --log-prefix "SYNFLOOD:"iptables -A SYNFLOOD -j DROP multiport 多端口 12#允许转发至多个 TCP 端口iptables -A FORWARD -p tcp -m multiport --dport 135,137,138,139,445 -j ACCEPT state 状态匹配 1234#允许从端口 eth1 进入的状态是 ESTABLISHED 和 RELATED 的转发包iptables -A FORWARD -i eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT#允许 http 新建连接iptables -A INPUT -p tcp --dport 80 -m state --state NEW -j ACCEPT string 字符串匹配 12#对匹配到字符串 GET /index.html 的 http 请求包进行日志记录 (--algo bm 为指定匹配算法)iptables -A INPUT -p tcp --dport 80 -m string --algo bm --string 'GET /index.html' -j LOG time 匹配时间 一些例子： 12345678910111213141516171819202122#清空规则iptables -F#查看 nat 表的所有规则iptables -t nat -nvL#设置 INPUT 链的默认规则iptables -P INPUT DROP#删除转发链中的第二条规则iptables -D FORWARD 2#允许内网 samba,smtp,pop3, 连接iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -A INPUT -p tcp -m multiport --dports 110,25 -j ACCEPTiptables -A INPUT -p tcp -s 192.168.0.0/24 --dport 139 -j ACCEPT#允许 DNS 连接iptables -A INPUT -i eth1 -p udp -m multiport --dports 53 -j ACCEPT#星期一到星期六的 8:15-12:30 禁止 qq 通信iptables -I FORWARD -p udp --dport 53 -m string --string "tencent" -m time --timestart 8:15 --timestop 12:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP#只允许每组 ip 同时 15 个 80 端口转发iptables -A FORWARD -p tcp --syn --dport 80 -m connlimit --connlimit-above 15 --connlimit-mask 24 -j DROP#保存规则到文件iptables-save &gt;/etc/sysconfig/iptables.rule#装载保存在文件中的规则iptables-restore &lt;/etc/sysocnfig/iptables.rule 由于mangle表和raw表很少使用，就没有举相关的例子，另外，如果允许linux主机进行转发(FORWARD)，需要设置内核参数：echo 1 &gt; /proc/sys/net/ipv4/ip_forward(临时)，或sysctl -w net.ipv4.ip_forward=1 &amp;&gt;/dev/null(永久)。iptables的规则定义较复杂，还有许多选项没有在例子中使用到，读者可以自行man。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之网络]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-networking%2F</url>
      <content type="text"><![CDATA[本文将讲述网络相关命令，作者假定读者具备 TCP/IP 协议栈的基础知识。对于相关命令及其输出只介绍它的基本的使用方法和大概的描述，具体协议将不作详细解释。 如今网络无疑是很重要的，linux 系统中提供了丰富的网络测试与管理命令。我们来一起看看它们。 1、ping 发送 TCMP 回显请求报文，并等待返回 TCMP 回显应答。1ping [OPTIONS]... destination 这里的目标 destination 可以是目的 IP 地址或者 域名/主机名 选项 -c 指定发送请求报文的次数，当 ping 没有任何选项时，在 linux 中默认将一直发送请求报文直到手动终止。 123456789$ ping -c 3 www.baidu.comPING www.a.shifen.com (61.135.169.121) 56(84) bytes of data.64 bytes from 61.135.169.121: icmp_seq=1 ttl=52 time=1.35 ms64 bytes from 61.135.169.121: icmp_seq=2 ttl=52 time=1.32 ms64 bytes from 61.135.169.121: icmp_seq=3 ttl=52 time=1.22 ms--- www.a.shifen.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2003msrtt min/avg/max/mdev = 1.225/1.303/1.359/0.064 ms 首先，ping 程序会向域名服务器 (DNS) 发送请求，解析域名 www.baidu.com 的 IP 地址。DNS 返回域名的一个别名 www.a.shifen.com 以及对应的 IP 地址 61.135.169.121。之后 ping 程序开始向这个地址发送请求报文，每 1s 发送一个，ping 收到 TCMP 回显应答并将结果显示在终端上，包括 ICMP 序列号 (icmp_seq)，生存时间(ttl) 和数据包往返时间(time)。最后，给出汇总信息，包括报文总收发情况，总时间，往返时间最小值、平均值、最大值、平均偏差(越大说明网络越不稳定)。 12$ ping www.a.comping: unknown host www.a.com 当目的域名无法解析出 IP 地址时，会报未知主机的错 12345$ ping 192.168.0.1PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data.^C #这里按 CTRL+C 键手动终止了进程--- 192.168.0.1 ping statistics ---6 packets transmitted, 0 received, 100% packet loss, time 4999ms 当目的 IP 地址没有路由时不会收到任何 ICMP 回显报文 12345678$ ping -c2 10.0.1.2PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data.From 10.0.1.254 icmp_seq=1 Destination Host UnreachableFrom 10.0.1.254 icmp_seq=2 Destination Host Unreachable--- 10.0.1.2 ping statistics ---2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 999mspipe 2 当有目的 IP 的路由但无法达到时显示目标不可达错误 (Destination Host Unreachable)。ICMP 回显应答还包括超时 (request time out) 等其他类型。 2、hostname 显示或设置系统主机名1hostname [OPTIONS]... [NAME] 直接执行命令 hostname 时将显示主机名： 123[root@centos7 temp]# hostnamecentos7[root@centos7 temp]# 这个主机名是系统的 gethostname(2) 函数返回的。 可以通过执行命令 hostname NAME 来临时改变主机名： 123[root@centos7 temp]# hostname NAME[root@centos7 temp]# hostnameNAME 这个临时修改实际上是修改了 linux kernel 中一个同为 hostname 的内核参数，它保存在 /proc/sys/kernel/hostname 中。如果需要永久修改则需要修改配置文件 /etc/sysconfig/network，centos 7 中需要修改 /etc/hostname。需要注意的是，如果配置文件中的主机名是 localhost 或 localhost.localdomain 时，系统会取得网络接口的 IP 地址，并用这个地址找出 /etc/hosts 文件中对应的主机名，然后将其设置成最终的 hostname。 3、host DNS 查询1host name host 命令通过配置文件 /etc/resolv.conf 中指定的 DNS 服务器查询 name 的 IP 地址： 1234[root@centos7 temp]# host www.baidu.comwww.baidu.com is an alias for www.a.shifen.com.www.a.shifen.com has address 61.135.169.121www.a.shifen.com has address 61.135.169.125 4、dig DNS 查询dig 和 host 命令的语法一致，但提供了更详细的信息和更多的选项： 12345678910111213141516171819202122$ dig www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-29.el7_2.2 &lt;&lt;&gt;&gt; www.baidu.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 22125;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;www.baidu.com. IN A;; ANSWER SECTION:www.baidu.com. 113 IN CNAME www.a.shifen.com.www.a.shifen.com. 113 IN A 61.135.169.125www.a.shifen.com. 113 IN A 61.135.169.121;; Query time: 2 msec;; SERVER: 223.5.5.5#53(223.5.5.5);; WHEN: 四 11 月 10 12:31:20 CST 2016;; MSG SIZE rcvd: 90$ 如只查询域名的 A 记录并以短格式显示： 12345$ dig www.baidu.com A +shortwww.a.shifen.com.61.135.169.12561.135.169.121$ 或者： 1234$ dig +nocmd www.baidu.com A +noall +answer www.baidu.com. 252 IN CNAME www.a.shifen.com.www.a.shifen.com. 252 IN A 61.135.169.125www.a.shifen.com. 252 IN A 61.135.169.121 还可以用 @server 的方式指定 DNS 服务器： 1234$ dig +noall +answer www.baidu.com A @8.8.8.8www.baidu.com. 21 IN CNAME www.a.shifen.com.www.a.shifen.com. 263 IN A 61.135.169.125www.a.shifen.com. 263 IN A 61.135.169.121 更多的命令及选项请自行 man 5、traceroute 或 tracepath 路由跟踪123456789101112$ tracepath www.baidu.com 1?: [LOCALHOST] pmtu 1500 1: 10.0.1.103 0.396ms 1: 10.0.1.103 0.350ms 2: 210.51.161.1 1.187ms asymm 3 3: 210.51.161.1 8.186ms 4: 210.51.175.81 1.117ms 5: 61.148.142.61 8.554ms asymm 12 6: 61.148.147.13 1.694ms asymm 12 7: 123.126.8.117 3.934ms asymm 10 8: 61.148.155.46 2.703ms asymm 10 ... 这里只列出部分输出，表示跟踪到目的地址的路由，每一跳都返回。 6、ifconfig 配置网络接口当命令没有任何参数时显示所有网络接口的信息： 123456789101112131415161718192021222324252627282930$ ifconfigens32: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.20.71.254 netmask 255.255.255.0 broadcast 172.20.71.255 inet6 fe80::250:56ff:fea4:fe34 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:a4:fe:34 txqueuelen 1000 (Ethernet) RX packets 11996157 bytes 775368588 (739.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 12 bytes 888 (888.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.1.254 netmask 255.255.255.0 broadcast 10.0.1.255 inet6 fe80::250:56ff:fea4:a09 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:a4:0a:09 txqueuelen 1000 (Ethernet) RX packets 20941185 bytes 1307830447 (1.2 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 147552 bytes 11833605 (11.2 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0$ 本例中显示了两个网卡 ens32 和 ens33 以及环回口 lo 的信息，包括 mtu，ip 地址，掩码，mac 地址，传输和接收数据量等等。 选项 -s 显示精简的信息： 123[root@idc-v-71253 ~]# ifconfig -s ens32Iface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgens32 1500 11996951 0 0 0 12 0 0 0 BMRU 如给 ens33 增加一个新地址 10.0.1.4： 12345$ ifconfig ens33:0 10.0.1.4/24 up$ ifconfig ens33:0 ens33:0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.1.4 netmask 255.255.255.0 broadcast 10.0.1.255 ether 00:50:56:a4:0a:09 txqueuelen 1000 (Ethernet) 命令中 /24 表明接口地址的掩码，up 表示启用此接口。注意如果 ip 地址已经被使用，这里依然会被设置成功，但此地址被访问时，可能会有冲突。 停用某接口： 1$ ifconfig ens33:0 down 如果需要永久增加或修改当前接口的地址，最好直接编辑网卡配置文件 /etc/sysconfig/network-scripts/ifcfg-ens33(其他系统换成相应文件) 中 IPADDR 字段，然后重启网络 systemctl restart network 或 service network restart 生效。 7、arp 和 arping命令 arp 显示系统的 arp 缓存，命令 arping 给邻居主机发送 ARP 请求。 123456[root@idc-v-71253 ~]# arp -a? (10.0.1.1) at 68:8f:84:01:f1:ff [ether] on ens33? (10.0.1.102) at 00:50:56:a4:18:9a [ether] on ens33? (10.0.1.254) at 00:50:56:a4:a9:16 [ether] on ens33? (10.0.1.10) at 00:50:56:a4:d2:e4 [ether] on ens33? (10.0.1.104) at 00:50:56:a4:37:a7 [ether] on ens33 ? 表示未知域名，最后的网卡名表示 arp 表项对应的网络接口 如发现某地址不稳定，可以使用 arping 测试该地址是否为 MAC 地址冲突： 1234$ arping 10.0.1.252 -I ens33ARPING 10.0.1.252 from 10.0.1.254 ens33Unicast reply from 10.0.1.252 [00:50:56:A4:65:71] 0.843msUnicast reply from 10.0.1.252 [00:50:56:A4:0A:09] 1.034ms 这里两条返回信息中的 MAC 地址不同，说明有两块网卡配置了相同的 IP 地址。选项 -I 指定发送 arp 请求的网络接口。 如果刚刚更改了网卡的 IP 地址，但上游设备 (如交换机) 的 arp 表项还是老的，可以使用 arping 来强制刷新： 1234567$ arping -c3 -I ens33 -s 10.0.1.254 10.0.1.1ARPING 10.0.1.1 from 10.0.1.254 ens33Unicast reply from 10.0.1.1 [68:8F:84:01:F1:FF] 19.466msUnicast reply from 10.0.1.1 [68:8F:84:01:F1:FF] 2.358msUnicast reply from 10.0.1.1 [68:8F:84:01:F1:FF] 24.305msSent 3 probes (1 broadcast(s))Received 3 response(s) -c 指定发送 arp 请求次数，-s 指定源地址，最后的 IP 表示发送目标 (这里是网关地址)。 8、route 显示或更改路由表12345678$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.1.0 0.0.0.0 255.255.255.0 U 0 0 0 ens33link-local 0.0.0.0 255.255.0.0 U 1002 0 0 ens32link-local 0.0.0.0 255.255.0.0 U 1003 0 0 ens33172.20.71.0 0.0.0.0 255.255.255.0 U 0 0 0 ens32192.168.78.0 10.0.1.104 255.255.255.0 UG 0 0 0 ens33 其中 Destination 表示目的网段或目标主机；Gateway 表示网关地址；Genmask 表示目的网段的掩码；Flags 表示路由标志：U 表示路由是启用 (up) 的、G 表示网关；Metric 表示目标距离，通常用跳数表示；Ref 表示路由的引用数；Use 表示路由查找计数；Iface 表示此条路由的出口。 选项 -n 表示用数字形式显示目的网段 选项 add 和 del 表示添加或删除一条路由。 选项 -net 和 netmask 表示指定目的网段及掩码。 选项 gw 表示指定网关。 选项 dev IF 表示指定出口网卡 如增加一条到 192.56.76.x 的路由，使它的出口为 ens32： 1route add -net 192.56.76.0 netmask 255.255.255.0 dev ens32 如增加一条默认路由，指明它的网关为 10.0.1.1 1route add default gw 10.0.1.1 如增加一条到 172.20.70.0 的路由，网关为 10.0.1.2 1route add -net 172.20.70.0/24 gw 10.0.1.2 如删除默认路由 1route del default 9、telnet 提供远程登录功能由于 telnet 协议使用明文传输，在要求安全登录的环境中并不适用。现在通常用它来进行网络服务的端口测试： 1234567$ telnet 10.0.1.251 80Trying 10.0.1.251...Connected to 10.0.1.251.Escape character is '^]'.^] #这里按了 CTRL+]，也可以按 CTRL+C 强行退出。telnet&gt; quitConnection closed. 这里对方的 80 端口是开启并允许通信的。当对端端口没有开启时： 123$ telnet 10.0.1.251 81Trying 10.0.1.251...telnet: connect to address 10.0.1.251: No route to host 当对端拒绝连接时： 123$ telnet 10.0.1.251 8085Trying 10.0.1.251...telnet: connect to address 10.0.1.251: Connection refused 10、ssh 远程登录程序1ssh [OPTIONS]... [user@]hostname [command] ssh 的全称是 Secure Shell，在不安全的网络主机间提供安全加密的通信，旨在代替其他远程登录协议。 12345678$ ssh 10.0.1.253The authenticity of host '10.0.1.253 (10.0.1.253)' can't be established.ECDSA key fingerprint is 96:bd:a3:a7:87:09:1b:53:44:4c:9b:b9:5f:b2:97:89.Are you sure you want to continue connecting (yes/no)? yes #这里输入 yesWarning: Permanently added '10.0.1.253' (ECDSA) to the list of known hosts.root@10.0.1.253's password: #这里输入密码Last login: Fri Nov 11 09:04:01 2016 from 192.168.78.137[root@idc-v-71253 ~]# #已登录 当命令 ssh 后直接跟主机 IP 时表示使用默认用户 root 登录，如果是首次登录，需要确认添加该主机的认证 key，当输入 yes 后，即会在本机 /root/.ssh/known_hosts 中增加一条该主机的记录，下一次登录时就不用再次确认了。然后需要输入用户密码，通过验证之后，我们就获得了目的主机的一个 shell，我们就可以在这个 shell 中执行命令了。 在新 shell 中输入 exit 即可退回到原 shell。 如果需要频繁登录某主机，但不想每次都输入密码，可以设置免密码登录： 123456789101112131415161718192021222324252627282930313233$ ssh-keygen -t rsa Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): #回车Enter passphrase (empty for no passphrase): #回车Enter same passphrase again: #回车Your identification has been saved in /root/.ssh/id_rsa. #私钥Your public key has been saved in /root/.ssh/id_rsa.pub. #公钥The key fingerprint is:be:c3:d0:02:50:35:35:fe:60:d6:2f:26:96:f0:e1:e6 root@centos7The key's randomart image is:+--[RSA 2048]----+| ...o.o || . o o || . . * . || . * = . || . .S + . || o=.o . || +E || o. || .. |+-----------------+$$ ssh-copy-id 10.0.1.253/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@10.0.1.253's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh'10.0.1.253'"and check to make sure that only the key(s) you wanted were added.$ 其中命令 ssh-keygen 用来生成公钥私钥，选项 -t 指明密钥类型。之后使用命令 ssh-copy-id 将公钥发送至目标主机，这里需要输入目标主机用户密码。然后就可以免密码登录了： 123$ ssh 10.0.1.253Last login: Fri Nov 11 11:08:37 2016 from 10.0.1.254[root@idc-v-71253 ~]# 还可以通过 ssh 远程执行命令： 1234$ ssh 10.0.1.252 "hostname"root@10.0.1.252's password: #输入密码idc-v-71252 #显示命令结果$ #并不登录 或者手动将公钥拷贝至目标主机： 12345$ cat /root/.ssh/id_rsa.pub | ssh 10.0.1.252 "cat - &gt;&gt; /root/.ssh/authorized_keys"root@10.0.1.252's password: #输入密码$ ssh 10.0.1.252 #免密登录Last login: Thu Nov 10 14:42:11 2016 from 192.168.78.135[root@idc-v-71252 ~]# 选项 -p 为登录指定端口： 123[root@centos7 temp]# ssh -p22 10.0.1.252Last login: Fri Nov 11 11:44:31 2016 from 10.0.1.254[root@idc-v-71252 ~]# 端口设置在服务端配置文件 /etc/ssh/sshd_config 中，默认端口号为 22，如更改需将 #Port 22 去掉注释并将 22 更改为需要的端口，然后重启 sshd 服务 service sshd restart 或 systemctl restart sshd。 如果需要使用另外的用户登录系统则执行 ssh user@host 我们可以用 tar 命令结合 ssh 和管道，将本地 (远程) 文件备份到远程(本地)： 12tar zc /home/temp | ssh user@host "tar xz" #本地 temp 目录备份到远程ssh user@host "tar cz /home/temp" | tar xz #远程 temp 目录备份到本地 选项 -L [bind_address:]port:host:hostport 设置本地端口转发 123$ ssh -L 2222:10.0.1.252:22 10.0.1.253Last login: Mon Nov 14 10:34:43 2016 from 10.0.1.254[root@idc-v-71253 ~]# #注意如果这里 exit 断开连接，则此转发也将终止。 此命令的意思是绑定本地端口 2222，并将所有发送至此端口的数据通过中间主机 10.0.1.253 转发至目标主机 10.0.1.252 的 22 端口，此时如果用 ssh 登录本机的 2222 端口，则实际登录的是主机 10.0.1.252 123$ ssh -p 2222 127.0.0.1Last login: Mon Nov 14 10:34:56 2016 from 10.0.1.253[root@idc-v-71252 ~]# 这里默认绑定的是本机的环回口 127.0.0.1，如绑定到其他地址，则根据语法设置 bind_address。 选项 -N 表示不执行命令，只设置端口转发时有用 由于上述端口转发命令 ssh -L 2222:10.0.1.252:22 10.0.1.253 会登录到中间主机，并且退出后端口转发也会终止，使用 -N 选项将不会登录，再配合 shell 后台执行，将会是一个不错的设置端口转发的选择 (但要注意对中间主机需要免密码登录)： 123$ ssh -N -L 2222:10.0.1.252:22 10.0.1.253 &amp;[1] 12432$ 命令最后的符号 &amp; 表示此命令将在后台执行，返回的信息中 [1] 表示后台命令编号，12432 表示命令的 PID。(关于 shell 后台命令，以后的文章中会有叙述) 选项 -R [bind_address:]port:host:hostport 设置远程端口转发 如我们在 10.0.1.253 上执行： 1ssh -R 2222:10.0.1.252:22 10.0.1.254 然后在 10.0.1.254 上登录： 123$ ssh -p 2222 localhostLast login: Mon Nov 14 10:40:44 2016 from 10.0.1.253[root@idc-v-71252 ~]# 这里的意思是使远程主机 10.0.1.254(相对 10.0.1.253 来说)监听端口 2222，然后将所有发送至此端口的数据转发至目标主机 10.0.1.252 的端口 22。之后再在 10.0.1.254登录本地 (localhost) 的 2222 端口时，实际通过中间主机 10.0.1.253 登录目标主机 10.0.1.252。 选项 -o OPTION 指定配置文件 (如 /etc/ssh/sshd_config) 内选项如避免第一次登录时输入 yes 确认，可增加 -o StrictHostKeyChecking=no。 11、scp 远程复制文件1scp [OPTIONS]... [[user@]host1:]file1 ... [[user@]host2:]file2 scp 命令通过 ssh 协议将数据加密传输，和 ssh 登录类似，需要输入远程主机用户密码。 如将远程主机 10.0.1.253 中文件 /root/tcp.sh 复制到本地当前目录下： 123$ scp root@10.0.1.251:/root/a.txt ./root@10.0.1.251's password:a.txt 100% 125 0.1KB/s 00:00 命令会显示传输状态 (传输百分比，大小，速度，用时)。 将本地文件复制到远程无非是将源和目的调换位置。 选项 -P 指定远端连接端口 (ssh 服务端口)，-o ssh_option 使用 ssh 选项。 选项 -l limit 传输限速，limit 单位为 Kbit/s。 和命令 cp 类似，选项 -r 表示复制目录，-p 表示保留文件权限时间等 12、netstat 打印网络信息选项 -a 显示所有端口信息： 12345678910111213141516171819$ netstat -aActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:ssh 0.0.0.0:* LISTEN tcp 0 0 localhost:smtp 0.0.0.0:* LISTEN tcp 0 52 10.0.1.254:ssh 192.168.78.143:49583 ESTABLISHEDtcp6 0 0 [::]:commplex-main [::]:* LISTEN tcp6 0 0 [::]:4243 [::]:* LISTEN tcp6 0 0 [::]:ssh [::]:* LISTEN tcp6 0 0 localhost:smtp [::]:* LISTEN raw6 0 0 [::]:ipv6-icmp [::]:* 7 raw6 0 0 [::]:ipv6-icmp [::]:* 7 Active UNIX domain sockets (servers and established)Proto RefCnt Flags Type State I-Node Pathunix 2 [ACC] STREAM LISTENING 12807 /run/systemd/privateunix 2 [ACC] STREAM LISTENING 12815 /run/lvm/lvmpolld.socketunix 2 [ ] DGRAM 12818 /run/systemd/shutdowndunix 2 [ACC] STREAM LISTENING 16403 /var/run/dbus/system_bus_socket.... 这里只显示部分信息 选项 -t 显示 TCP 连接信息 选项 -n 显示 IP 地址而不进行域名转换 选项 -p 显示 PID 和程序名 1234567891011$ netstat -antpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1358/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2162/master tcp 0 52 10.0.1.254:22 192.168.78.143:49583 ESTABLISHED 12044/sshd: root@pttcp6 0 0 :::5000 :::* LISTEN 17222/docker-proxy tcp6 0 0 :::4243 :::* LISTEN 16983/docker tcp6 0 0 :::22 :::* LISTEN 1358/sshd tcp6 0 0 ::1:25 :::* LISTEN 2162/master $ 其中 Proto 表示协议 (包括 TCP、UDP 等)；Recv-Q 和 Send-Q 表示接收和发送队列，一般都为 0，如果非 0 则表示本地的接收或发送缓存区有数据等待处理；Local Address 和 Foreign Address 分别表示本地地址和远端地址；State 表示连接状态，对应于 TCP 各种连接状态；PID/Program name 表示进程号和程序名。 选项 -l 表示只显示状态为 LISTEN 的连接 12345678910$ netstat -ntlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN tcp6 0 0 :::5000 :::* LISTEN tcp6 0 0 :::4243 :::* LISTEN tcp6 0 0 :::22 :::* LISTEN tcp6 0 0 ::1:25 :::* LISTEN $ 选项 -u 表示显示 UDP 连接信息 选项 -r 表示显示路由信息 1234567$ netstat -rKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Ifacedefault 10.0.1.103 0.0.0.0 UG 0 0 0 ens3310.0.1.0 0.0.0.0 255.255.255.0 U 0 0 0 ens33172.20.71.0 0.0.0.0 255.255.255.0 U 0 0 0 ens32192.168.78.0 10.0.1.104 255.255.255.0 UG 0 0 0 ens33 选项 -i 显示接口信息 123456$ netstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgens32 1500 13196107 0 77 0 3246 0 0 0 BMRUens33 1500 25312388 0 88 0 2516050 0 0 0 BMRUlo 65536 2503589 0 0 0 2503589 0 0 0 LRU 13、tcpdump 网络抓包工具命令 tcpdump 捕获某网络接口符合表达式 expression 的数据包，并打印出数据包内容的描述信息。 选项 -i 指定网卡： 12345678910111213[root@idc-v-71253 ~]# tcpdump -i ens33tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ens33, link-type EN10MB (Ethernet), capture size 65535 bytes15:41:59.121948 IP 10.0.1.108.3693 &gt; 239.100.1.1.websm: UDP, length 5815:41:59.122191 IP 10.0.1.109.35673 &gt; 239.100.1.1.websm: UDP, length 5715:41:59.128282 IP 10.0.1.253.ssh &gt; 192.168.78.143.51694: Flags [P.], seq 749565300:749565496, ack 3522345564, win 255, length 19615:41:59.134127 IP 192.168.78.143.51694 &gt; 10.0.1.253.ssh: Flags [.], ack 196, win 3977, length 015:41:59.140319 ARP, Request who-has 10.0.1.31 tell 10.0.1.102, length 4615:41:59.168328 ARP, Request who-has 10.0.1.37 tell 10.0.1.102, length 4615:41:59.262235 ARP, Request who-has 192.168.10.150 tell 192.168.10.151, length 4615:41:59.622090 IP 10.0.1.108.3693 &gt; 239.100.1.1.websm: UDP, length 5815:41:59.622178 IP 10.0.1.109.35673 &gt; 239.100.1.1.websm: UDP, length 57.... 启动命令之后显示出可以使用 -v 或 -vv 显示更详细的信息，开始从 ens33 捕获数据包。输出显示出各个发送或接收数据包包头信息 (包括 ARP、IP、TCP、UDP 等等协议)。此命令并未指定 expression，所以默认将捕获所有数据包。 如果需要将数据包捕获然后通过其他程序 (如 wireshark) 分析，可以使用选项 -w file 将数据写入文件，同时还需要使用选项 -s 0 指定能够捕获的数据包大小为 65535 字节，以避免数据包被截断而无法被分析。 真实环境中，流经网卡的数据包量是巨大的。可以使用表达式来对数据包进行过滤，对于每个数据包，都要经过表达式的过滤，只有表达式的值为 true 时，才会输出。 expression 中可以包含一到多个关键字指定的条件，可以使用 and(或 &amp;&amp;)、or(或 ||)、not(或!)和括号 () 表示各个关键字间的逻辑关系，可以用 &gt;、&lt; 表示比较，还可以进行计算。其中关键字包括： type 类型关键字，如 host、net、port 和 portrange，分别表示主机、网段、端口号、端口段。 direction 方向关键字，如 src、dst 分别表示源和目的。 proto 协议关键字，如 fddi、arp、ip、tcp、udp 等分别表示各种网络协议。 由于篇幅所限，下面的例子中将只描述选项和表达式所起到的作用，不再解释输出内容： 12345678910111213tcpdump -i ens33 dst host 10.0.1.251#监视所有从端口 ens33 发送到主机 10.0.1.251 的数据包，主机也可以是主机名tcpdump -i eth0 host ! 211.161.223.70 and ! 211.161.223.71 and dst port 80#监听端口 eth0，抓取不是来自或去到主机 211.161.223.70 和 211.161.223.71 并且目标端口为 80 的包tcpdump tcp port 23 host 210.27.48.1#获取主机 210.27.48.1 接收或发出的 telnet 包tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0) and src net (183.60.190 or 122.13.220)' -s0 -i eth0 -w ipdump#抓取源或目的端口是 80, 且源网络是（183.60.190.0/24 或者 122.13.220.0/24），并且含有数据, 而不是 SYN,FIN 以及 ACK-only 等不含数据的 TCP 数据包写入文件 ipdump#注意这里表达式使用单引号引起来以避免其中的特殊字符被 shell 解析而造成语法错误tcpdump 'tcp[tcpflags] &amp; (tcp-syn|tcp-fin) != 0 and ! src and dst net 10.0.0'#只打印 TCP 的开始和结束包 (SYN 和 FIN 标记)，并且源和目标网段均不是 10.0.0.0/24tcpdump 'gateway 10.0.1.1 and ip[2:2] &gt; 576'#表示抓取发送至网关 10.0.1.1 并且大于 576 字节的 IP 数据包 网络相关命令内容较多，下一篇将继续介绍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之推陈出新]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-new%2F</url>
      <content type="text"><![CDATA[本文介绍ip、ss、journalctl和firewall-cmd，它们旨在代替linux中原有的一些命令或服务。 1、ipip [OPTIONS] OBJECT COMMANDip是iproute2软件包里面的一个强大的网络配置工具，它能够替代一些传统的网络管理工具，例如ifconfig、route等，使用权限为超级用户。OPTIONS是修改ip行为或改变其输出的选项。OBJECT是要获取信息的对象。包括： address 表示设备的协议(IPv4或IPv6)地址link 表示网络设备monitor 表示监控网络连接信息neighbour 表示管理ARP缓存表netns 表示管理网络命名空间route 表示路由表接口tunnel 表示IP隧道….对象名可以是全称或简写格式，比如address可以简写为addr或a。COMMAND设置针对指定对象执行的操作，它和对象的类型有关。 address如显示网卡ens33的信息： [root@centos7 ~]# ip addr show ens333: ens33: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:a4:a9:16 brd ff:ff:ff:ff:ff:ff inet 10.0.1.254/24 brd 10.0.1.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:fea4:a916/64 scope link valid_lft forever preferred_lft forever选项-s表示输出更多的信息 [root@centos7 ~]# ip -s addr show ens333: ens33: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:a4:a9:16 brd ff:ff:ff:ff:ff:ff inet 10.0.1.254/24 brd 10.0.1.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:fea4:a916/64 scope link valid_lft forever preferred_lft forever RX: bytes packets errors dropped overrun mcast 133518854 1415841 0 0 0 0 TX: bytes packets errors dropped carrier collsns 14033474 59479 0 0 0 0为ens33增加一个新地址 [root@centos7 ~]# ip addr add 192.168.0.193/24 dev ens33[root@centos7 ~]# ip a sh ens333: ens33: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:a4:a9:16 brd ff:ff:ff:ff:ff:ff inet 10.0.1.254/24 brd 10.0.1.255 scope global ens33 valid_lft forever preferred_lft forever inet 192.168.0.193/24 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:fea4:a916/64 scope link valid_lft forever preferred_lft forever #删除[root@centos7 ~]# ip addr del 192.168.0.193/24 dev ens33neighbour如查看arp表项(neighbour可以简写为neigh或n) [root@centos7 ~]# ip neigh172.20.71.253 dev ens32 lladdr 68:8f:84:03:71:e6 STALE10.0.1.102 dev ens33 lladdr 00:50:56:a4:18:9a STALE10.0.1.1 dev ens33 lladdr 68:8f:84:01:f1:ff STALE10.0.1.103 dev ens33 lladdr 00:1c:7f:3b:da:b0 STALE10.0.1.104 dev ens33 lladdr 00:50:56:a4:37:a7 DELAY10.0.1.252 dev ens33 lladdr 00:50:56:a4:65:71 STALEneighbour可以使用的COMMAND包括add添加、change修改、replace替换、delete删除、flush清除等。如在设备ens33上为地址10.0.1.253添加一个永久的ARP条目： [root@centos7 ~]# ip nei add 10.0.1.253 lladdr 78:A3:51:14:F7:98 dev ens33 nud permanent[root@centos7 ~]# ip nei show dev ens3310.0.1.103 lladdr 00:1c:7f:3b:da:b0 STALE10.0.1.1 lladdr 68:8f:84:01:f1:ff STALE10.0.1.104 lladdr 00:50:56:a4:37:a7 REACHABLE10.0.1.102 lladdr 00:50:56:a4:18:9a STALE10.0.1.253 lladdr 78:a3:51:14:f7:98 PERMANENT10.0.1.252 lladdr 00:50:56:a4:65:71 STALElink如更改ens33的MTU(最大传输单元)的值为1600 [root@centos7 ~]# ip link set dev ens33 mtu 1600[root@centos7 ~]# ip link show dev ens333: ens33: mtu 1600 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 00:50:56:a4:a9:16 brd ff:ff:ff:ff:ff:ff关闭设备ens32 [root@centos7 ~]# ip link set dev ens32 down[root@centos7 ~]# ip li ls dev ens322: ens32: mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 00:50:56:a4:f6:f8 brd ff:ff:ff:ff:ff:ff创建一个关联到ens32的网桥 [root@centos7 ~]# ip link add link ens32 name br1 type bridge[root@centos7 ~]# ip link show dev br18: br1: mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 0e:00:3a:f2:fa:ee brd ff:ff:ff:ff:ff:ff #启用[root@centos7 ~]# ip link set dev br1 up #停用[root@centos7 ~]# ip link set dev br1 down #删除[root@centos7 ~]# ip link del dev br1route如显示路由表(这里使用了命令column -t对输出进行了格式化) [root@centos7 ~]# ip route show|column -tdefault via 10.0.1.103 dev ens33 proto static metric 10010.0.1.0/24 dev ens33 proto kernel scope link src 10.0.1.254 metric 100172.20.71.0/24 dev ens32 proto kernel scope link src 172.20.71.254 metric 100192.168.78.0/24 via 10.0.1.104 dev ens33如添加一条到192.168.0.0/16下一跳是10.0.1.101的路由 [root@centos7 ~]# ip route add 192.168.0.0/16 via 10.0.1.101 dev ens33[root@centos7 ~]# ip route show|column -tdefault via 10.0.1.103 dev ens33 proto static metric 10010.0.1.0/24 dev ens33 proto kernel scope link src 10.0.1.254 metric 100172.20.71.0/24 dev ens32 proto kernel scope link src 172.20.71.254 metric 100192.168.0.0/16 via 10.0.1.101 dev ens33192.168.78.0/24 via 10.0.1.104 dev ens33 #删除[root@centos7 ~]# ip route del 192.168.0.0/16还可以使用change、replace等表示改变/替换原有路由条目。如获取单条路由信息 [root@centos7 ~]# ip rou get 10.0.1.0/24broadcast 10.0.1.0 dev ens33 src 10.0.1.254 cache 2、ssss [options] [FILTER]ss命令可以用来获取socket统计信息，它可以显示和netstat类似的内容。但ss的优势在于它能够显示更多详细的有关TCP和连接状态的信息，而且比netstat更高效。当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢。ss命令利用了TCP协议栈中tcp_diag，tcp_diag是一个用于分析统计的模块，可以获得linux内核的第一手信息，这确保了ss的快捷高效。选项-a表示显示所有连接状态信息选项-t表示显示TCP sockets选项-u表示显示UDP sockets选项-n表示不转换数字为服务名选项-p表示显示进程 [root@centos7 ~]# ss -antp|column -tState Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 0 128 :22 : users:((“sshd”,pid=1355,fd=3))LISTEN 0 100 127.0.0.1:25 : users:((“master”,pid=2214,fd=13))ESTAB 0 0 10.0.1.254:22 192.168.78.141:50332 users:((“sshd”,pid=18294,fd=3))ESTAB 0 52 10.0.1.254:22 192.168.78.178:51667 users:((“sshd”,pid=18433,fd=3))LISTEN 0 128 :::5000 ::: users:((“exe”,pid=5908,fd=7))LISTEN 0 128 :::22 ::: users:((“sshd”,pid=1355,fd=4))LISTEN 0 100 ::1:25 ::: users:((“master”,pid=2214,fd=14))选项-l表示只显示监听状态的sockets [root@centos7 ~]# ss -lt|column -tState Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 0 128 :ssh :LISTEN 0 100 127.0.0.1:smtp :LISTEN 0 128 :::commplex-main :::LISTEN 0 128 :::ssh :::LISTEN 0 100 ::1:smtp :::选项-s表示显示汇总信息 [root@centos7 ~]# ss -sTotal: 270 (kernel 575)TCP: 8 (estab 1, closed 1, orphaned 0, synrecv 0, timewait 0/0), ports 0 Transport Total IP IPv6 575 - -RAW 2 0 2UDP 0 0 0TCP 7 3 4INET 9 3 6FRAG 0 0 0还可以使用state STATE-FILTER [EXPRESSION]指定过滤格式如显示源或目的端口为8080，状态为established的连接： ss state established ‘( dport = :8080 or sport = :8080 )’如来自193.233.7/24，状态为fin-wait-1的http或https连接 ss state fin-wait-1 ‘( sport = :http or sport = :https )’ dst 193.233.7/243、journalctljournalctl [OPTIONS…] [MATCHES…]在基于systemd的系统中，可以使用一个新工具Journal代替原来的系统服务Syslog来记录日志。关于Journal优越性就不在这里叙述了，我们来直接看它怎么使用。 Journal服务的配置文件是/etc/systemd/journald.conf，在默认配置中，Journal日志保存在目录/run/log/journal内(tmpfs内存文件系统)，系统重启将不会保留，可以手动将日志刷到(通过命令journalctl –flush)磁盘文件系统上(/var/log/journal内)。 Journal服务随系统启动而启动，默认会记录从开机到关机全过程的内核和应用程序日志 #查看服务状态[root@centos7 ~]# systemctl status -l systemd-journald● systemd-journald.service - Journal Service Loaded: loaded (/usr/lib/systemd/system/systemd-journald.service; static; vendor preset: disabled) Active: active (running) since 二 2016-12-20 11:15:22 CST; 1 weeks 0 days ago Docs: man:systemd-journald.service(8) man:journald.conf(5) Main PID: 539 (systemd-journal) Status: “Processing requests…” CGroup: /system.slice/systemd-journald.service └─539 /usr/lib/systemd/systemd-journald 12月 20 11:15:22 centos7 systemd-journal[539]: Runtime journal is using 8.0M (max allowed 391.1M, trying to leave 586.7M free of 3.8G available → current limit 391.1M).12月 20 11:15:22 centos7 systemd-journal[539]: Runtime journal is using 8.0M (max allowed 391.1M, trying to leave 586.7M free of 3.8G available → current limit 391.1M).12月 20 11:15:22 centos7 systemd-journal[539]: Journal started12月 20 11:15:22 centos7 systemd-journal[539]: Runtime journal is using 8.0M (max allowed 391.1M, trying to leave 586.7M free of 3.8G available → current limit 391.1M).当命令journalctl不带任何选项时会分页显示系统的所有日志(从本次开机到现在时间)选项-k表示显示内核kernel日志选项-u UNIT表示显示指定服务单元UNIT的日志 #如上一篇中配置的计时器(ping252.timer)和服务(ping252.service)日志[root@centos7 ~]# journalctl -u ping252.timer– Logs begin at 二 2016-12-20 11:15:19 CST, end at 二 2016-12-27 20:39:54 CST. –12月 23 14:27:26 centos7 systemd[1]: Started ping 252 every 30s.12月 23 14:27:26 centos7 systemd[1]: Starting ping 252 every 30s.12月 23 14:36:57 centos7 systemd[1]: Stopped ping 252 every 30s.….[root@centos7 ~]# journalctl -u ping252– Logs begin at 二 2016-12-20 11:15:19 CST, end at 二 2016-12-27 20:41:34 CST. –12月 23 14:28:28 centos7 systemd[1]: Started ping 252.12月 23 14:28:28 centos7 systemd[1]: Starting ping 252…12月 23 14:28:28 centos7 systemd[11428]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error12月 23 14:28:28 centos7 systemd[1]: ping252.service: main process exited, code=exited, status=203/EXEC12月 23 14:28:28 centos7 systemd[1]: Unit ping252.service entered failed state.12月 23 14:28:28 centos7 systemd[1]: ping252.service failed.12月 23 14:29:03 centos7 systemd[1]: Started ping 252.….选项-r表示反向输出日志(从当前时间到本次开机)选项-n N表示输出最新的N行日志 [root@centos7 ~]# journalctl -n 5 -u ping252– Logs begin at 二 2016-12-20 11:15:19 CST, end at 二 2016-12-27 20:48:54 CST. –12月 23 17:27:12 centos7 systemd[1]: Starting 252…12月 23 17:29:12 centos7 systemd[1]: Started 252.12月 23 17:29:12 centos7 systemd[1]: Starting 252…12月 23 17:31:12 centos7 systemd[1]: Started 252.12月 23 17:31:12 centos7 systemd[1]: Starting 252…选项-f表示显示最新的10行日志并继续等待输出新日志(类似于命令tail -f)选项-p n表示过滤输出指定级别的日志，其中n的值可以是： 0 表示 emerg1 表示 alert2 表示 crit3 表示 err4 表示 warning5 表示 notice6 表示 info7 表示 debug如 [root@centos7 ~]# journalctl -u ping252 -p 3– Logs begin at 二 2016-12-20 11:15:19 CST, end at 二 2016-12-27 21:13:34 CST. –12月 23 14:28:28 centos7 systemd[11428]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error12月 23 14:29:03 centos7 systemd[11442]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error12月 23 14:30:32 centos7 systemd[11452]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error选项–since=和–until=表示显示晚于指定时间(–since=)的日志、显示早于指定时间(–until=)的日志。时间格式如上一篇systemd.timer所示： [root@centos7 ~]# journalctl -u ping252 –since “2016-12-20 11:15:19” –until “now” -p 3– Logs begin at 二 2016-12-20 11:15:19 CST, end at 二 2016-12-27 21:37:14 CST. –12月 23 14:28:28 centos7 systemd[11428]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error12月 23 14:29:03 centos7 systemd[11442]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error12月 23 14:30:32 centos7 systemd[11452]: Failed at step EXEC spawning /root/temp/ping252.sh: Exec format error选项–disk-usage表示显示日志磁盘占用量 [root@centos7 ~]# journalctl –disk-usageArchived and active journals take up 104.8M on disk.选项–vacuum-size=用于设置日志最大磁盘使用量（值可以使用K、M、G、T等后缀）。选项–vacuum-time=用于清除指定时间之前的日志（可以使用”s”, “m”, “h”, “days”, “weeks”, “months”, “years” 等后缀） [root@centos7 ~]# journalctl –vacuum-time=”1 days”Deleted archived journal /run/log/journal/9……2e.journal (48.0M).Deleted archived journal /run/log/journal/9……a1.journal (48.8M).Vacuuming done, freed 96.8M of archived journals on disk.选项-o表示控制输出格式，可以带一个如下参数： short 默认格式，和传统的syslog格式相似，每条日志一行short-iso 和short类似，但显示ISO 8601时间戳short-precise 和short类似，只是将时间戳字段的秒数精确到微秒级别short-monotonic 和short类似，只是将时间戳字段的零值从内核启动时开始计算。short-unix 和short类似，只是将时间戳字段显示为从”UNIX时间原点”(1970-1-1 00:00:00 UTC)以来的秒数。 精确到微秒级别。verbose 以结构化的格式显示每条日志的所有字段。export 将日志序列化为二进制字节流(大部分依然是文本)以适用于备份与网络传输。json 将日志项按照JSON数据结构格式化， 每条日志一行。json-pretty 将日志项按照JSON数据结构格式化， 但是每个字段一行， 以便于人类阅读。json-sse 将日志项按照JSON数据结构格式化，每条日志一行，但是用大括号包围。cat 仅显示日志的实际内容， 而不显示与此日志相关的任何元数据(包括时间戳)。4、firewall-cmd同iptables一样，firewalld也通过内核的netfilter来实现防火墙功能(netfilter的简介)，比iptables先进的地方在于，firewalld可以动态修改单条规则，而不需要像iptables那样，在修改了规则之后必须全部刷新才可以生效。而且firewalld在使用上更人性化，不需要理解netfilter的原理也能实现大部分功能。 firewalld需要开启守护进程，查看防火墙服务状态： [root@idc-v-71252 ~]# systemctl status firewalld● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: active (running) since 三 2016-12-14 14:07:04 CST; 1 weeks 4 days ago Main PID: 898 (firewalld) CGroup: /system.slice/firewalld.service └─898 /usr/bin/python -Es /usr/sbin/firewalld –nofork –nopid 12月 14 14:07:03 centos7 systemd[1]: Starting firewalld - dynamic firewall daemon…12月 14 14:07:04 centos7 systemd[1]: Started firewalld - dynamic firewall daemon.或者通过自身的firewall-cmd查看 [root@centos7 ~]# firewall-cmd –statrunning[root@centos7 ~]#firewalld的配置文件以xml格式为主(主配置文件firewalld.conf除外)，它们有两个存储位置： 1、/etc/firewalld2、/usr/lib/firewalld使用时的规则是这样的：当需要一个文件时，firewalld会首先到第一个目录中查找，如果可以找到，那么就直接使用，否则会继续到第二个目录中查找。不推荐在目录/usr/lib/firewalld中直接修改配置文件，最好是在/usr/lib/firewalld中复制一份配置文件到/etc/firewalld的相应目录中，然后进行修改。这样，在恢复默认配置时，直接删除/etc/firewalld中的文件即可。 firewalld中引入了两个概念：service(服务)和zone(区域)。 service通用配置文件(位于目录/usr/lib/firewalld/services内)中定义了服务与端口的映射，firewalld在使用时可以直接引用服务名而不是像iptables那样引用端口号(就像DNS服务将域名和IP地址做了映射)； 默认时firewalld提供了九个zone配置文件，位于/usr/lib/firewalld/zones中： [root@centos7 ~]# ls /usr/lib/firewalld/zonesblock.xml dmz.xml drop.xml external.xml home.xml internal.xml public.xml trusted.xml work.xml每个文件中定义了一套规则集，或者说判断方案。firewalld通过判断配置文件中如下三个地方来决定具体使用哪套方案来过滤包： 1、source 原地址2、interface 接收包的网卡3、默认zone(可在/etc/firewalld/firewalld.conf中配置)这三个优先级按顺序依次降低，也就是说如果按照source可以找到就不会再按interface去查找，如果前两个都找不到才会使用第三个。 zonepublic.xml内容： [root@centos7 ~]# cat /usr/lib/firewalld/zones/public.xml&lt;?xml version=”1.0” encoding=”utf-8”?&gt; Public For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. [root@centos7 ~]#zone配置文件中可以配置的项包括： zone 定义zone起始和结束的标签，只能用于zone配置文件，可以设置两个属性： version 版本 target 本zone的默认规则，包括四个可选值：default、ACCEPT、%%REJECT%%、DROP，如果不设置则表示默认值default，如果默认规则不是default，除source和interface两个配置项以外的其他规则项都将被忽略，而直接跳转到默认规则。short 区域简短描述description 区域描述interface 绑定一个本地接口到本zonesource 绑定一个或一组源地址到本zoneservice 表示一个服务port 端口，使用port可以不通过service而直接对端口进行设置icmp-block icmp报文阻塞，可以按照icmp类型进行设置masquerade ip地址伪装，也就是按照源网卡地址进行NAT转发forward-port 端口转发rule 自定义规则firewalld默认区域中，ACCEPT用在trusted区域，%%REJECT%%用在block区域，DROP用在drop区域。使用时可以复制一份需要的文件至/etc/firewalld/zones中，然后将需要的源地址或接口配置在相应的文件中。 配置source.source在zone的xml文件中的格式为 需要注意的是相同的source项只能在一个zone中进行配置，也就是说同一个源地址只能对应一个zone，另外，直接编辑xml文件之后需要执行命令firewall-cmd –reload才能生效。 当然也可以使用firewall-cmd命令进行配置(可选项–permanent表示是否保存到配置文件中，使用后需要–reload才能生效；–zone表示指定zone，不指定表示使用默认zone)： #列出指定zone的所有绑定的source地址firewall-cmd [–permanent] [–zone=zone] –list-sources #查询指定zone是否跟指定source地址进行了绑定firewall-cmd [–permanent] [–zone=zone] –query-source=source[/mask] #将一个source地址绑定到指定的zone(只可绑定一次，第二次绑定到不同的zone会报错)firewall-cmd [–permanent] [–zone=zone] –add-source=source[/mask] #改变source地址所绑定的zonefirewall-cmd [–zone=zone] –change-source=source[/mask] #删除source地址跟zone的绑定firewall-cmd [–permanent] [–zone=zone] –remove-source=source[/mask]如将源地址192.168.0.0/16添加至默认zone [root@centos7 zones]# firewall-cmd –add-source=192.168.0.0/16success[root@centos7 zones]# firewall-cmd –list-sources192.168.0.0/16[root@centos7 zones]# firewall-cmd –remove-source=192.168.0.0/16success[root@centos7 zones]#配置interface.同source配置项相同，同一个interface只能对应一个zone。interface在zone的xml文件中的格式为： 还可以将zone配置在网卡配置文件(ifcfg-*文件)中，使接口绑定到指定zone： ZONE=public相应命令： #列出指定zone的绑定接口firewall-cmd [–permanent] [–zone=zone] –list-interfaces #绑定接口到指定zonefirewall-cmd [–permanent] [–zone=zone] –add-interface=interface #改变接口绑定zonefirewall-cmd [–zone=zone] –change-interface=interface #查询接口是否和指定zone绑定firewall-cmd [–permanent] [–zone=zone] –query-interface=interface #删除绑定firewall-cmd [–permanent] [–zone=zone] –remove-interface=interface如将ens32移除出默认zone [root@centos7 zones]# firewall-cmd –list-interfacesens32 ens33[root@centos7 zones]#[root@centos7 zones]# firewall-cmd –remove-interface=ens32success[root@centos7 zones]# firewall-cmd –list-interfacesens33[root@centos7 zones]#配置service同一个service可以配置到多个不同的zone中 相应命令： firewall-cmd [–permanent] [–zone=zone] –list-services #–timeout=seconds表示生效时间，过期后自动删除。不能和–permanent一起使用firewall-cmd [–permanent] [–zone=zone] –add-service=service [–timeout=seconds]firewall-cmd [–permanent] [–zone=zone] –remove-service=servicefirewall-cmd [–permanent] [–zone=zone] –query-service=service #列出所有可用服务firewall-cmd –get-service如增加http服务到默认zone [root@centos7 zones]# firewall-cmd –add-service=httpsuccess[root@centos7 zones]# firewall-cmd –remove-service=httpsuccess[root@centos7 zones]#配置port.需要同时指定协议和端口号，端口号可以用-连接表示一个范围。 命令 firewall-cmd [–permanent] [–zone=zone] –list-portsfirewall-cmd [–permanent] [–zone=zone] –add-port=portid[-portid]/protocol [–timeout=seconds]firewall-cmd [–permanent] [–zone=zone] –remove-port=portid[-portid]/protocolfirewall-cmd [–permanent] [–zone=zone] –query-port=portid[-portid]/protocol如限时10秒允许80端口的访问 [root@centos7 zones]# firewall-cmd –add-port=80/tcp –timeout=10success[root@centos7 zones]#配置icmp-block. string处配置需要阻塞的ICMP类型命令 #列出所有ICMP类型firewall-cmd –get-icmptypesfirewall-cmd [–permanent] [–zone=zone] –list-icmp-blocksfirewall-cmd [–permanent] [–zone=zone] –add-icmp-block=icmptype [–timeout=seconds]firewall-cmd [–permanent] [–zone=zone] –remove-icmp-block=icmptypefirewall-cmd [–permanent] [–zone=zone] –query-icmp-block=icmptype如禁止ping [root@centos7 zones]# firewall-cmd –add-icmp-block=echo-requestsuccess #在另一台机器ping本机：[root@idc-v-71252 ~]# ping 10.0.1.254PING 10.0.1.254 (10.0.1.254) 56(84) bytes of data.From 10.0.1.254 icmp_seq=1 Destination Host ProhibitedFrom 10.0.1.254 icmp_seq=2 Destination Host ProhibitedFrom 10.0.1.254 icmp_seq=3 Destination Host Prohibited^C #取消[root@centos7 zones]# firewall-cmd –remove-icmp-block=echo-requestsuccess[root@centos7 zones]#配置masquerade. NAT转发，将接收到的请求的源地址设置为转发请求网卡的地址。命令 firewall-cmd [–permanent] [–zone=zone] –add-masquerade [–timeout=seconds]firewall-cmd [–permanent] [–zone=zone] –remove-masqueradefirewall-cmd [–permanent] [–zone=zone] –query-masquerade配置forward-port. 命令(其中转发规则FORWARD为port=portid[-portid]:proto=protocol[:toport=portid[-portid]][:toaddr=address[/mask]]) firewall-cmd [–permanent] [–zone=zone] –list-forward-portsfirewall-cmd [–permanent] [–zone=zone] –add-forward-port=FORWARD [–timeout=seconds]firewall-cmd [–permanent] [–zone=zone] –remove-forward-port=FORWARDfirewall-cmd [–permanent] [–zone=zone] –query-forward-port=FORWARD如将80端口接收到的请求转发到本机的8080端口(如需转发至其他地址则添加:to-addr=address[/mask])： [root@centos7 zones]# firewall-cmd –add-forward-port=port=80:proto=tcp:toport=8080success[root@centos7 zones]# firewall-cmd –list-forward-portsport=80:proto=tcp:toport=8080:toaddr=[root@centos7 zones]# firewall-cmd –remove-forward-port=port=80:proto=tcp:toport=8080success[root@centos7 zones]#配置rule.rule可以用来定义一条复杂的规则，其在文件中定义如下： [ ] [ ] [ | | | | | ] [ [] ] [ [] ] [ | | ] 这里的rule就相当于使用iptables时的一条规则。命令 firewall-cmd [–permanent] [–zone=zone] –list-rich-rulesfirewall-cmd [–permanent] [–zone=zone] –add-rich-rule=’rule’ [–timeout=seconds]firewall-cmd [–permanent] [–zone=zone] –remove-rich-rule=’rule’firewall-cmd [–permanent] [–zone=zone] –query-rich-rule=’rule’如源地址为192.168.10.0/24的http连接都drop掉： [root@centos7 zones]# firewall-cmd –add-rich-rule=’rule family=”ipv4” source address=”192.168.10.0/24” service name=”http” drop’success[root@centos7 zones]# firewall-cmd –query-rich-rule=’rule family=”ipv4” source address=”192.168.10.0/24” service name=”http” drop’yes[root@centos7 zones]# firewall-cmd –remove-rich-rule=’rule family=”ipv4” source address=”192.168.10.0/24” service name=”http” drop’success[root@centos7 zones]#serviceservice配置文件格式为： [short description] [description] [] [] []其中最重要的配置项是port，表示将端口绑定到指定服务，当该端口收到包时即表示对该服务的请求，防火墙从而到对应的zone中去查找规则，判断是否放行。 一个service中可以配置多个port项，单个port项中可以配置单个端口，也可以是一个端口段，比如port=80-85表示80到85之间的端口号。 destination表示根据目的地址绑定服务，可以是ipv4地址也可以是ipv6地址，可以使用掩码。module用来设置netfilter连接跟踪模块 firewall-cmd提供了两个选项用于创建和删除service，–new-service和–delete-service。不过直接编辑xml文件是更好的选择。 direct直接使用防火墙的过滤规则，配置文件为/etc/firewalld/direct.xml(可以手动创建或通过命令生成)，文件结构如下： &lt;?xml version=”1.0” encoding=”utf-8”?&gt; [ ] [ args ] [ args ]可以在配置文件中直接配置iptables规则，其中： ipv 表示ip版本table 表示iptables中的tablechain 表示iptables中的chain，可以是自定义的priority 优先级，类似于iptables中规则的前后顺序，数字越小优先级越高args 表示具体规则，也可以是自定义的chain如自定义一个叫blacklist的链，然后将所有来自192.168.1.0/24和192.168.5.0/24的数据包都指向了这个链，指定这个链的规则：首先使用’blacklisted: ‘前缀进行日志记录(每分钟记录一次)，然后drop。 &lt;?xml version=”1.0” encoding=”utf-8”?&gt; -s 192.168.1.0/24 -j blacklist -s 192.168.5.0/24 -j blacklist -m limit –limit 1/min -j LOG –log-prefix “blacklisted: “ -j DROP相关命令： firewall-cmd [–permanent] –direct –get-all-chainsfirewall-cmd [–permanent] –direct –get-chains { ipv4 | ipv6 | eb } tablefirewall-cmd [–permanent] –direct –add-chain { ipv4 | ipv6 | eb } table chainfirewall-cmd [–permanent] –direct –remove-chain { ipv4 | ipv6 | eb } table chainfirewall-cmd [–permanent] –direct –query-chain { ipv4 | ipv6 | eb } table chain firewall-cmd [–permanent] –direct –get-all-rulesfirewall-cmd [–permanent] –direct –get-rules { ipv4 | ipv6 | eb } table chainfirewall-cmd [–permanent] –direct –add-rule { ipv4 | ipv6 | eb } table chain priority argsfirewall-cmd [–permanent] –direct –remove-rule { ipv4 | ipv6 | eb } table chain priority argsfirewall-cmd [–permanent] –direct –remove-rules { ipv4 | ipv6 | eb } table chainfirewall-cmd [–permanent] –direct –query-rule { ipv4 | ipv6 | eb } table chain priority args firewall-cmd –direct –passthrough { ipv4 | ipv6 | eb } argsfirewall-cmd –permanent –direct –get-all-passthroughsfirewall-cmd –permanent –direct –get-passthroughs { ipv4 | ipv6 | eb }firewall-cmd –permanent –direct –add-passthrough { ipv4 | ipv6 | eb } argsfirewall-cmd –permanent –direct –remove-passthrough { ipv4 | ipv6 | eb } argsfirewall-cmd –permanent –direct –query-passthrough { ipv4 | ipv6 | eb } args上述例子转化成命令即为： firewall-cmd –permanent –direct –add-chain ipv4 raw blacklistfirewall-cmd –permanent –direct –add-rule ipv4 raw PREROUTING 0 -s 192.168.1.0/24 -j blacklistfirewall-cmd –permanent –direct –add-rule ipv4 raw PREROUTING 1 -s 192.168.5.0/24 -j blacklistfirewall-cmd –permanent –direct –add-rule ipv4 raw blacklist 0 -m limit –limit 1/min -j LOG –log-prefix “blacklisted: “firewall-cmd –permanent –direct –add-rule ipv4 raw blacklist 1 -j DROP #重载生效firewall-cmd –reload在实际生产环境中如果防火墙规则只是由root设定的话，最好将firewall-cmd(此文件为python脚本)的权限限制为只有root能执行： [root@centos7 ~]# ls -l /usr/bin/firewall-cmd-rwxr-xr-x. 1 root root 62012 11月 20 2015 /usr/bin/firewall-cmd[root@centos7 ~]# file /usr/bin/firewall-cmd/usr/bin/firewall-cmd: Python script, ASCII text executable[root@centos7 ~]# chmod 750 /usr/bin/firewall-cmd关于firewalld的更多内容请查看相关文档 至此，linux基础命令介绍系列就结束了。前后十五篇文章，记录了百余个常用命令。之后将开启新的系列：shell编程。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux 基础之软件包管理]]></title>
      <url>%2F2016%2F10%2F28%2Flinux-rpm%2F</url>
      <content type="text"><![CDATA[linux 中软件包的管理随着系统发行版本的不同而不同，RPM 和 DPKG 为最常见的两类软件包管理工具，分别应用于基于 rpm 软件包的 linux 发行版和基于 deb 软件包的 linux 发行版。本文只描述 RPM 的使用方法，另一种命令不同，但用法类似，就不做介绍了。 1、RPM 包管理器选项 -q 表示查询系统安装的软件包 12345[root@centos7 ~]# rpm -q sudosudo-1.8.6p7-16.el7.x86_64[root@centos7 ~]# rpm -q nginx未安装软件包 nginx[root@centos7 ~]# 选项 -a 表示查询所有安装的 rpm 包 123456[root@centos7 ~]# rpm -qa|grep vimvim-filesystem-7.4.160-1.el7.x86_64vim-common-7.4.160-1.el7.x86_64vim-enhanced-7.4.160-1.el7.x86_64vim-minimal-7.4.160-1.el7.x86_64[root@centos7 ~]# 选项 -f file 表示查询文件所属软件包 123[root@centos7 ~]# rpm -qf /usr/bin/lscoreutils-8.22-15.el7.x86_64[root@centos7 ~]# 选项 -c 表示查询软件包的配置文件 1234567[root@centos7 ~]# rpm -qc sudo/etc/pam.d/sudo/etc/pam.d/sudo-i/etc/sudo-ldap.conf/etc/sudo.conf/etc/sudoers[root@centos7 ~]# 选项 -e 表示卸载软件包 12345[root@centos7 ~]# rpm -e sudo警告：/etc/sudoers 已另存为 /etc/sudoers.rpmsave[root@centos7 ~]# rpm -q sudo未安装软件包 sudo[root@centos7 ~]# 选项 -i 表示安装 -v 表示显示详细信息 -h 表示显示安装进度 12345678#下载 rpm 包[root@centos7 tmp]# wget ftp.scientificlinux.org/linux/scientific/7rolling/x86_64/os/Packages/sudo-1.8.6p7-16.el7.x86_64.rpm#安装[root@centos7 tmp]# rpm -ivh sudo-1.8.6p7-16.el7.x86_64.rpm警告：sudo-1.8.6p7-16.el7.x86_64.rpm: 头 V4 DSA/SHA1 Signature, 密钥 ID 192a7d7d: NOKEY准备中... ################################# [100%]正在升级 / 安装... 1:sudo-1.8.6p7-16.el7 ################################# [100%] 有很多软件并不是只有一个 rpm 包，它们之间有各种各样的依赖关系，当安装 (或卸载) 时，需要将所有依赖的包都安装 (或卸载) 之后才能安装 (或卸载) 成功 123[root@centos7 tmp]# rpm -e vim-common错误：依赖检测失败： vim-common = 2:7.4.160-1.el7 被 (已安裝) vim-enhanced-2:7.4.160-1.el7.x86_64 需要 选项 –nodeps 表示忽略依赖关系 123456[root@centos7 tmp]# rpm -q vim-commonvim-common-7.4.160-1.el7.x86_64[root@centos7 tmp]# rpm -e --nodeps vim-common警告：/etc/vimrc 已另存为 /etc/vimrc.rpmsave[root@centos7 tmp]# rpm -q vim-common未安装软件包 vim-common 选项 -U 表示对软件包升级 12345678910[root@centos7 tmp]# rpm -q wgetwget-1.14-10.el7_0.1.x86_64[root@centos7 tmp]# rpm -Uvh wget-1.14-13.el7.x86_64.rpm准备中... ################################# [100%]正在升级 / 安装... 1:wget-1.14-13.el7 ################################# [50%]正在清理 / 删除... 2:wget-1.14-10.el7_0.1 ################################# [100%][root@centos7 tmp]# rpm -q wgetwget-1.14-13.el7.x86_64 2、yum 下载更新器1yum [options] [command] [package ...] yum 是一个基于 rpm 的交互式软件包管理器。yum 在安装软件时并不需要像 rpm 那样手动查找安装，它在工作时会搜索源中的 rpm 包，并自动解决依赖关系，自动下载并安装。yum 默认源配置文件位于目录 /etc/yum.repos.d 内。 命令 install 表示安装 12345678910111213141516171819202122232425262728293031323334[root@centos7 ~]# yum install vim-common已加载插件：fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.yun-idc.com * extras: mirrors.yun-idc.com * updates: mirrors.yun-idc.com正在解决依赖关系--&gt; 正在检查事务---&gt; 软件包 vim-common.x86_64.2.7.4.160-1.el7 将被 安装--&gt; 解决依赖关系完成依赖关系解决.... #省略部分输出安装 1 软件包总下载量：5.9 M安装大小：21 MIs this ok [y/d/N]:y #需要在这里输入确认是否安装Downloading packages:vim-common-7.4.160-1.el7.x86_64.rpm | 5.9 MB 00:00:00 Running transaction checkRunning transaction testTransaction test succeededRunning transaction警告：RPM 数据库已被非 yum 程序修改。** 发现 1 个已存在的 RPM 数据库问题， 'yum check' 输出如下：2:vim-enhanced-7.4.160-1.el7.x86_64 有缺少的需求 vim-common = ('2', '7.4.160', '1.el7') 正在安装 : 2:vim-common-7.4.160-1.el7.x86_64 1/1 验证中 : 2:vim-common-7.4.160-1.el7.x86_64 1/1已安装: vim-common.x86_64 2:7.4.160-1.el7 完毕！ 命令 check-update 表示检查更新 命令 update 表示升级 命令 search 表示搜索软件包 命令 list 表示列出可用软件包 命令 remove 表示卸载 命令 clean 表示清除 yum 缓存目录内容 选项 -y 表示在所有需要交互式确认的地方默认输入 yes 当 yum 源中没有所需要安装的包时，会报没有可用软件包的错误。此时可以通过添加新的 yum 源来解决 如 centos7 中安装 nginx： 1234567#安装 repo[root@centos7 tmp]# rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm获取 http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm警告：/var/tmp/rpm-tmp.cUANoe: 头 V4 RSA/SHA1 Signature, 密钥 ID 7bd9bf62: NOKEY准备中... ################################# [100%]正在升级 / 安装... 1:nginx-release-centos-7-0.el7.ngx ################################# [100%] 此时 /etc/yum.repos.d 中增加了一个新文件 nginx.repo 12345678[root@centos7 tmp]# cat /etc/yum.repos.d/nginx.repo# nginx.repo[nginx]name=nginx repos #名称baseurl=http://nginx.org/packages/centos/7/$basearch/ #源地址gpgcheck=0 #是否检查 key，0 表示不检查enabled=1 #这里等于 0 表示不启用 baseurl，为 1 表示启用 baseurl 现在就可以通过命令 yum install -y nginx 安装 nginx 了 当服务器没有网络可用时，还能够设置本地 yum 源。此时需要手动配置 repo 文件 首先将安装光盘或 ios 文件挂载至系统 (关于挂载的更多内容请看这一篇) 12345678910[root@centos7 tmp]# mount CentOS-7-x86_64-DVD-1611.iso /mediamount: /dev/loop2 写保护，将以只读方式挂载[root@centos7 tmp]##如果是光盘则如此挂载：[root@centos7 tmp]# mount /dev/cdrom /mediamount: /dev/sr0 写保护，将以只读方式挂载[root@centos7 tmp]##卸载用 umount 或 eject[root@centos7 tmp]# umount /media[root@centos7 tmp]# eject 编辑 yum 源配置文件 123456vim /etc/yum.repos.d/local.repo [local] name=test baseurl=file:///media #这里 baseurl 写 前缀 (file://)+ 挂载点 enabled=1 gpgcheck=0 然后将原有网络源配置文件备份到另一个目录，/etc/yum.repo.d 中只保留 local.repo 文件。安装软件： 1yum install bc -y 3、源码包前面所说的 rpm 和 deb 都是二进制软件包，由于这些软件包都是已经经过编译的，用户不能设置编译选项，也不能对软件做任何更改。相对来说，使用源码包编译安装软件提供了更多的灵活性，在编译时可指定各种选项，对于有能力的用户，还可以修改源代码。下面介绍一下 linux 中是如何安装源码包的 获取源码包1wget http://mirrors.sohu.com/nginx/nginx-1.9.6.tar.gz 解压1tar zxf nginx-1.9.6.tar.gz 配置12[root@idc-v-71252 src]# cd nginx-1.9.6[root@idc-v-71252 nginx-1.9.6]# ./configure --prefix=/usr/local/nginx 这里配置选项 –prefix=/usr/local/nginx 表示指定 nginx 的安装路径为 /usr/local/nginx。 可以执行./configure –help 查看有哪些配置参数，此步骤的执行会检查系统是否符合编译要求。如果报错，很多情况下是因为少了一些编译工具，可以使用 yum 安装这些工具 (当然也可以装源码)。 在本例中报错：./configure: error: the HTTP rewrite module requires the PCRE library.。 说明少了 pcre 库，查看一下系统： 123[root@idc-v-71252 nginx-1.9.6]# rpm -qa pcrepcre-8.32-15.el7.x86_64[root@idc-v-71252 nginx-1.9.6]# 系统有 pcre 安装，但没有 devel 包，使用 yum 安装 1[root@idc-v-71252 nginx-1.9.6]# yum install pcre-devel -y 再次执行 configure 发现报错变了：./configure: error: the HTTP gzip module requires the zlib library. 重复上述操作直到所需软件都安装完毕，之后再次执行./configure –prefix=/usr/local/nginx 编译1[root@idc-v-71252 nginx-1.9.6]# make -j8 使用 make 进行编译，选项 -j 表示指定并发执行的数量，这里指定了和系统逻辑 CPU 数 (可以使用命令 grep -c “^processor” /proc/cpuinfo 查看逻辑 CPU 数) 相同的并发数。 此步骤也可能会出现报错，一般也是因为缺少包，仔细阅读报错信息，一般都不难解决。 安装1[root@idc-v-71252 nginx-1.9.6]# make install 如果没有错误，这个软件包就安装完毕了，可以在 /usr/local/nginx 中找到安装后的文件。 这里说了源码包的一般安装过程，有些源码包的安装可能会有所不同，一般源码包中都有相应的安装说明文件(README或INSTALL)，仔细阅读这些文件或者通过查询软件官网，就能找到它们的安装方法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[什么是 Kubernetes?]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-what-is-k8s%2F</url>
      <content type="text"><![CDATA[Kubernetes 是一个开源的自动化部署， Containers 和 PodsPod 生命周期Pod Phase （Pod 状态） Pending Runing Succeeded Failed Unknow Pod Conditions（Pod 条件）Container Probes（容器探查）Container Statuses（容器状态）RestartPolicy Always OnFailure Never Pod lifetime（Pod 生命周期）Examples（范例）Advanced livenessProbe example12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-httpspec: containers: - args: - /server image: gcr.io/google_containers/liveness livenessProbe: httpGet: # when "host" is not defined, "PodIP" will be used # host: my-host # when "scheme" is not defined, "HTTP" scheme will be used. Only "HTTP" and "HTTPS" are allowed # scheme: HTTPS path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 15 timeoutSeconds: 1 name: liveness]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Volumes]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-volumes%2F</url>
      <content type="text"><![CDATA[k8s 支持一下类型的 volumes emptyDir hostPath gcePersistentDisk awsElasticBlockStore nfs iscsi flocker glusterfs rbd cephfs gitRepo secret persistentVolumeClaim downwardAPI azureFileVolume azureDisk vsphereVolume Quobyte emptyDir当一个 pod 指定到一个 node 的时候，一个 emptyDir volumn 就会被创建，只要 pod 在 node 上运行，该 volume 就会一直存在。就像他的名称一样，初始化的时候是个空的目录。在这个 emptyDir 中，该 Pod 中的所有容器可以读写到相同的文件。 虽然，这个 emptyDir 会被挂载到每一个容器相同或者不同的目录。当一个 Pod 被从 node 移除后，对应的 emptyDir 中的数据会被永久删除。 注意：一个容器崩溃不会将 pod 从 node 上移除，所以一个 container 崩溃，在 emptyDir 中的数据是安全的。 Some uses for an emptyDir are: scratch space, such as for a disk-based merge sort checkpointing a long computation for recovery from crashes holding files that a content-manager container fetches while a webserver container serves the data 默认情况下，emptyDir volume are stored 不论你后台机器的存储媒介是什么。媒介可能是磁盘或 SSD，或者网络存储，这依赖于你的环境。你可以通过把 emptyDir.medium 设置成 “memory” 来告诉 k8s, 挂载一个 tmpfs（RAM-backed filesystem）。tmpfs 非常快，但是它不像 disk，当你的物理机重启后，tmpfs 将会被清空，你所生成的任何文件，都会对你的 container 的内存限制造成影响。 创建一个 pod 并且使用 emptyDir volume: 123456789101112131415apiVersion: v1kind: Podmetadata: name: test-emptydirspec: containers: - name: nginx image: nginx volumeMounts: - name: empty mountPath: /mnt volumes: - name: empty emptyDir: Medium: &quot;&quot;# 这里可以使用空或者使用&quot;memory&quot; 关键字，来制定不通的存储媒介 创建完后，我们通过进入 container 中查看 / mnt 所挂载的类型 12$ kubectl exec -p test-emptydir -it /bin/bashroot@test-emptydir:/# findmnt -m /mnt hostPath一个 hostPath Volume 挂载一个来自 host node 的文件系统的一个文件或者目录到你的 pod，这里不需要 pod 做什么， it offers a powerful escape hatch for some applications。 下面是，一些使用 hostPath 的例子 运行一个容器，需要访问 Docker 的内部，则用 hostPath 挂载 / var/lib/docker 在 container 中运行一个 cAdvisor，使用 hostPath 挂载 /dev/cgroups 当使用 hostPath 的时候需要注意，因为： 因为在不同的 node 中，文件不同，所以使用相同配置的 pod（比如，从 podTemplate 创建的）可能在不同的 node 上，有不同的行为。 当 k8s 添加 resource-aware scheduling，就不能认为资源是由 hostPath 使用（ when Kubernetes adds resource-aware scheduling, as is planned, it will not be able to account for resources used by a hostPath） 在 underlying hosts 创建的文件夹，只能被 root 写，你需要在一个特权容器中，使用 root 运行你的进程，或者更改 host 的文件权限，以便可以写入 hostPath volume。 例： 123456789101112131415161718192021222324252627282930apiVersion: v1kind: Podmetadata: name: test-pdspec: containers: - image: gcr.io/google_containers/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data--- apiVersion: v1kind: Podmetadata: name: test-hostpathspec: containers: - image: myimage name: test-container volumeMounts: - mountPath: /test-hostpath name: test-volume volumes: - name: test-volume hostPath: path: /path/to/my/dir NFS一个 nfs volume 允许一个已存在的 NFS（Network file system） 可以共享的挂载到你的 pod， 不像 emptyDir, 当一个 Pod 移除的时候，就会被擦出， nfs volume 中的内容是保留的，volume 仅仅是变为 unmounted，这意味着一个 NFS volume 可以在其中预先放入一些数据。这些数据可以被 “handed off” 在 pod 之间。 NFS 可以被多个写入着同时挂载（ NFS can be mounted by multiple writers simultaneously）。 重要： 在你可以使用 share exports 前，你必须让你自己的 NFS 服务器运行起来。 一些 k8s 的概念（pv,pvc） Persistent Volumes 定义一个持久的 disk（disk 的生命周期不和 Pod 相关联） Services to enable Pods to locate one another. 1234567891011121314151617apiVersion: v1kind: PersistentVolumemetadata: name: zk-mysql-pv-2 labels: type: nfs app: mysql tier: zkspec: capacity: storage: 20Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /mnt/usercenter/zk/mysql-pv-2 server: 192.168.1.41 glusterfs 参考：https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/glusterfs GlusterFS 是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数 PB 存储容量和处理数千客户端。同样地，Kubernetes 支持 Pod 挂载到 GlusterFS，这样数据将会永久保存。 首先需要一个 GlusterFS 环境，本文使用 2 台机器（CentOS 7）安装 GlusterFS 服务端，在 2 台机器的 /etc/hosts 配置以下信息： 192.168.3.150 gfs1 192.168.3.151 gfs2 在 2 台机器上安装： 12345$ wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm$ rpm -ivh epel-release-7-5.noarch.rpm$ wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo$ yum install glusterfs-server$ service glusterd start 注意: GlusterFS 服务端需要关闭 SELinux：修改 / etc/sysconfig/selinux 中 SELINUX=disabled。然后清空 iptables: iptables -F 安装后查询状态： 1$ service glusterd status 添加集群： 在 gfs1 上操作： 1$ gluster peer probe gfs2 在 gfs2 上操作： 1$ gluster peer probe gfs1 创建 volume ： 在 gfs1 上操作： 1234$ mkdir /data/brick/gvol0$ gluster volume create gvol0 replica 2 gfs1:/data/brick/gvol0 gfs2:/data/brick/gvol0$ gluster volume start gvol0$ gluster volume info 安装成功的话可以挂载试验下： 1$ mount -t glusterfs gfs1:/gvol0 /mnt GlusterFS 服务端安装成功后，需要在每个 Node 上安装 GlusterFS 客户端: 1$ yum install glusterfs-client 接着创建 GlusterFS Endpoint，gluasterfs-endpoints.json： 123456789101112131415161718192021222324252627282930313233&#123;"kind": "Endpoints","apiVersion": "v1","metadata": &#123;"name": "glusterfs-cluster"&#125;,"subsets": [&#123; "addresses": [ &#123; "IP": "192.168.3.150" &#125; ], "ports": [ &#123; "port": 1 &#125; ]&#125;,&#123; "addresses": [ &#123; "IP": "192.168.3.151" &#125; ], "ports": [ &#123; "port": 1 &#125; ]&#125;]&#125; 最后创建一个 Pod 挂载 GlusterFS Volume, busybox-pod.yaml: 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata:name: busyboxlabels:name: busyboxspec:containers:- image: busybox command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent name: busybox volumeMounts: - mountPath: /busybox-data name: datavolumes:- glusterfs: endpoints: glusterfs-cluster path: gvol0 name: data 查看 Pod 的容器，可以看到容器挂载 1/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~glusterfs/data =&gt; /busybox-data 在 kubernetes.io~glusterfs/data 目录下执行查询： 12$ mount | grep gvol0192.168.3.150:gvol0 on /var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~glusterfs/data type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Service]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-service%2F</url>
      <content type="text"><![CDATA[概述Kubernetes Pod 是平凡的，它门会被创建，也会死掉（生老病死），并且他们是不可复活的。 ReplicationControllers 动态的创建和销毁 Pods(比如规模扩大或者缩小，或者执行动态更新)。每个 pod 都由自己的 ip，这些 IP 也随着时间的变化也不能持续依赖。这样就引发了一个问题：如果一些 Pods（让我们叫它作后台，后端）提供了一些功能供其它的 Pod 使用（让我们叫作前台），在 kubernete 集群中是如何实现让这些前台能够持续的追踪到这些后台的？ 答案是：Service Kubernete Service 是一个定义了一组 Pod 的策略的抽象，我们也有时候叫做宏观服务。这些被服务标记的 Pod 都是（一般）通过 label Selector 决定的（下面我们会讲到我们为什么需要一个没有 label selector 的服务） 举个例子，我们假设后台是一个图形处理的后台，并且由 3 个副本。这些副本是可以相互替代的，并且前台并需要关心使用的哪一个后台 Pod，当这个承载前台请求的 pod 发生变化时，前台并不需要直到这些变化，或者追踪后台的这些副本，服务是这些去耦 对于 Kubernete 原生的应用，Kubernete 提供了一个简单的 Endpoints API，这个 Endpoints api 的作用就是当一个服务中的 pod 发生变化时，Endpoints API 随之变化，对于哪些不是原生的程序，Kubernetes 提供了一个基于虚拟 IP 的网桥的服务，这个服务会将请求转发到对应的后台 pod Defining a service(定义一个服务)一个 Kubernete 服务是一个最小的对象，类似 pod, 和其它的终端对象一样，我们可以朝 paiserver 发送请求来创建一个新的实例，比如，假设你拥有一些 Pod, 每个 pod 都开放了 9376 端口，并且均带有一个标签 app=MyApp 12345678910111213141516171819&#123;"kind": "Service","apiVersion": "v1","metadata": &#123;"name": "my-service"&#125;,"spec": &#123;"selector": &#123;"app": "MyApp"&#125;,"ports": [&#123;"protocol": "TCP","port": 80,"targetPort": 9376&#125;]&#125;&#125; 这段代码会创建一个新的服务对象，名称为：my-service，并且会连接目标端口 9376，并且带有 label app=MyApp 的 pod, 这个服务会被分配一个 ip 地址，这个 ip 是给服务代理使用的（下面我们会看到），服务的选择器会持续的评估，并且结果会被发送到一个 Endpoints 对象，这个 Endpoints 的对象的名字也叫 “my-service”. 服务可以将一个 “入端口” 转发到任何”目标端口”，默认情况下 targetPort 的值会和 port 的值相同，更有趣的是，targetPort 可以是字符串，可以指定到一个 name, 这个 name 是 pod 的一个端口。并且实际指派给这个 name 的端口可以是不同在不同的后台 pod 中，这样让我们能更加灵活的部署我们的服务，比如；我们可以在下一个更新版本中修改后台 pod 暴露的端口而不会影响客户的使用（更新过程不会打断） 服务支持 tcp 和 UDP，但是默认的是 TCP Services without selectors（没有选择器的服务） 服务总体上抽象了对 Pod 的访问，但是服务也抽象了其它的内容，比如： 比如你希望有一个额外的数据库云在生产环境中，但是在测试的时候，我们希望使用自己的数据库 我们希望将服务指向其它的服务或者其它命名空间或者其它的云平台上的服务 我们正在向 kubernete 迁移，并且我们后台并没有在 Kubernete 中 如上的情况下，我们可以定义一个服务没有选择器 12345678910111213141516&#123;&quot;kind&quot;: &quot;Service&quot;,&quot;apiVersion&quot;: &quot;v1&quot;,&quot;metadata&quot;: &#123;&quot;name&quot;: &quot;my-service&quot;&#125;,&quot;spec&quot;: &#123;&quot;ports&quot;: [&#123;&quot;protocol&quot;: &quot;TCP&quot;,&quot;port&quot;: 80,&quot;targetPort&quot;: 9376&#125;]&#125;&#125; 因为没有选择器，所以相应的 Endpoints 对象就不会被创建，但是我们手动把我们的服务和 Endpoints 对应起来 1234567891011121314151617&#123;"kind": "Endpoints","apiVersion": "v1","metadata": &#123;"name": "my-service"&#125;,"subsets": [&#123;"addresses": [&#123;"IP": "1.2.3.4" &#125;],"ports": [&#123;"port": 80 &#125;]&#125;]&#125; 这样的话，这个服务虽然没有 selector，但是却可以正常工作，所有的请求都会被转发到 1.2.3.4:80 Virtual IPs and service proxies（虚拟 IP 和服务代理）每一个节点上都运行了一个 kube-proxy，这个应用监控着 Kubermaster 增加和删除服务，对于每一个服务，kube-proxy 会随机开启一个本机端口，任何发向这个端口的请求都会被转发到一个后台的 Pod 当中，而如何选择是哪一个后台的 pod 的是基于 SessionAffinity 进行的分配。kube-proxy 会增加 iptables rules 来实现捕捉这个服务的 Ip 和端口来并重定向到前面提到的端口。 最终的结果就是所有的对于这个服务的请求都会被转发到后台的 Pod 中，这一过程用户根本察觉不到。 默认的，后台的选择是随机的，基于用户 session 机制的策略可以通过修改 service.spec.sessionAffinity 的值从 NONE 到 ClientIP。 Multi-Port Services（多端口服务）可能很多服务需要开发不止一个端口, 为了满足这样的情况，Kubernetes 允许在定义时候指定多个端口，当我们使用多个端口的时候，我们需要指定所有端口的名称，这样 endpoints 才能清楚，例如： 1234567891011121314151617181920212223242526&#123;"kind": "Service","apiVersion": "v1","metadata": &#123;"name": "my-service"&#125;,"spec": &#123;"selector": &#123;"app": "MyApp"&#125;,"ports": [&#123;"name": "http","protocol": "TCP","port": 80,"targetPort": 9376&#125;,&#123;"name": "https","protocol": "TCP","port": 443,"targetPort": 9377&#125;]&#125;&#125; 选择自己的 IP 地址我们可以在创建服务的时候指定 IP 地址，将 spec.clusterIP 的值设定为我们想要的 IP 地址即可。例如，我们已经有一个 DNS 域我们希望用来替换，或者遗留系统只能对指定 IP 提供服务，并且这些都非常难修改，用户选择的 IP 地址必须是一个有效的 IP 地址，并且要在 API server 分配的 IP 范围内，如果这个 IP 地址是不可用的，apiserver 会返回 422http 错误代码来告知是 IP 地址不可用 为什么不使用循环的 DNS一个问题持续的被提出来，这个问题就是我们为什么不使用标准的循环 DNS 而使用虚拟 IP，我们主要有如下几个原因 DNS 不遵循 TTL 查询和缓存 name 查询的问题由来已久（这个还真不知道，就是 DNS 更新的问题估计） 许多的应用的 DNS 查询查询一次后就缓存起来 即使如上亮点被解决了，但是不停的进行 DNS 进行查询，大量的请求也是很难被管理的 我们希望阻止用户使用这些可能会 “伤害” 他们的事情，但是如果足够多的人要求这么作，那么我们将对此提供支持，来作为一个可选项. Discovering services（服务的发现）Kubernetes 支持两种方式的来发现服务 ，环境变量和 DNS 环境变量当一个 Pod 在一个 node 上运行时，kubelet 会针对运行的服务增加一系列的环境变量，它支持 Docker links compatible 和普通环境变量 举例子来说： redis-master 服务暴露了 TCP 6379 端口，并且被分配了 10.0.0.11 IP 地址 那么我们就会有如下的环境变量 1234567REDIS_MASTER_SERVICE_HOST=10.0.0.11REDIS_MASTER_SERVICE_PORT=6379REDIS_MASTER_PORT=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP_PROTO=tcpREDIS_MASTER_PORT_6379_TCP_PORT=6379REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 这样的话，对系统有一个要求：所有的想要被 POD 访问的服务，必须在 POD 创建之前创建，否则这个环境变量不会被填充，使用 DNS 则没有这个问题 DNS一个可选择的云平台插件就是 DNS，DNS 服务器监控着 API SERVER ，当有服务被创建的时候，DNS 服务器会为之创建相应的记录，如果 DNS 这个服务被添加了，那么 Pod 应该是可以自动解析服务的。 举个例子来说：如果我们在 my-ns 命名空间下有一个服务叫做 “my-service”，这个时候 DNS 就会创建一个 my-service.my-ns 的记录，所有 my-ns 命名空间下的 Pod, 都可以通过域名 my-service 查询找到对应的 ip 地址，不同命名空间下的 Pod 在查找是必须使用 my-sesrvice.my-ns 才可以。 Kubernete 同样支持端口的解析，如果 my-service 有一个提供 http 的 TCP 主持的端口，那么我们可以通过查询 “_http._tcp.my-service.my-ns” 来查询这个端口。 Headless services有时候我们可能不需要一个固定的 IP 和分发，这个时候我们只需要将 spec.clusterIP 的值设置为 none 就可以了 对于这样的服务来说，集群 IP 没有分配，这个时候当你查询服务的名称的时候，DNS 会返回多个 A 记录，这些记录都是指向后端 Pod 的。Kube 代理不会处理这个服务，在服务的前端也没有负载均衡器。但是 endpoints controller 还是会创建 Endpoints This option allows developers to reduce coupling to the Kubernetes system, if they desire, but leaves them freedom to do discovery in their own way. Applications can still use a self-registration pattern and adapters for other discovery systems could easily be built upon this API. External services（外部服务）对于我们的应用程序来说，我们可能有一部分是放在 Kubernete 外部的（比如我们有单独的物理机来承担数据库的角色），Kubernetes 支持两种方式：NodePorts，LoadBalancers每一个服务都会有一个字段定义了该服务如何被调用（发现），这个字段的值可以为： ClusterIP: 使用一个集群固定 IP，这个是默认选项 NodePort：使用一个集群固定 IP，但是额外在每个 POD 上均暴露这个服务，端口 LoadBalancer：使用集群固定 IP，和 NODEPord, 额外还会申请申请一个负载均衡器来转发到服务（load balancer ） NodePort 支持 TCP 和 UDN，但是 LoadBalancers 在 1.0 版本只支持 TCP Type NodePort如果你选择了 “NodePort”，那么 Kubernetes master 会分配一个区域范围内，（默认是 30000-32767），并且，每一个 node，都会代理（proxy）这个端口到你的服务中，我们可以在 spec.ports[*].nodePort 找到具体的值 如果我们向指定一个端口，我们可以直接写在 nodePort 上，系统就会给你指派指定端口，但是这个值必须是指定范围内的。 这样的话就能够让开发者搭配自己的负载均衡器，去撘建一个 kubernete 不是完全支持的系统，又或者是直接暴露一个 node 的 ip 地址 Type LoadBalancer在支持额外的负载均衡器的的平台上，将值设置为 LoadBalancer 会提供一个负载均衡器给你的服务，负载均衡器的创建其实是异步的。下面的例子 12345678910111213141516171819202122232425262728293031&#123;"kind": "Service","apiVersion": "v1","metadata": &#123;"name": "my-service"&#125;,"spec": &#123;"selector": &#123;"app": "MyApp"&#125;,"ports": [&#123;"protocol": "TCP","port": 80,"targetPort": 9376,"nodePort": 30061&#125;],"clusterIP": "10.0.171.239","type": "LoadBalancer"&#125;,"status": &#123;"loadBalancer": &#123;"ingress": [&#123;"ip": "146.148.47.155"&#125;]&#125;&#125;&#125; 所有服务的请求均会直接到到 Pod, 具体是如何工作的是由平台决定的 缺点 我们希望使用 IPTABLES 和用户命名空间来代理虚拟 IP 能在中小型规模的平台上正常运行，但是可能出现问题在比较大的平台上当应对成千上万的服务的时候。 这个时候，使用 kube-proxy 来封装服务的请求，这使得这些变成可能 LoadBalancers 只支持 TCP，不支持 UDP Type 的值是设定好的，不同的值代表不同的功能，并不是所有的平台都需要的，但是是所有 API 需要的 Future work在将来，我们预想 proxy 的策略能够更加细致，不再是单纯的转发，比如 master-elected or sharded，我们预想将来服务会拥有真正的负载均衡器，到时候虚拟 IP 直接转发到负载均衡器 将来有倾向与将所的工作均通过iptables来进行，从而小从用户命名空间代理，这样的话会有更好的性能和消除一些原地值IP的问题，尽管这样的会减少一些灵活性.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Secrets]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-secrets%2F</url>
      <content type="text"><![CDATA[Secrets 构建yaml 构建123456789test-secret.yaml：apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: password: MWYyZDFlMmU2N2Rm username: YWRtaW4= 构建 test-secret Secret 1$ kubectl create -f demo.yaml 文件构建创建 password.txt, username.txt: 123# Create files needed for rest of example.$ echo -n "admin" &gt; ./username.txt$ echo -n "1f2d1e2e67df" &gt; ./password.txt 构建 db-user-pass Secret 12$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txtsecret "db-user-pass" created 手动创建 SecretEncode: 1234$ echo -n "admin" | base64YWRtaW4=$ echo -n "1f2d1e2e67df" | base64MWYyZDFlMmU2N2Rm Decode: 12$ echo "MWYyZDFlMmU2N2Rm" | base64 -d1f2d1e2e67df 在 pod 中应用 Secret案例 1：在环境变量中的使用 Secret参数列表： env[x].valueFrom.secretKeyRef. 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: secret-env-podspec: containers: - name: mycontainer image: redis env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never 案例 2：通过数据卷插件使用 Secret参数列表： spec.volumes[].secret.secretName spec.containers[].volumeMounts[] spec.containers[].volumeMounts[].readOnly = true spec.containers[].volumeMounts[].mountPath pod 应用： 12345678910111213141516171819202122232425&#123; "apiVersion": "v1", "kind": "Pod", "metadata": &#123; "name": "mypod", "namespace": "myns" &#125;, "spec": &#123; "containers": [&#123; "name": "mypod", "image": "redis", "volumeMounts": [&#123; "name": "foo", "mountPath": "/etc/foo", "readOnly": true &#125;] &#125;], "volumes": [&#123; "name": "foo", "secret": &#123; "secretName": "mysecret" &#125; &#125;] &#125;&#125; Secrets 挂载到数据卷指定目录下 参数列表： spec.volumes[].secret.items 123456789101112131415161718192021222324252627282930&#123; &quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot;: &quot;Pod&quot;, &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;mypod&quot;, &quot;namespace&quot;: &quot;myns&quot; &#125;, &quot;spec&quot;: &#123; &quot;containers&quot;: [&#123; &quot;name&quot;: &quot;mypod&quot;, &quot;image&quot;: &quot;redis&quot;, &quot;volumeMounts&quot;: [&#123; &quot;name&quot;: &quot;foo&quot;, &quot;mountPath&quot;: &quot;/etc/foo&quot;, &quot;readOnly&quot;: true &#125;] &#125;], &quot;volumes&quot;: [&#123; &quot;name&quot;: &quot;foo&quot;, &quot;secret&quot;: &#123; &quot;secretName&quot;: &quot;mysecret&quot;, &quot;items&quot;: [&#123; &quot;key&quot;: &quot;username&quot;, &quot;path&quot;: &quot;my-group/my-username&quot; &#125;] &#125; &#125;] &#125;&#125;# 挂在目录：/etc/foo/my-group/my-username 案例 3：加密拉取镜像myregistrykey.yaml： 1234567891011121314151617181920apiVersion: v1kind: Secretmetadata: name: myregistrykey namespace: awesomeappsdata: .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg==type: kubernetes.io/dockerconfigjsonpod 应用：apiVersion: v1kind: Podmetadata: name: foo namespace: awesomeappsspec: containers: - name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: - name: myregistrykey]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[构建 High-Availability 集群]]></title>
      <url>%2F2016%2F10%2F26%2Fkuberntes-ha%2F</url>
      <content type="text"><![CDATA[这篇文章主要介绍怎样部署一个 高可用(HA) 的 Kubernetes 集群。这是一个相当高阶的主题。 概述建立一个真正可靠、高可用性的分布式系统需要若干步骤，它就像是穿着内衣、裤子、皮带、吊带，另外一对内裤和另一条裤子。下面，我们对这些步骤进行详细的探究，但在这里进行了一些总结，以帮助指导用户。 所涉及的步骤如下： ---- 下面是构建完成后，系统应该是什么样子： 初始化设置]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Heapster 在 Kubernetes 集群中的部署--包含 InfluxDB 后台 和 Grafana UI]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-heapster%2F</url>
      <content type="text"><![CDATA[原文引用：https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md 启动 k8s 集群启动所有的 pods 和 services为部署 Heapster 和 InfluxDB，你需要通过 deploy/kube-config/influxdb 创建 kubernetes 资源。确保你已经在 root目录下 checkout 有效的 Heapster 资源。 1git clone https://github.com/kubernetes/heapster.git 运行： 1$ kubectl create -f deploy/kube-config/influxdb/ 默认 Grafana 服务负载均衡器的请求。 如果在你的集群中不可用，可以考虑更换 NodePort。为了访问 Grafana，为其指派 extenal IP 。默认的用户名和密码都是 &#39;admin&#39;。在登录到 Grafana 后，为其添加 InfluxDB 数据源。 InfluxDB 的 url 为 http://localhost:8086。数据库名为&#39;k8s&#39;。默认的用户名和密码均为&#39;root&#39;。 Grafana 的 InfluxDB 文档在这里 为了更好的理解 InfluxDB，可以了解 storage schema Grafana 通过使用模版设置填充节点和pods。 Grafana Web 界面也能够通过 api-server proxy 访问。一旦资源被创建后，就可以通过 kubectl cluster-info 查看其 URL。 Troubleshooting也可以通过 debugging documentation 查看。 1. 如果 Grafana service 无法访问，可能是因为服务还没启动。使用 kubectl 验证 heapster， influxdb 和 grafana 的 pods 是否存活。 1234567$ kubectl get pods --namespace=kube-system...monitoring-grafana-927606581-0tmdx 1/1 Running 0 6dmonitoring-influxdb-3276295126-joqo2 1/1 Running 0 15d...$ kubectl get services --namespace=kube-system monitoring-grafana monitoring-influxdb 2. 如果你发现 InfluxDB 占用过多的 CPU 或 内存，可以考虑在中限制 InfluxDB &amp; Grafana pod 的资源。在 用 cpu: &lt;millicores&gt; and memory: &lt;bytes&gt;2. 如果你发现 InfluxDB 占用过多的 CPU 或 内存，可以考虑在中限制 InfluxDB &amp; Grafana pod 的资源。在 Controller Spec 中添加 cpu: &lt;millicores&gt; and memory: &lt;bytes&gt;，运行: 1kubectl apply -f deploy/kube-config/influxdb/influxdb-grafana-controller.yaml 重启 Controllers 并删除就的 InfluxDB pods。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Labels]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-labels%2F</url>
      <content type="text"><![CDATA[简介标签其实就一对 key/value，被关联到对象上，比如 Pod，标签的使用我们倾向于能够标示对象的特殊特点，并且对用户而言是有意义的（就是一眼就看出了这个 Pod 是尼玛数据库），但是标签对内核系统是没有直接意义的。标签可以用来划分特定组的对象（比如，所有女的），标签可以在创建一个对象的时候直接给与，也可以在后期随时修改，每一个对象可以拥有多个标签，但是，key 值必须是唯一的 1234"labels": &#123; "key1" : "value1", "key2" : "value2" &#125; 我们最终会索引并且反向索引（ reverse-index ）labels，以获得更高效的查询和监视，把他们用到 UI 或者 CLI 中用来排序或者分组等等。我们不想用那些不具有指认效果的 label 来污染 label，特别是那些体积较大和结构型的的数据。不具有指认效果的信息应该使用 annotation 来记录。 MotivationLabel 可以让用户将他们自己的有组织目的的结构以一种松耦合的方式应用到系统的对象上，且不需要客户端存放这些对应关系（mappings）。 服务部署和批处理管道通常是多维的实体（例如多个分区或者部署，多个发布轨道，多层，每层多微服务）。管理通常需要跨越式的切割操作，这会打破有严格层级展示关系的封装，特别对那些是由基础设施而非用户决定的很死板的层级关系。 Label 例子 12345"release" : "stable", "release" : "canary", … "environment" : "dev", "environment" : "qa", "environment" : "production" "tier" : "frontend", "tier" : "backend", "tier" : "middleware" "partition" : "customerA", "partition" : "customerB", … "track" : "daily", "track" : "weekly" Label 的语法和字符集Label 其实是一对 key/value，有效的 Label keys 必须是部分：一个可选前缀 + 名称，通过 / 来区分，名称部分是必须的，并且最多 63 个字符，开始和结束的字符必须是字母或者数字，中间是字母数字和”_”，”-“，”.”，前缀是刻有可无的，如果指定了，那么前缀必须是一个 DNS 子域，一系列的 DNSlabel 通过”.” 来划分，长度不超过 253 个字符，”/“ 来结尾。如果前缀被省略了，这个 Label 的 key 被假定为对用户私有的，自动系统组成部分（比如 kube-scheduler, kube-controller-manager, kube-apiserver, kubectl）, 这些为最终用户添加标签的必须要指定一个前缀，Kuberentes.io 前缀是为 Kubernetes 内核部分保留的。 合法的 label 值必须是 63 个或者更短的字符。要么是空，要么首位字符必须为字母数字字符，中间必须是横线，下划线，点或者数字字母。 Label 选择器与 name 和 UID 不同，label 不提供唯一性。通常，我们会看到很多对象有着一样的 label。通过 label 选择器，客户端 / 用户能方便辨识出一组对象。label 选择器是 kubernetes 中核心的组织原语。 API 目前支持两种选择器：基于相等的和基于集合的。一个 label 选择器一可以由多个必须条件组成，由逗号分隔。在多个必须条件指定的情况下，所有的条件都必须满足，因而逗号起着 AND 逻辑运算符的作用。 一个空的 label 选择器（即有 0 个必须条件的选择器）会选择集合中的每一个对象。 一个 null 型 label 选择器（仅对于可选的选择器字段才可能）不会返回任何对象。 Equality-based requirement基于相等性或者不相等性的条件允许用 label 的键或者值进行过滤。匹配的对象必须满足所有指定的 label 约束，尽管他们可能也有额外的 label。有三种运算符是允许的，”=”，”==” 和 “!=”。前两种代表相等性（他们是同义运算符），后一种代表非相等性。例如： 12environment = productiontier != frontend 第一个选择所有键等于 environment 值为 production 的资源。后一种选择所有键为 tier 值不等于 frontend 的资源，和那些没有键为 tier 的 label 的资源。要过滤所有处于 production 但不是 frontend 的资源，可以使用逗号操作符， 12environment=production,tier!=frontendenvironment=production,tier!=frontend 基于 set 的条件基于集合的 label 条件允许用一组值来过滤键。支持三种操作符: in ， notin , 和 exists(仅针对于 key 符号) 。例如： 1234environment in (production, qa)tier notin (frontend, backend)partition!partitio 第一个例子，选择所有键等于 environment ，且 value 等于 production 或者 qa 的资源。 第二个例子，选择所有键等于 tier 且值是除了 frontend 和 backend 之外的资源，和那些没有 label 的键是 tier 的资源。 第三个例子，选择所有所有有一个 label 的键为 partition 的资源；值是什么不会被检查。 第四个例子，选择所有的没有 lable 的键名为 partition 的资源；值是什么不会被检查。 类似的，逗号操作符相当于一个 AND 操作符。因而要使用一个 partition 键（不管值是什么），并且 environment 不是 qa 过滤资源可以用 partition,environment notin (qa) 。 基于集合的选择器是一个相等性的宽泛的形式，因为 environment=production 相当于 environment in (production) ，与 != and notin 类似。 基于集合的条件可以与基于相等性 的条件混合。例如， partition in (customerA,customerB),environment!=qa 。 APILIST 和 WATCH 过滤LIST 和 WATCH 操作，可以使用 query 参数来指定 label 选择器来过滤返回对象的集合。两种条件都可以使用： 基于相等性条件： ?labelSelector=environment%3Dproduction,tier%3Dfrontend 基于集合条件的： 1labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29 两种 label 选择器风格都可以用来通过 REST 客户端来列表或者监视资源。比如使用 kubectl 来针对 apiserver ，并且使用基于相等性的条件，可以用： 1$ kubectl get pods -l environment=production,tier=frontend or using set-based requirements: 或者使用基于集合的条件： 1$ kubectl get pods -l 'environment in (production),tier in (frontend)' 如以上已经提到的，基于集合的条件表达性更强。例如，他们可以实现值上的 OR 操作： 1$ kubectl get pods -l 'environment in (production, qa)' 或者通过 exists 操作符进行否定限制匹配： 1$ kubectl get pods -l 'environment,environment notin (frontend)' Set references in API objects一些 Kubernetes 对象，比如 service 和 replication controller 的，也使用 label 选择器来指定其他资源的集合，比如 pods。 Service and ReplicationController一个 service 针对的 pods 的集合是用 label 选择器来定义的。类似的，一个 replicationcontroller 管理的 pods 的群体也是用 label 选择器来定义的。 对于这两种对象的 Label 选择器是用 map 定义在 json 或者 yaml 文件中的，并且只支持基于相等性的条件： 123"selector": &#123;"component" : "redis",&#125; 或者 12selector:component: redis 这个选择器（分别是位于 json 或者 yaml 格式的）相等于 component=redis 或者 component in(redis) 。 Job 和其他新的资源较新的资源，如 job，也支持基于集合的条件。 123456selector:matchLabels: 至少于一个component: redismatchExpressions:– &#123;key: tier, operator: In, values: [cache]&#125;– &#123;key: environment, operator: NotIn, values: [dev]&#125; matchLabels 是一个键值对的映射。一个单独的 {key,value} 相当于 matchExpressions 的一个元素，它的键字段是”key”,操作符是 In ，并且值数组值包含”value”。 matchExpressions 是一个pod的选择器条件的列表。合法的操作符包含In, NotIn, Exists, and DoesNotExist。在In和NotIn的情况下，值的组必须不能为空。所有的条件，包含 matchLabels andmatchExpressions 中的，会用AND符号连接，他们必须都被满足以完成匹配。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Deployment 与 Replica Set]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-deployment-and-replica-set%2F</url>
      <content type="text"><![CDATA[Deployment简介Deployment 是新一代用于 Pod 管理的对象，与 RC 相比，它提供了更加完善的功能，使用起来更加简单方便。 Deployment 与 RC 的定义也基本相同，需要注意的是 apiVersion 和 kind 是有差异的。 Deployment 会声明 Replica Set 和 Pod Deployment 更新 rolling-update。只有当 Pod template 发生变更时，Deployment 才会触发 rolling-update。此时 Deployment 会自动完成更新，且会保证更新期间始终有一定数量的 Pod 为运行状态。 其他变更，如暂停 / 恢复更新、修改 replica 数量、修改变更记录数量限制等操作。这些操作不会修改 Pod 参数，只影响 Deployment 参数，因此不会触发 rolling-update。 通过 kubectl edit 指令更新 Deployment，可以将例子中的 nginx 镜像版本改成 1.9.1 来触发一次 rolling-update。期间通过 kubectl get 来查看 Deployment 的状态，可以发现 CURRENT、UP-TO-DATE 和 AVAILABLE 会发生变化。 删除 Deploymentkubectl delete 指令可以用来删除 Deployment。需要注意的是通过 API 删除 Deployment 时，对应的 RS 和 Pods 不会自动删除，需要依次调用删除 Deployment 的 API、删除 RS 的 API 和删除 Pods 的 API。使用 RS 管理 Pod Replica Set - RSReplica Set（简称 RS ）是 k8s 新一代的 Pod controller。与 RC 相比仅有 selector 存在差异，RS 支持了 set-based selector（可以使用 in、notin、key 存在、key 不存在四种方式来选择满足条件的 label 集合）。Deployment 是基于 RS 实现的，我们可以使用 kubectl get rs 命令来查看 Deployment 创建的 RS： 1234$ kubectl get rsNAME DESIRED CURRENT AGEnginx-deployment-1564180365 3 3 6snginx-deployment-2035384211 0 0 36s 由 Deployment 创建的 RS 的命名规则为 “-“。由于之前的操作中我们触发了一次 rolling-update，因此会查看到两个 RS。更新前后的 RS 都会保留下来。 弹性伸缩与 RC 相同，只需要修改. spec.replicas 就可以实现 Pod 的弹性伸缩。 重新部署如果设置 Deployment 的 .spec.strategy.type==Recreate 时，更新时会将所有已存在的 Pod 杀死后再创建新 Pod。与 RC 不同的是，修改 Deployment 的 Pod template 后更新操作将会自动执行，无需手动删除旧 Pod。 更完善的 rolling-update与 RC 相比，Deployment 提供了更完善的 rolling-update 功能： Deployment 不需要使用 kubectl rolling-update 指令来触发 rolling-update，只需修改 pod template 内容即可。这条规则同样适用于使用 API 来修改 Deployment 的场景。这就意味着使用 API 集成的应用，无须自己实现一套基于 RC 的 rolling-udpate 功能，Pod 更新全都交给 Deployment 即可。 Deployment 会对更新的可用性进行检查。当使用新 template 创建的 Pod 无法运行时，Deployment 会终止更新操作，并保留一定数量的旧版本 Pod 来提供服务。例如我们更新 nginx 镜像版本为 1.91（一个不存在的版本），可以看到以下结果： 此外 Deployment 还支持在 rolling-update 过程中暂停和恢复更新过程。通过设置. spec.paused 值即可暂停和恢复更新过程。暂停更新后的 Deployment 可能会处于与以下示例类似的状态： 支持多重更新，在更新过程中可以执行新的更新操作。Deployment 会保证更新结果为最后一次更新操作的执行结果。 影响更新的一些参数： spec.minReadySeconds 参数用来设置确认运行状态的最短等待时间。更新 Pod 之后，Deployment 至少会等待配置的时间再确认 Pod 是否处于运行状态。也就是说在更新一批 Pod 之后，Deployment 会至少等待指定时间再更新下一批 Pod。 spec.strategy.rollingUpdate.maxUnavailable 用来控制不可用 Pod 数量的最大值，从而在删除旧 Pod 时保证一定数量的可用 Pod。如果配置为 1，且 replicas 为 3。则更新过程中会保证至少有 2 个可用 Pod。默认为 1。 spec.strategy.rollingUpdate.maxSurge 用来控制超过期望数量的 Pod 数量最大值，从而在创建新 Pod 时限制总量。如配置为 1，且 replicas 为 3。则更新过着中会保证 Pod 总数量最多有 4 个。默认为 1。 后两个参数不能同时为 0。 Deployment 模版：12345678910111213141516apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: my-nginxspec: replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes Dashboard]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-dashboard%2F</url>
      <content type="text"><![CDATA[原文引用：https://github.com/kubernetes/dashboard#kubernetes-dashboard Kubernetes Dashboard 是一个通用的 Kubernetes 集群 web UI，它允许用户管理在集群中运行的应用，同时也能够管理集群本身。 部署通过如下指令检查集群中是否已安装 Dashboard： 1$ kubectl get pods --all-namespaces | grep dashboard 如果没有安装，通过以下指令安装最新的稳定版本： 1$ kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml 你也可安装最新的非稳定版本–development guide 1kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard-head.yaml 注意：为了使数据和图表可用，你需要在集群中部署 heapster。 使用 来访问 Dashboard 最简单的方法是使用 kubectl。运行如下指令： 1$ kubectl proxy kubectl 将会处理 apiserver 认证并且使 Dashboard 能够通过 http://localhost:8001/ui 访问。 UI 只能在执行命令的的机器上访问。通过 kubectl proxy --help 指令查看更多。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kubernetes 概述]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-overview%2F</url>
      <content type="text"><![CDATA[Kubernetes 是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes 提供了应用部署，规划，更新，维护的一种机制。 Kubernetes 一个核心的特点就是能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行着（比如用户想让 apache 一直运行，用户不需要关心怎么去做，Kubernetes 会自动去监控，然后去重启，新建，总之，让 apache 一直提供服务），管理员可以加载一个微型服务，让规划器来找到合适的位置，同时，Kubernetes 也系统提升工具以及人性化方面，让用户能够方便的部署自己的应用（就像 canary deployments）。 现在 Kubenetes 着重于不间断的服务状态（比如 web 服务器或者缓存服务器）和原生云平台应用（Nosql）, 在不久的将来会支持各种生产云平台中的各种服务，例如，分批，工作流，以及传统数据库。 在 Kubenetes 中，所有的容器均在 Pod 中运行, 一个 Pod 可以承载一个或者多个相关的容器，在后边的案例中，同一个 Pod 中的容器会部署在同一个物理机器上并且能够共享资源。一个 Pod 也可以包含 O 个或者多个磁盘卷组（volumes）, 这些卷组将会以目录的形式提供给一个容器，或者被所有 Pod 中的容器共享，对于用户创建的每个 Pod, 系统会自动选择那个健康并且有足够容量的机器，然后创建类似容器的容器, 当容器创建失败的时候，容器会被 node agent 自动的重启, 这个 node agent 叫 kubelet, 但是，如果是 Pod 失败或者机器，它不会自动的转移并且启动，除非用户定义了 replication controller。 用户可以自己创建并管理 Pod,Kubernetes 将这些操作简化为两个操作：基于相同的 Pod 配置文件部署多个 Pod 复制品；创建可替代的 Pod 当一个 Pod 挂了或者机器挂了的时候。而 Kubernetes API 中负责来重新启动，迁移等行为的部分叫做 “replication controller”，它根据一个模板生成了一个 Pod, 然后系统就根据用户的需求创建了许多冗余，这些冗余的 Pod 组成了一个整个应用，或者服务，或者服务中的一层。一旦一个 Pod 被创建，系统就会不停的监控 Pod 的健康情况以及 Pod 所在主机的健康情况，如果这个 Pod 因为软件原因挂掉了或者所在的机器挂掉了，replication controller 会自动在一个健康的机器上创建一个一摸一样的 Pod, 来维持原来的 Pod 冗余状态不变，一个应用的多个 Pod 可以共享一个机器。 我们经常需要选中一组 Pod，例如，我们要限制一组 Pod 的某些操作，或者查询某组 Pod 的状态，作为 Kubernetes 的基本机制，用户可以给 Kubernetes Api 中的任何对象贴上一组 key:value 的标签，然后，我们就可以通过标签来选择一组相关的 Kubernetes Api 对象，然后去执行一些特定的操作，每个资源额外拥有一组（很多） keys 和 values, 然后外部的工具可以使用这些 keys 和 vlues 值进行对象的检索，这些 Map 叫做 annotations（注释）。 Kubernetes 支持一种特殊的网络模型，Kubernetes 创建了一个地址空间，并且不动态的分配端口，它可以允许用户选择任何想使用的端口，为了实现这个功能，它为每个 Pod 分配 IP 地址。 现代互联网应用一般都会包含多层服务构成，比如 web 前台空间与用来存储键值对的内存服务器以及对应的存储服务，为了更好的服务于这样的架构，Kubernetes 提供了服务的抽象，并提供了固定的 IP 地址和 DNS 名称，而这些与一系列 Pod 进行动态关联，这些都通过之前提到的标签进行关联，所以我们可以关联任何我们想关联的 Pod，当一个 Pod 中的容器访问这个地址的时候，这个请求会被转发到本地代理（kube proxy）, 每台机器上均有一个本地代理，然后被转发到相应的后端容器。Kubernetes 通过一种轮训机制选择相应的后端容器，这些动态的 Pod 被替换的时候, Kube proxy 时刻追踪着，所以，服务的 IP 地址（dns 名称），从来不变。 所有 Kubernetes 中的资源，比如 Pod, 都通过一个叫 URI 的东西来区分，这个 URI 有一个 UID，URI 的重要组成部分是：对象的类型（比如 pod），对象的名字，对象的命名空间，对于特殊的对象类型，在同一个命名空间内，所有的名字都是不同的，在对象只提供名称，不提供命名空间的情况下，这种情况是假定是默认的命名空间。UID 是时间和空间上的唯一。 Kubernetes 是什么？Kubernetes 一个用于容器集群的自动化部署、扩容以及运维的开源平台。 通过 Kubernetes, 你可以快速有效地响应用户需求： 快速而有预期地部署你的应用 极速地扩展你的应用 无缝对接新的应用功能 节省资源，优化硬件资源的使用 我们希望培育出一个组件及工具的生态，帮助大家减轻在公有云及私有云上运行应用的负担。 Kubernetes 特点: 可移植: 支持公有云，私有云，混合云，多重云（multi-cloud） 可扩展: 模块化, 插件化, 可挂载, 可组合 自愈: 自动布置，自动重启，自动复制，自动扩展 Kubernetes 始于 Google 2014 年的一个项目。 Kubernetes 的构建基于 Google 十多年运行大规模负载产品的经验，同时也吸取了社区中最好的意见和经验。 为什么要选择容器？当容器技术这么热门的时候有是不是在疑惑为什么要选用这样的技术呢？传统的应用部署方式是通过操作系统的包管理器来安装应用。然而这样做的一个劣势在于，它把应用的运行，配置，库和生存周期和机器的操作系统纠缠在一起。当然你可以通过创建虚机镜像的方式来获得可以预期的前滚和回滚操作，然而虚拟机太重量级并且不可移植。 新的方式是通过部署基于操作系统级别虚拟化的容器进行虚拟化而非通过硬件来进行虚拟化。这些容器之间相互隔离：它们有自己的文件系统，然而它们也无法看到彼此之间的进程，并且它们之间的计算资源也是有界限的。相较于虚拟机容器也更容易部署，并且因为它们是和底层设施和机器文件系统解耦的，它们可以在云和不同版本的操作系统间进行迁移。 因为容器小而快，一个应用可以被打包进一个容器映像。正是应用与容器镜像间一对一的关系解锁了容器的很多优点： 在 build 或者 release 的阶段（而非部署阶段）可以创建不变的容器镜像，因为每个应用都用和其他的应用栈相组合，也不依赖于生产环境基础设施。这使 容器比虚机要更加透明，这更便于监控和管理。尤其是因为窗口的进程的生命周期是被基础设施直接管理而不是被容器中的进程管理器隐藏起来管理。 因为一个容器包含一个应用，这让对容器的管理等同于对应用部署的管理。 总结一下容器的优点： 敏捷地应用创建和部署：相较于 VM 增加了容器镜像创建的效率。 持续开发，集成和部署：通过快速的回滚操作（因为镜像的稳定性）提供可靠的经常的容器镜像的创建和部署。 开发和运行相分离：在 build 或者 release 的阶段（而非部署阶段），使得应用和基础设施解耦。 开发，测试和生产环境的持续：在笔记本上可以像在云中一样的运行。 云和操作系统版本的可移植性：可以运行在 Ubuntu, RHEL, CoreOS, on-prem, Google Container Engine, 和任何其它的运行环境中。 应用为中心的管理：提升了虚拟化的层次，从虚拟硬件上运行操作系统的抽象到操作系统中应用逻辑资源的虚拟。 松耦合，分布式，弹性，自由的微服务：应用被打散成更小的，独立的小碎片并且可以动态地部署和管理——而非是一个在用途单一的庞大机器中运行的一个臃肿堆栈中。 资源隔离：可以预测的应用性能。 资源使用：高效。 为什么需要 Kubernetes，利用它又能做些什么呢？Kubernetes 可以安排物理机或者虚拟机上运行的应用容器的使用。 当然它不只可以做这些。 为了充分发挥它的潜能，你需要剪断物理机虚拟机的束缚。 然而，一旦特定的容器不再局限于特定的主机，主机为中心的基础设施便不再适用了：组管理，负载均衡，自动扩展等。你需要的是以容器为中心的基础设施。而这正是 Kubernetes 所提供的。 Kubernetes 可以满足很多运行环境中应用的通用的需求，比如： 进程协同，利用复合应用保证应用和容器一对一的模型。 存储系统挂载 分发密钥 应用健康检测 应用实例复制 水平自动扩展 命名和发现 负载均衡 滚动更新 资源监控 日志访问 自检和调试 识别和认证 这为 PaaS 提供了 IaaS 层的便利，提供了基础设施提供者间的可移植性。 Kubernetes 是个什么样的平台呢？尽管 Kubernetes 提供了很多功能，总有一些新的场景可以从这些功能中获益。特定的应用工作流可以提高开发者的开发速度。最新可以接受的组合常常需要强劲的大规模的自动化。这也是为什么 Kubernetes 构建以来为什么要做一个让应用的部署、扩展和管理更便捷的生态平台的原因。 Labels 让用户可以随心所欲地组织自己的资源。 Annotations 让用户可以给资源添加定制化的信息以充分使用自己的工作流，提供一种简单的管理工具。 此外，Kubernetes control plane 本身也是基于公布给开发者和用户相同 的一组 API。用户可以自己定开发自己的 controllers, schedulers 等。如果愿意，它们甚至可以用自己的 API 开发自己的 command-line tool. 这样的设计也让很多其它系统可以构建于 Kubernetes 之上。 Kubernetes 不是什么？Kubernetes 不是一个传统的，包罗一切的 PaaS 系统。我们保留用户的选择，这一点非常重要。 Kubernetes 不限制支持应用的种类。它不限制应用框架，或者支持的运行时语言，也不去区分 “apps” 或者 “services”。 Kubernetes 致力于支持不同负载应用，包括有状态、无状态、数据处理类型的应用。只要这个应用可以在容器里运行，那么它就可以在 Kubernetes 上很多地运行。 Kubernetes 不提供中间件（如 message buses），数据处理框架（如 Spark），数据库 (如 Mysql)，或者集群存储系统 (如 Ceph)。但这些应用都可以运行于 Kubernetes。 Kubernetes 没有一个点击即可用的应用市场。 Kubernetes 不部署源码不编译应用。持续集成的 (CI) 工作流方面，不同的用户有不同的需求和偏好，因此，我们提供分层的 CI 工作流，但并不定义它应该怎么做。 Kubernetes 允许用户选择自己的日志、监控和报警系统。 Kubernetes 不提供可理解的应用配置语言 (e.g., jsonnet). Kubernetes 不提供或者任何综合的机器配置，维护，管理或者自愈系统。 另一方面，大量的 Paas 系统都可以运行在 Kubernetes 上，比如 Openshift, Deis, 和 Gondor。你可以构建自己的 Paas 平台，CI 集成。 因为 Kubernetes 运行在应用而非硬件层面，它提供了普通的 Paas 平台提供的一些通用功能，比如部署，扩展，负载均衡，日志，监控等。然而，Kubernetes 并非一个庞然大物，这些功能是可选的。 另外，Kubernetes 不仅仅是一个 “编排系统”；它消弥了编排的需要。”编排” 的定义是指执行一个预定的工作流：先做 A，之后 B，然后 C。相反地，Kubernetes 是由一系列独立的、可组合的驱使当前状态走向预想状态的控制进程组成的。怎么样从 A 到 C 并不重要：达到目的就好。当然也是需要中心控制的；方法更像排舞的过程。这让这个系统更加好用更加强大、健壮、 有弹性且可扩展。 Kubernetes 单词是什么意思呢? 为什么又叫 K8s?Kubernetes起源于希腊语, 是”舵手”或者”领航员”的意思，是”管理者”和”控制论”的根源。 K8s是把用8代替8个字符”ubernete”而成的缩写。 快速跳转 kuberntes 概述 Kubernetes 构架设计 Deployment 与 Replica Set Replication Controller - RC Service Pods Labels Volumes ConfigMap Secrets 在 Linux 上安装 kubernetes]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ConfigMap]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-configmap%2F</url>
      <content type="text"><![CDATA[ConfigMap 构建yaml 构建example-config.yaml： 12345678910111213kind: ConfigMapapiVersion: v1metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: defaultdata: example.property.1: hello example.property.2: world example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3 构建 example-config ConfigMap 1$ kubectl create -f demo.yaml 文件目录构建 – Creating from directories1234567891011121314151617#目录下有如下两个 properties 配置文件$ ls docs/user-guide/configmap/kubectl/game.propertiesui.properties$ cat docs/user-guide/configmap/kubectl/game.propertiesenemies=alienslives=3enemies.cheat=trueenemies.cheat.level=noGoodRottensecret.code.passphrase=UUDDLRLRBABASsecret.code.allowed=truesecret.code.lives=30$ cat docs/user-guide/configmap/kubectl/ui.propertiescolor.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNice 如下命令将目录下的配置文件构建在 ConfigMap 中： 1$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl 12345678910111213141516171819202122232425# 查看 ConfigMap$ kubectl get configmaps game-config -o yamlapiVersion: v1data: game.properties: |- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNicekind: ConfigMapmetadata: creationTimestamp: 2016-02-18T18:34:05Z name: game-config namespace: default resourceVersion: &quot;407&quot;- selfLink: /api/v1/namespaces/default/configmaps/game-config uid: 30944725-d66e-11e5-8cd0-68f728db1985 文件构建 – Creating from files123$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties --from-file=docs/user-guide/configmap/kubectl/ui.properties# 以 yaml 格式查看上面的 ConfigMap$ kubectl get configmaps game-config-2 -o yaml 字面量构建 – Creating from literal values12$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm$ kubectl get configmaps special-config -o yaml ConfigMap 在 Pods 中的应用案例 1：ConfigMap 在环境变量中的使用1234567891011121314151617181920212223242526272829303132333435special-config.yaml：apiVersion: v1kind: ConfigMapmetadata: name: special-config namespace: defaultdata: special.how: very special.type: charmpod 应用:apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot;] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type restartPolicy: Never# 打印： SPECIAL_LEVEL_KEY=very SPECIAL_TYPE_KEY=charm 案例 2： ConfigMap 作为命令行参数使用 $(VAR_NAME)-special-config.yaml: 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: ConfigMapmetadata: name: special-config namespace: defaultdata: special.how: very special.type: charmpod:apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type restartPolicy: Never# 打印： very charm 案例 3： 通过数据卷插件使用 ConfigMapspecial-config.yaml 12345678apiVersion: v1kind: ConfigMapmetadata: name: special-config namespace: defaultdata: special.how: very special.type: charm pod: 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/special.how&quot;] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config restartPolicy: Never# 打印： very ConfigMap 挂载到数据卷指定目录下 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: dapi-test-podspec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/path/to/special-key&quot;] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config items: - key: special.how path: path/to/special-key restartPolicy: Never# 挂载文件为：/etc/config/path/to/special-key# 打印 very]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 构架设计]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-architecture%2F</url>
      <content type="text"><![CDATA[Kubernetes 集群包含有节点代理 kubelet 和 Master 组件 (APIs, scheduler, etc)，一切都基于分布式的存储系统。下面这张图是 Kubernetes 的架构图。 原文：kubernetes architecture Kubernetes 节点在这张系统架构图中，我们把服务分为运行在工作节点上的服务和组成集群级别控制板的服务。 Kubernetes 节点有运行应用容器必备的服务，而这些都是受 Master 的控制。 每次个节点上当然都要运行 Docker。Docker 来负责所有具体的映像下载和容器运行。 kubeletkubelet 负责管理 pods 和它们上面的容器，images 镜像、volumes、etc。 kube-proxy每一个节点也运行一个简单的网络代理和负载均衡（详见 services FAQ )（PS: 官方 英文）。 正如 Kubernetes API 里面定义的这些服务（详见 the services doc）（PS: 官方 英文）也可以在各种终端中以轮询的方式做一些简单的 TCP 和 UDP 传输。 服务端点目前是通过 DNS 或者环境变量 (Docker-links-compatible 和 Kubernetes{FOO}_SERVICE_HOST 及 {FOO}_SERVICE_PORT 变量都支持)。这些变量由服务代理所管理的端口来解析。 Kubernetes 控制面板Kubernetes 控制面板可以分为多个部分。目前它们都运行在一个 master 节点，然而为了达到高可用性，这需要改变。不同部分一起协作提供一个统一的关于集群的视图。 etcd所有 master 的持续状态都存在 etcd 的一个实例中。这可以很好地存储配置数据。因为有 watch(观察者) 的支持，各部件协调中的改变可以很快被察觉。 Kubernetes API ServerAPI 服务提供 Kubernetes API （PS: 官方 英文）的服务。这个服务试图通过把所有或者大部分的业务逻辑放到不两只的部件中从而使其具有 CRUD 特性。它主要处理 REST 操作，在 etcd 中验证更新这些对象（并最终存储）。 Scheduler调度器把未调度的 pod 通过 binding api 绑定到节点上。调度器是可插拔的，并且我们期待支持多集群的调度，未来甚至希望可以支持用户自定义的调度器。 Kubernetes 控制管理服务器所有其它的集群级别的功能目前都是由控制管理器所负责。例如，端点对象是被端点控制器来创建和更新。这些最终可以被分隔成不同的部件来让它们独自的可插拔。 replicationcontroller 是一种建立于简单的 pod API之上的一种机制。一旦实现，我们最终计划把这变成一种通用的插件机制。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Pods]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-pods%2F</url>
      <content type="text"><![CDATA[在 Kubernetes 中，最小的管理元素不是一个个独立的容器，而是 Pod 是最小的，管理，创建，计划的最小单元。 什么是 Pod一个 Pod（就像一群鲸鱼，或者一个豌豆夹）相当于一个共享 context 的配置组，在同一个 context 下，应用可能还会有独立的 cgroup 隔离机制，一个 Pod 是一个容器环境下的 “逻辑主机”，它可能包含一个或者多个紧密相连的应用，这些应用可能是在同一个物理主机或虚拟机上。 Pod 的 context 可以理解成多个 linux 命名空间的联合 PID 命名空间（同一个 Pod 中应用可以看到其它进程） 网络 命名空间（同一个 Pod 的中的应用对相同的 IP 地址和端口有权限） IPC 命名空间（同一个 Pod 中的应用可以通过 VPC 或者 POSIX 进行通信） UTS 命名空间（同一个 Pod 中的应用共享一个主机名称） 同一个 Pod 中的应用可以共享磁盘，磁盘是 Pod 级的，应用可以通过文件系统调用，额外的，一个 Pod 可能会定义顶级的 cgroup 隔离，这样的话绑定到任何一个应用（好吧，这句是在没怎么看懂，就是说 Pod，应用，隔离） 由于 docker 的架构，一个 Pod 是由多个相关的并且共享磁盘的容器组成，Pid 的命名空间共享还没有应用到 Docker 中 与相互独立的容器一样，Pod 是一种相对短暂的存在，而不是持久存在的，正如我们在 Pod 的生命周期中提到的，Pod 被安排到结点上，并且保持在这个节点上直到被终止（根据重启的设定）或者被删除，当一个节点死掉之后，上面的所有 Pod 均会被删除。特殊的 Pod 永远不会被转移到的其他的节点，作为替代，他们必须被 replace. Pod 的发展资源的共享及通信Pod 使 Pod 内的数据共享及通信变得容易 Pod 的中的应用均使用相同的网络命名空间及端口，并且可以通过 localhost 发现并沟通其他应用，每个 Pod 都有一个扁平化的网络命名空间下 IP 地址，它是 Pod 可以和其他的物理机及其他的容器进行无障碍通信，（The hostname is set to the pod’s Name for the application containers within the pod）主机名被设置为 Pod 的名称（这个没翻译出来…） 除了定义了在 Pod 中运行的应用之外，Pod 还定义了一系列的共享的磁盘，磁盘让这些数据在容器重启的时候不回丢失并且可以将这些数据在 Pod 中的应用进行共享 管理Pod 通过提供一个高层次抽象而不是底层的接口简化了应用的部署及管理，Pod 作为最小的部署及管理单位，位置管理，拷贝复制，资源共享，依赖关系都是自动处理的。（fate sharing 估计就说什么时候该死了，什么时候该新增一个了…） Pod 的使用Pod 可以作为垂直应用整合的载体，但是它的主要特点是支持同地协作，同地管理程序，例如： 内容管理系统，文件和数据加载，本地缓存等等 日志和检查点备份，压缩，循环，快照等等 数据交换监控，日志追踪，日志记录和监控适配器，以及事件发布等等 代理，网桥，适配器 控制，管理，配置，更新 总体来说，独立的 Pod 不会去加载多个相同的应用实例 考虑过的其他方案为什么不直接在一个容器上运行所有的应用？ 透明，Pod 中的容器对基础设施可见使的基础设施可以给容器提供服务，例如线程管理和资源监控，这为用户提供很多便利 解耦软件依赖关系, 独立的容器可以独立的进行重建和重新发布，Kubernetes 甚至会在将来支持独立容器的实时更新 易用，用户不需要运行自己的线程管理器，也不需要关心程序的信号以及异常结束码等 高效，因为基础设施承载了更多的责任，所以容器可以更加高效 为什么不支持容器的协同调度？ 容器的协同调度可以提供，但是它不具备 Pod 的大多数优点，比如资源共享，IPC，选择机制，简单管理等 Pod 的持久性 Pod 并不是被设计成一个持久化的资源，它不会在调度失败，节点崩溃，或者其他回收中（比如因为资源的缺乏，或者其他的维护中）幸存下来 总体来说，用户并应该直接的去创建 Pod，用户因该一直使用 controller(replication controller), 即使是一个节点的情况，这是因为 controller 提供了集群范围内的自我修复，以及复制还有展示管理 集群 API 的使用是用户的主要使用方式，这是相对普遍的在如下云管理平台中（ Borg, Marathon, Aurora, and Tupperware.） Pod 的直接暴露是如下操作变得更容器 调度和管理的易用性 在没有代理的情况下通过 API 可以对 Pod 进行操作 Pod 的生命周期与管理器的生命周期的分离 解偶控制器和服务，后段管理器仅仅监控 Pod 划分清楚了 Kubelet 级别的功能与云平台级别的功能，kubelet 实际上是一个 Pod 管理器 高可用，当发生一些删除或者维护的过程时，Pod 会自动的在他们被终止之前创建新的替代 目前对于宠物的最佳实践是，创建一个副本等于 1 和有对应 service 的一个 replication 控制器。如果你觉得这太麻烦，请在这里留下你的意见。 容器的终止因为 pod 代表着一个集群中节点上运行的进程，让这些进程不再被需要，优雅的退出是很重要的（与粗暴的用一个 KILL 信号去结束，让应用没有机会进行清理操作）。用户应该能请求删除，并且在室进程终止的情况下能知道，而且也能保证删除最终完成。当一个用户请求删除 pod，系统记录想要的优雅退出时间段，在这之前 Pod 不允许被强制的杀死，TERM 信号会发送给容器主要的进程。一旦优雅退出的期限过了，KILL 信号会送到这些进程，pod 会从 API 服务器其中被删除。如果在等待进程结束的时候，Kubelet 或者容器管理器重启了，结束的过程会带着完整的优雅退出时间段进行重试。 一个示例流程： 用户发送一个命令来删除 Pod，默认的优雅退出时间是 30 秒 API 服务器中的 Pod 更新时间，超过该时间 Pod 被认为死亡 在客户端命令的的里面，Pod 显示为”Terminating（退出中）” 的状态 （与第 3 同时）当 Kubelet 看到 Pod 标记为退出中的时候，因为第 2 步中时间已经设置了，它开始 pod 关闭的流程 如果该 Pod 定义了一个停止前的钩子，其会在 pod 内部被调用。如果钩子在优雅退出时间段超时仍然在运行，第二步会意一个很小的优雅时间断被调用 进程被发送 TERM 的信号 （与第三步同时进行）Pod 从 service 的列表中被删除，不在被认为是运行着的 pod 的一部分。缓慢关闭的 pod 可以继续对外服务，当负载均衡器将他们轮流移除。 当优雅退出时间超时了，任何 pod 中正在运行的进程会被发送 SIGKILL 信号被杀死。 Kubelet 会完成 pod 的删除，将优雅退出的时间设置为 0（表示立即删除）。pod 从 API 中删除，不在对客户端可见。 默认情况下，所有的删除操作的优雅退出时间都在30秒以内。kubectl delete命令支持–graceperiod=的选项，以运行用户来修改默认值。0表示删除立即执行，并且立即从API中删除pod这样一个新的pod会在同时被创建。在节点上，被设置了立即结束的的pod，仍然会给一个很短的优雅退出时间段，才会开始被强制杀死。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Replication Controller]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-replication_controller%2F</url>
      <content type="text"><![CDATA[已被 RS 取代 RC 伸缩 - Resizing a Replication Controller语法：123$ kubectl scale rc NAME --replicas=COUNT \ [--current-replicas=COUNT] \ [--resource-version=VERSION] NAME: 待更新的 RC 名称，必选。 replicas: 期望伸缩的副本数，必选。 current-replicas: 当前 RC 副本数，可选。 resource-version: 匹配 RC labels[].version 的值，可选。 滚动更新：滚动跟新仅支持 RC，不支持 Deployment 通过配置文件滚动更新先决条件： RC 必须制定 metadata.name 的在值。 Overwrite at least one common label in its spec.selector field. 使用同一的命名空间 metadata.namespace 12345 Update pods of frontend-v1 using new replication controller data in frontend-v2.json.$ kubectl rolling-update frontend-v1 -f frontend-v2.json// Update pods of frontend-v1 using JSON data passed into stdin.$ cat frontend-v2.json | kubectl rolling-update frontend-v1 -f - 滚动更新容器镜像1$ kubectl rolling-update NAME [NEW_NAME] --image=IMAGE:TAG NEW_NAME: 为 RC 重命名，可选。 回滚：1$ kubectl rolling-update NAME --rollback 模版yaml: 1234567891011121314151617181920apiVersion: v1kind: ReplicationControllermetadata: name: labels: # labels1: values1 # labels2: values2 namespace:spec: replicas: int selector: # labels1: values1 # labels2: values2 template: metadata: labels: # labels1: values1 # labels2: values2 spec: # please, See pod spec schema. json： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; "kind": "ReplicationController", "apiVersion": "v1", "metadata": &#123; "name": "frontend-controller", "labels": &#123; "state": "serving" &#125; &#125;, "spec": &#123; "replicas": 2, "selector": &#123; "app": "frontend" &#125;, "template": &#123; "metadata": &#123; "labels": &#123; "app": "frontend" &#125; &#125;, "spec": &#123; "volumes": null, "containers": [ &#123; "name": "php-redis", "image": "redis", "ports": [ &#123; "containerPort": 80, "protocol": "TCP" &#125; ], "imagePullPolicy": "IfNotPresent" &#125; ], "restartPolicy": "Always", "dnsPolicy": "ClusterFirst" &#125; &#125; &#125;&#125; 案例： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; "kind": "ReplicationController", "apiVersion": "v1", "metadata": &#123; "name": "frontend-controller", "labels": &#123; "state": "serving" &#125; &#125;, "spec": &#123; "replicas": 2, "selector": &#123; "app": "frontend" &#125;, "template": &#123; "metadata": &#123; "labels": &#123; "app": "frontend" &#125; &#125;, "spec": &#123; "volumes": null, "containers": [ &#123; "name": "php-redis", "image": "redis", "ports": [ &#123; "containerPort": 80, "protocol": "TCP" &#125; ], "imagePullPolicy": "IfNotPresent" &#125; ], "restartPolicy": "Always", "dnsPolicy": "ClusterFirst" &#125; &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在 Linux 上安装 kubernetes]]></title>
      <url>%2F2016%2F10%2F26%2Fkubernetes-install-kubernetes%2F</url>
      <content type="text"><![CDATA[先决条件 操作系统： Ubuntu 16.04, CentOS 7 or HypriotOS v1.0.1+ 至少 1GB RAM 确保集群内所有计算机之间的网络连接（公共或专用网络都行） 目标 在你的机器上安装一个安全的 Kubernetes 集群 在集群上安装一个 pod 网络，一遍应用组件（pods）之间可以正常通信。 安装在主机上安装 kubelet 和 kubeadm以下为将要在你的主机上安装的包： docker kubelet kubectl kubeadm 依次为每个主机进行安装配置： 1. 切换为 root 用户 su root 2. 如果你的机器是运行的 ubuntu 16.04 或 HypriotOS v1.0.1，执行如下命令： 12345678# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -# cat &lt;&lt;EOF&gt; /etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOF# apt-get update# # Install docker if you don't have it already.# apt-get install -y docker.io# apt-get install -y kubelet kubeadm kubectl kubernetes-cni 3. CentOS 7，执行如下命令： 1234567891011121314# cat &lt;&lt;EOF&gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF# setenforce 0# yum install -y docker kubelet kubeadm kubectl kubernetes-cni# systemctl enable docker &amp;&amp; systemctl start docker# systemctl enable kubelet &amp;&amp; systemctl start kubelet 初始化 master1# kubeadm init 输出结果大体这样： 123456789101112131415161718&lt;master/tokens&gt; generated token: "f0c861.753c505740ecde4c"&lt;master/pki&gt; created keys and certificates in "/etc/kubernetes/pki"&lt;util/kubeconfig&gt; created "/etc/kubernetes/kubelet.conf"&lt;util/kubeconfig&gt; created "/etc/kubernetes/admin.conf"&lt;master/apiclient&gt; created API client configuration&lt;master/apiclient&gt; created API client, waiting for the control plane to become ready&lt;master/apiclient&gt; all control plane components are healthy after 61.346626 seconds&lt;master/apiclient&gt; waiting for at least one node to register and become ready&lt;master/apiclient&gt; first node is ready after 4.506807 seconds&lt;master/discovery&gt; created essential addon: kube-discovery&lt;master/addons&gt; created essential addon: kube-proxy&lt;master/addons&gt; created essential addon: kube-dnsKubernetes master initialised successfully!You can connect any number of nodes by running:kubeadm join --token &lt;token&gt; &lt;master-ip&gt; 记录下 kubeadm init 输出的 kubeadm join 命令行。 安装节点网络插件你必须在安装一个 pod 网络插件，以确保 pods 之间能够相互通信。 kubernetes 支持的 pod 网络插件类型 Calico Canal Flannel Romana Weave 通过如下命令安装 pod 网络插件： 1# kubectl apply -f &lt;add-on.yaml&gt; 以 Calico 网络插件为例，在 Calico 官网 上下载 calico.yaml 文件到本地，然后执行如下命令： 1# kubectl apply -f calico.yaml 具体细节请参阅特定插件安装指南。一个集群中只能安装一个 pod 网络。 添加节点节点作为工作负载运行容器和 pods 等。如果你要将一个新的机器作为节点加入集群中，须将每个机器切换为 root 用户，并执行之前 kubeadm init 的输出命令，例如： 123456789101112131415# kubeadm join --token &lt;token&gt; &lt;master-ip&gt;&lt;util/tokens&gt; validating provided token&lt;node/discovery&gt; created cluster info discovery client, requesting info from "http://138.68.156.129:9898/cluster-info/v1/?token-id=0f8588"&lt;node/discovery&gt; cluster info object received, verifying signature using given token&lt;node/discovery&gt; cluster info signature and contents are valid, will use API endpoints [https://138.68.156.129:443]&lt;node/csr&gt; created API client to obtain unique certificate for this node, generating keys and certificate signing request&lt;node/csr&gt; received signed certificate from the API server, generating kubelet configuration&lt;util/kubeconfig&gt; created "/etc/kubernetes/kubelet.conf"Node join complete:* Certificate signing request sent to master and response received.* Kubelet informed of new secure connection details.Run 'kubectl get nodes' on the master to see this machine join. 在 master 上运行 kubectl get nodes 命名即可插件节点集群信息。 可选配置非 master 节点控制集群12# scp root@&lt;master ip&gt;:/etc/kubernetes/admin.conf .# kubectl --kubeconfig ./admin.conf get nodes 撤销 kubeadm撤销 kubeadm，只需执行如下命令： 1# kubeadm reset 如果你想重新启动集群，执行 systemctl start kubelet ，再执行 kubeadm init 或 kubeadm join 。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker 集中化 Web 界面管理平台 shipyard]]></title>
      <url>%2F2016%2F10%2F25%2Fshipyard-index%2F</url>
      <content type="text"><![CDATA[简介shipyard 是一个集成管理 Docker 容器、镜像、仓库的系统，他最大亮点应该是支持多节点的集成管理，可以动态加载节点，可托管 node 下的容器。 首次部署脚本1$ curl -sSL https://shipyard-project.com/deploy | bash -s ACTION: 可以使用的指令 (deploy, upgrade, node, remove) DISCOVERY: 集群系统采用 Swarm 进行采集和管理 (在节点管理中可以使用‘node’) IMAGE: 镜像，默认使用 shipyard 的镜像 PREFIX: 容器名字的前缀 SHIPYARD_ARGS: 容器的常用参数 TLS_CERT_PATH: TLS 证书路径 PORT: 主程序监听端口 (默认端口: 8080) PROXY_PORT: 代理端口 (默认: 2375) 脚本可选项如果你要自定义部署，请参考以下规范 部署 action：指令有效变量 deploy: 部署新的 shipyard 实例 upgrade: 更新已存在的实例（注意：你要保持相同的系统环境、变量来部署同样的配置） node: 使用 Swarm 增加一个新的 node remove: 删除已存在的 shipyard 实例（容器） 镜像使用你可以采取规范的镜像来部署实例，比如以下的测试版本，你也已这样做 1$ curl -sSL https://shipyard-project.com/deploy | IMAGE=shipyard/shipyard:test bash -s 前缀使用你可以定义你想要的前缀，比如 1$ curl -sSL https://shipyard-project.com/deploy | PREFIX=shipyard-test bash -s 参数使用这里增加一些 shipyard 运行参数，你可以像这样进行调整： 1$ curl -sSL https://shipyard-project.com/deploy | SHIPYARD_ARGS="--ldap-server=ldap.example.com --ldap-autocreate-users" bash -s TLS 证书使用1. 启用 TLS 对组建进行部署，包括代理（proxy）、swarm 集群系统、shipyard 管理平台的配置，这是一个配置规范。证书必须采用以下命名规范： - ca.pem: 安全认证证书 - server.pem: 服务器证书 - server-key.pem: 服务器私有证书 - cert.pem: 客户端证书 - key.pem: 客户端证书的 key 2. 注意：证书将被放置在一个 docker 容器中，并在各个组成部分之间共享。如果需要调试，可以将此容器连接到调试容器。数据容器名称为前缀的证书。 123456789$ docker run --rm \ -v $(pwd)/certs:/certs \ ehazlett/certm \ -d /certs \ bundle \ generate \ -o shipyard \ --host proxy \ --host 127.0.0.1 3. 你也可以按如下指令来部署 shipyard 1$ curl -sSL https://shipyard-project.com/deploy | TLS_CERT_PATH=$(pwd)/certs bash -s 增加一个部署节点shipyard 节点部署脚本将自动的安装 key/value 存储系统（etcd 系统）。增加一个节点到 swarm 集群，你可以使用以下的节点部署脚本 1$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://10.0.1.10:4001 bash -s 10.0.1.10 这个 ip 地址你需要修改为你的首次初始化 shipyard 系统的主机地址 删除 shipyard 系统1$ curl -sSL https://shipyard-project.com/deploy | ACTION=remove bash -s 实例1. 在 manager0 节点上执行部署脚本 1$ curl -sSL https://shipyard-project.com/deploy | bash -s 123456789$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5636aef7bec1 shipyard/shipyard:latest "/bin/controller --de" 4 minutes ago Up 4 minutes 0.0.0.0:8080-&gt;8080/tcp shipyard-controller8a0f609b05f1 swarm:latest "/swarm j --addr 192." 4 minutes ago Up 4 minutes 2375/tcp shipyard-swarm-agent916aaa6a59ba swarm:latest "/swarm m --replicati" 4 minutes ago Up 4 minutes 2375/tcp shipyard-swarm-manager20619c435f9f shipyard/docker-proxy:latest "/usr/local/bin/run" 4 minutes ago Up 4 minutes 0.0.0.0:2375-&gt;2375/tcp shipyard-proxy98e7f61e436f alpine "sh" 4 minutes ago Up 4 minutes shipyard-certsb09af2fc0b92 microbox/etcd:latest "/bin/etcd -addr 192." 5 minutes ago Up 4 minutes 0.0.0.0:4001-&gt;4001/tcp, 0.0.0.0:7001-&gt;7001/tcp shipyard-discovery3d294a262ea0 rethinkdb "rethinkdb --bind all" 5 minutes ago Up 4 minutes 8080/tcp, 28015/tcp, 29015/tcp shipyard-rethinkdb 2. 部署节点 node3 1$ curl -sSL https://shipyard-project.com/deploy | ACTION=node PREFIX=shipyard-node3 DISCOVERY=etcd://192.168.93.145:4001 bash –s 123456$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES195acdbe1818 swarm:latest "/swarm j --addr 192." 2 minutes ago Up 2 minutes 2375/tcp shipyard-node3-swarm-agent3e43f1c85af0 swarm:latest "/swarm m --replicati" 2 minutes ago Up 2 minutes 2375/tcp shipyard-node3-swarm-manager5a38d292f81e shipyard/docker-proxy:latest "/usr/local/bin/run" 3 minutes ago Up 3 minutes 0.0.0.0:2375-&gt;2375/tcp shipyard-node3-proxyf8d80de583e6 alpine "sh" 3 minutes ago Up 3 minutes shipyard-node3-certs 3. 部署节点 node4 1$ curl -sSL https://shipyard-project.com/deploy | ACTION=node PREFIX=shipyard-node4 DISCOVERY=etcd://192.168.93.145:4001 bash –s 123456$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb9a4f9f6d4c5 swarm:latest "/swarm j --addr 192." 3 minutes ago Up 3 minutes 2375/tcp shipyard-node4-swarm-agent765f3898c925 swarm:latest "/swarm m --replicati" 3 minutes ago Up 3 minutes 2375/tcp shipyard-node4-swarm-manager16e000eae193 shipyard/docker-proxy:latest "/usr/local/bin/run" 3 minutes ago Up 3 minutes 0.0.0.0:2375-&gt;2375/tcp shipyard-node4-proxyae224eafce6a alpine "sh" 3 minutes ago Up 3 minutes shipyard-node4-certs 4. 启动界面（ip:8080） 5. 容器详细情况 6. 镜像 7. 节点]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[高级网络配置]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-network-containers%2F</url>
      <content type="text"><![CDATA[使用 Docker 网络功能可以帮助我们在建设 web 应用时保证一致和安全性。网络的定义就是为容器之间提供完全的独立性。因此对应用运行的网络控制是非常重要的。Docker 容器网络功能正好给我们提供了这样的控制。下面主要会介绍 Docker Engine 原生发布的默认网络的特性，描述默认的网络类型以及如何自定义我们自己的网络，并且提供了关于如何在单机或者集群创建网络所需要的资源。 bridge 默认网络我们安装 Docker 后，它默认创建了 三种网络，我们使用 docker network ls 命令来看下具体列表: 123456$ docker network lsNETWORK ID NAME DRIVER7fca4eb8c647 bridge bridge9f904ee27bf5 none nullcf03ee007fb4 host host 从历史上看，这三个网络是 Docker 实现的一部分。当我们运行容器时我们可以使用 --net 标志指定要运行的网络时，这三种类型网络依然可用。名字为 bridge 的网络 代表我们安装 Docker 主机上的名字为 docker0 的网络。默认情况下容器会使用此网络，除非我们执行 docker run --net=&lt;NETWORK&gt; 指定网络自定义的网络。我们可以在宿主主机上执行 ifconfig docker0 查看 docker0 网络配置信息。 123456789$ ifconfigdocker0 Link encap:Ethernet HWaddr 02:42:47:bc:3a:eb inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:47ff:febc:3aeb/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:9001 Metric:1 RX packets:17 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 none 的网络 将容器添加到一个 容器特定网络栈 中。这样的容器缺少一个网络接口，附着到这样的容器上可以看到其网络栈内容如下： 123456789101112131415161718$ docker attach nonenetcontainerroot@0cb243cd1293:/# cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allroutersroot@0cb243cd1293:/# ifconfiglo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 我们可以使用 ctrl+p + ctrl+q 快捷键来脱离正在运行中的容器而不影响容器运行。 host 的网络将容器添加到了宿主的网络栈上，这样容器的网络配置信息与宿主主机的网络配置是一样的。 bridge 网络是我们默认使用的网络，我们可以使用 docker network inspect bridge 查看它的详细信息。我们也可以自定义新的 bridge 驱动的网络，并且可以在我们不需要使用的时候删除掉自定义的网络，然而 bridge 名称的网络我们无法删除，它是 Docker 引擎安装时需要使用的。 值得一提的是在 bridge 驱动的网络正常仅用于单机内部使用，对于宿主以外的主机如果想要访问容器就需要通过 docker run -p 或者 -P 参数将需要访问的服务端口开放映射到宿主主机的网络端口，并且配置放开本地防火墙的拦截规则。这样外部主机就可以访问容器的服务了。具体如下图: bridge 网络比较适用于在单机运行相对较小的网络。对于比较大型的网络我们可以使用 overlay 网络来实现。 overlay 网络Docker 的 overlay 网络驱动器支持多宿主主机互联。这种支撑能力得益于 libnetwork 库，libnetwork 是一个基于 VXLAN 的 overlay 网络驱动器。 overlay 网络需要一个有效的 key-value 存储服务，目前 Docker 支持的有 Consul,、Etcd 和 ZooKeeper (分布式存储)。在创建 overlay 网络之前我们必须先安装并配置好我们指定的 key-value 存储服务，并且需要保证要接入网络的宿主主机和服务是可以通信的。示例如下图: 每个接入 overlay 网络的宿主主机必须运行了 Docker 引擎实例，比较简便的做法是使用 Docker Machine 来完成。 我们同时还需要开放如下几个端口服务： Protocol Port Description udp 4789 Data plane (VXLAN) tcp/udp 7946 Control plane 对于 key-value 服务需要开放的端口需要查看所选择服务的厂商文档来确定了。 对于多台主机的管理，我们可以使用 Docker Swarm 来快速的把他们加入到一个集群中，这个集群中需要包含一个发现服务。 在创建 overlay 网络前，我们需要配置 Docker 的 daemon 的几个参数选项，具体内容如下: Option Description –cluster-store=PROVIDER://URL Describes the location of the KV service. –cluster-advertise=HOST_IP or HOST_IFACE:PORT The IP address or interface of the HOST used for clustering. –cluster-store-opt=KEY-VALUE OPTIONS Options such as TLS certificate or tuning 执行如下命令在集群中的主机上创建一个 overlay 网络： 1$ docker network create --driver overlay my-multi-host-network 这样一个跨多个主机的网络就创建完成了， overlay 网络为这些容器提供了一个完全隔离的环境。 接下来，我们在每个宿主主机上就可以运行指定为使用 overlay 网络的容器了，执行命令为: 1$ docker run -itd --net=my-multi-host-network busybox 一旦网络建立连接后，网络中的容器都可以互相访问，无论这个容器是否启动了。下图为我们最终创建的 overlay 网络结构。 自定义网络插件我们可以使用Docker提供的插件基础设施来实现自己的网络驱动器，这样的一个插件就是一个运行在宿主主机上同 daemon 进程一样的进程。想了解关于如何编写网络驱动器参考下面两个站点： Docker 引擎扩展 Docker 引擎网络插件 Docker内嵌的DNS服务器Docker 的守护进程 daemon 运行了一个内嵌的 DNS 服务来为用户自定义网络上容器提供一个自动服务发现的功能。来自于容器的域名解析请求首先会被这个内嵌 DNS 服务器获取，如果它无法解析此域名就将这个请求转发给为配置给该容器的外部 DNS 服务器。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用网络]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-container-networking%2F</url>
      <content type="text"><![CDATA[大量的互联网应用服务包括多个服务组件，这往往需要多个容器之间通过网络进行相互配合。Docker 目前提供映射端口到宿主机和容器互联机制为容器提供网络服务。 端口映射实现访问容器外部访问容器容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射。 当使用 -P 标记时，Docker 会随机映射一个 49000~49900 的端口到内部容器开放的网络端口。 使用 docker ps 可以看到，本地主机的 49155 被映射到了容器的 5000 端口。此时访问本机的 49155 端口即可访问容器内 web 应用提供的界面。 1234$ sudo docker run -d -P training/webapp python app.py$ sudo docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbc533791f3f5 training/webapp:latest python app.py 5 seconds ago Up 2 seconds 0.0.0.0:49155-&gt;5000/tcp nostalgic_morse 同样的，可以通过 docker logs 命令来 查看应用的信息 。 1234$ sudo docker logs -f nostalgic_morse* Running on http://0.0.0.0:5000/10.0.2.2 - - [23/May/2014 20:16:31] "GET / HTTP/1.1" 200 -10.0.2.2 - - [23/May/2014 20:16:31] "GET /favicon.ico HTTP/1.1" 404 - -p（小写的）则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。支持的格式有 ip:hostPort:containerPort | ip:containerPort | hostPort:containerPort。 映射所有接口地址使用 hostPort:containerPort 格式本地的 5000 端口映射到容器的 5000 端口，可以执行。 1$ sudo docker run -d -p 5000:5000 training/webapp python app.py 此时默认会绑定本地所有接口上的所有地址。 映射到指定地址的指定端口可以使用 ip:hostPort:containerPort 格式指定映射使用一个特定地址，比如 localhost 地址 127.0.0.1。 1$ sudo docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py 映射到指定地址的任意端口使用 ip::containerPort 绑定 localhost 的任意端口到容器的 5000 端口，本地主机会自动分配一个端口。 1$ sudo docker run -d -p 127.0.0.1::5000 training/webapp python app.py 还可以使用 udp 标记来指定 udp 端口。 1$ sudo docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py 查看映射端口配置使用 docker port 来查看当前映射的端口配置，也可以查看到绑定的地址。12$ docker port nostalgic_morse 5000127.0.0.1:49155. 容器有自己的内部网络和 ip 地址（使用 docker inspect 可以获取所有的变量，Docker 还可以有一个可变的网络配置）。 -p 标记可以多次使用来绑定多个端口，例如。 1$ sudo docker run -d -p 5000:5000 -p 3000:80 training/webapp python app.py 容器互联容器的连接（linking）系统是除了端口映射外，另一种跟容器中应用交互的方式。该系统会在源和接收容器之间创建一个隧道，接收容器可以看到源容器指定的信息。 自定义容器命名连接系统依据容器的名称来执行。因此，首先需要自定义一个好记的容器命名。虽然当创建容器的时候，系统默认会分配一个名字。自定义命名容器有 2 个好处： 自定义的命名，比较好记，比如一个 web 应用容器我们可以给它起名叫 web 当要连接其他容器时候，可以作为一个有用的参考点，比如连接 web 容器到 db 容器 使用 --name 标记可以为 容器自定义命名。 1$ sudo docker run -d -P --name web training/webapp python app.py 使用 docker ps 来验证设定的命名。 123$ sudo docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaed84ee21bde training/webapp:latest python app.py 12 hours ago Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp web 也可以使用 docker inspect 来查看容器的名字 12$ sudo docker inspect -f "&#123;&#123;\.Name&#125;&#125;" aed84ee21bde/web 容器的名称是唯一的。如果已经命名了一个叫 web 的容器，当你要再次使用 web 这个名称的时候，需要先用 docker rm 来删除之前创建的同名容器。 在执行 docker run 的时候如果添加 --rm 标记，则容器在终止后会立刻删除。注意，--rm 和 -d 参数不能同时使用。 容器互联使用 --link 参数可以让容器之间安全的进行交互。下面先创建一个新的数据库容器。 1$ sudo docker run -d --name db training/postgres 删除之前创建的 web 容器 1$ docker rm -f web 然后创建一个新的 web 容器，并将它连接到 db 容器 1$ sudo docker run -d -P --name web --link db:db training/webapp python app.py 此时，db 容器和 web 容器建立互联关系。 --link 参数的格式为 --link name:alias，其中 name 是要链接的容器的名称，alias 是这个连接的别名。 使用 docker ps 来查看容器的连接 1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES349169744e49 training/postgres:latest su postgres -c /usr About a minute ago Up About a minute 5432/tcp db, web/dbaed84ee21bde training/webapp:latest python app.py 16 hours ago Up 2 minutes 0.0.0.0:49154-&gt;5000/tcp web 可以看到自定义命名的容器，db 和 web，db 容器的 names 列有 db 也有 web/db。这表示 web 容器链接到 db 容器，web 容器将被允许访问 db 容器的信息。 Docker 在两个互联的容器之间创建了一个安全隧道，而且不用映射它们的端口到宿主主机上。在启动 db 容器的时候并没有使用 -p 和 -P 标记，从而避免了暴露数据库端口到外部网络上。 Docker 通过 2 种方式为容器公开连接信息： 环境变量 更新 /etc/hosts 文件 使用 env 命令来查看 web 容器的环境变量 123456789$ sudo docker run --rm --name web2 --link db:db training/webapp env. . .DB_NAME=/web2/dbDB_PORT=tcp://172.17.0.5:5432DB_PORT_5000_TCP=tcp://172.17.0.5:5432DB_PORT_5000_TCP_PROTO=tcpDB_PORT_5000_TCP_PORT=5432DB_PORT_5000_TCP_ADDR=172.17.0.5. . . 其中 DB_ 开头的环境变量是供 web 容器连接 db 容器使用，前缀采用大写的连接别名。除了环境变量，Docker 还添加 host 信息到父容器的 /etc/hosts 的文件。下面是父容器 web 的 hosts 文件 12345$ sudo docker run -t -i --rm --link db:db training/webapp /bin/bashroot@aed84ee21bde:/opt/webapp# cat /etc/hosts172.17.0.7 aed84ee21bde. . .172.17.0.5 db 这里有 2 个 hosts，第一个是 web 容器，web 容器用 id 作为他的主机名，第二个是 db 容器的 ip 和主机名。 可以在 web 容器中安装 ping 命令来测试跟 *db 容器的连通。 123456root@aed84ee21bde:/opt/webapp# apt-get install -yqq inetutils-pingroot@aed84ee21bde:/opt/webapp# ping dbPING db (172.17.0.5): 48 data bytes56 bytes from 172.17.0.5: icmp_seq=0 ttl=64 time=0.267 ms56 bytes from 172.17.0.5: icmp_seq=1 ttl=64 time=0.250 ms56 bytes from 172.17.0.5: icmp_seq=2 ttl=64 time=0.256 ms 用 ping 来测试 db 容器，它会解析成 172.17.0.5。 官方的 ubuntu 镜像默认没有安装 ping，需要自行安装。 用户可以链接多个父容器到子容器，比如可以链接多个 web 到 db 容器上。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[存储驱动]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-storage-drivers%2F</url>
      <content type="text"><![CDATA[镜像，容器与存储驱动 images，container，storage driver 镜像与层（Images and Layers）以 ubuntu15.04 为例，由四个镜像层堆叠而成，而且每层都是只读的。存储驱动负责堆叠这些镜像层并提供统一视口（The Docker storage driver is responsible for stacking these layers and providing a single unified view.） 如下图所示，当创建一个容器时，会在原有的 layer 上添加一个新的，稀疏的，可读写的 “容器层”，在容器上进行的所有读，写，删除文件的指令都会在容器层完成 容器与层（container and layer）容器和镜像最大的不同就在于顶层可读写的容器层。所有在容器层进行的读写操作，在容器被删除后，数据将会删除，而底层的镜像层保持不变 如下图所示，用同一个 ubuntu15.04 镜像，启动多个 ubuntu15.04 容器，每个容器都有负责各自读写操作的 “容器层”，而底层的镜像层是容器间共享的。 Docker 模型的核心部分是有效利用分层镜像机制，镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。Docker 1.10 引入新的可寻址存储模型，使用安全内容哈希代替随机的 UUID 管理镜像。同时，Docker 提供了迁移工具，将已经存在的镜像迁移到新模型上。不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的可读写层，大大提高了存储的效率。其中主要的机制就是分层模型和将不同目录挂载到同一个虚拟文件系统。 存储驱动器之所以能负责管理容器层和镜像层，主要依靠镜像分层机制和写时拷贝拷贝技术（stackable image layers and copy-on-write (CoW)） 数据卷与存储驱动（Data volumes and the storage driver）当容器被删除时，除了数据卷的数据，其他容器层数据都将被删除。 数据卷： Docker 宿主机挂载目录或文件到容器指定位置。 数据卷不由存储驱动器管理，对数据卷的读写等操作绕过了存储驱动器。 多个容器可以共享一个或多个数据卷。 由上图可以看出，数据卷是属于 Docker 宿主机的本地存储空间内的，进一步加强了数据卷的独立，摆脱存储驱动器控制。当容器被删除时，数据卷中所有数据都将继续保留在 Docker 宿主机中。 存储选型可插入的存储驱动架构Docker 提供一个可插入的的存储器架构，使之能根据用户需求灵活的选择存储驱动器。Docker 存储器是基于 Linux 文件系统或卷的。此外，每种存储驱动器可以自由地以其独特的方式镜像像层和容器层的管理。 Technology Storage driver name OverlayFS overlay or overlay2 AUFS aufs Btrfs btrfs Device Mapper devicemapper VFS vfs ZFS zfs 查看存储驱动 12345678910111213# docker infoContainers: 1 Running: 1 Paused: 0 Stopped: 0Images: 29Server Version: 1.12.1Storage Driver: aufs Root Dir: /var/lib/docker/aufs Backing Filesystem: extfs Dirs: 112 Dirperm1 Supported: trueLogging Driver: json-file 上表中，docker daemon 使用的是 aufs 存储驱动，docker 宿主机使用的是 extfs。docker daemon 选择何种存储驱动器，一部分取决于其宿主机，因为有些后台文件系统支持 docker daemon 使用不同的类型的存储驱动器，而有些不是。如表所示： Storage driver Commonly used on Disabled on overlay ext4 xfs btrfs aufs overlay overlay2 zfs eCryptfs overlay2 ext4 xfs btrfs aufs overlay overlay2 zfs eCryptfs aufs ext4 xfs btrfs aufs eCryptfs btrfs btrfs only N/A devicemapper direct-lvm N/A vfs debugging only N/A zfs zfs only N/A 可以通过修改 docker 配置文件来指定存储驱动 –storage-driver=，也可以在创建容器时指定。 Docker 存储驱动选择 稳定性 团队经验和专业性 关于存储驱动选择需要明白一下两点： 没有一个存储驱动是万能的，不能够适用于所有的案例中。 存储驱动是不断改善和进化发展的。 AUFSAUFS（AnotherUnionFS）是一种 Union FS，是文件级的存储驱动。所谓 UnionFS 就是把不同物理位置的目录合并 mount 到同一个目录中。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS 创建该文件的一个副本，使用 CoW 将文件从只读层复制到可写层进行修改，结果也保存在可写层。在 Docker 中，底下的只读层就是 image，可写层就是 Container。结构如下图所示： layerID 和 docker 宿主机中的目录并不是一致的。 尽管 AUFS 是最早支持 Docker 的，但是一些 linux 发行版本并不支持 AUFS。 AUFS 特性： 容器启动快 高效存储 有效利用内存 例子运行一个实例应用并删除一个文件 / etc/shadow，看 AUFS 的结果： 123456$ docker run centos rm /etc/shadow $ ls -la /var/lib/docker/aufs/diff/$(docker ps --no-trunc -lq)/etctotal 8drwxr-xr-x 2 root root 4096 Sep 2 18:35 .drwxr-xr-x 5 root root 4096 Sep 2 18:35 ..-r--r--r-- 2 root root 0 Sep 2 18:35 .wh.shadow 目录结构 分支，各层（包括只读镜像层与可写容器层）挂载点，与镜像或容器 ID 不匹配：/var/lib/docker/aufs/diff/ 镜像索引表，每个镜像引用镜像名，存放镜像元数据：/var/lib/docker/aufs/layers/ 容器挂载点，运行时容器挂载点：/var/lib/docker/aufs/mnt/ 容器元数据与配置文件挂载点：/var/lib/docker/containers/ 其他 AUFS 文件系统可使用的磁盘空间大小 123$ df -h /var/lib/docker/Filesystem Size Used Avail Use% Mounted on/dev/vda1 20G 4.0G 15G 22% / 系统挂载方式 启动的 Docker 123456789101112$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3f2e9de1d9d5 mesos/bamboo:v0.1c "/usr/bin/bamboo-hap 5 days ago Up 5 days mesos-20150825-162813-1248613158-5050-1-S0.88c909bc-6301-423a-8283-5456435f12d3dc9a7b000300 mesos/nginx:base "/bin/sh -c nginx" 7 days ago Up 7 days 0.0.0.0:31967-&gt;80/tcp mesos-20150825-162813-1248613158-5050-1-S0.42667cb2-1134-4b1a-b11d-3c565d4de4181b466b5ad049 mesos/marathon:omega.v0.1 "/usr/bin/dataman_ma 7 days ago Up 16 hours dataman-marathon0a01eb99c9e7 mesos/nginx:base "/bin/sh -c nginx" 7 days ago Up 7 days 0.0.0.0:31587-&gt;80/tcp mesos-20150825-162813-1248613158-5050-1-S0.4f525828-1217-4b3d-a169-bc0eb901eef1c2fb2e8bd482 mesos/dns:v0.1c "/usr/bin/dataman_me 7 days ago Up 7 days mesos-20150825-162813-1248613158-5050-1-S0.82d500eb-c3f0-4a00-9f7b-767260d1ee9adf102527214d mesos/zookeeper:omega.v0.1 "/data/run/dataman_z 8 days ago Up 8 days dataman-zookeeperb076a43693c1 mesos/slave:omega.v0.1 "/usr/sbin/mesos-sla 8 days ago Up 8 days dataman-slavee32e9fc9a788 mesos/master:omega.v0.1 "/usr/sbin/mesos-mas 8 days ago Up 8 days dataman-masterc8454c90664e shadowsocks_server "/usr/local/bin/ssse 9 days ago Up 9 days 0.0.0.0:57980-&gt;57980/tcp shadowsocks6dcd5bd46348 registry:v0.1 "docker-registry" 9 days ago Up 9 days 0.0.0.0:5000-&gt;5000/tcp dataman-registry 对照系统挂载点 123456789101112$ grep aufs /proc/mounts/dev/mapper/ubuntu--vg-root /var/lib/docker/aufs ext4 rw,relatime,errors=remount-ro,data=ordered 0 0none /var/lib/docker/aufs/mnt/6dcd5bd463482edf33dc1b0324cf2ba4511c038350e745b195065522edbffb48 aufs rw,relatime,si=d9c018051ec07f56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/c8454c90664e9a2a2abbccbe31a588a1f4a5835b5741a8913df68a9e27783170 aufs rw,relatime,si=d9c018051ba00f56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/e32e9fc9a788e73fc7efc0111d7e02e538830234377d09b54ffc67363b408fca aufs rw,relatime,si=d9c018051b336f56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/b076a43693c1d5899cda7ef8244f3d7bc1d102179bc6f5cd295f2d70307e2c24 aufs rw,relatime,si=d9c018051bfecf56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/df102527214d5886505889b74c07fda5d10b10a4b46c6dab3669dcbf095b4154 aufs rw,relatime,si=d9c01807933e1f56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/c2fb2e8bd4822234633d6fd813bf9b24f9658d8d97319b1180cb119ca5ba654c aufs rw,relatime,si=d9c01806c735ff56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/0a01eb99c9e702ebf82f30ad351d5a5a283326388cd41978cab3f5c5b7528d94 aufs rw,relatime,si=d9c018051bfebf56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/1b466b5ad049d6a1747d837482264e66a87871658c1738dfd8cac80b7ddcf146 aufs rw,relatime,si=d9c018052b2b1f56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/dc9a7b000300a36c170e4e6ce77b5aac1069b2c38f424142045a5ae418164241 aufs rw,relatime,si=d9c01806d9ddff56,dio,dirperm1 0 0none /var/lib/docker/aufs/mnt/3f2e9de1d9d51919e1b6505fd7d3f11452c5f00f17816b61e6f6e97c6648b1ab aufs rw,relatime,si=d9c01806c708ff56,dio,dirperm1 0 0 分析 虽然 AUFS 是 Docker 第一版支持的存储方式，但到它现在还没有加入 Linux 内核主线 ( CentOS 无法直接使用）。 从原理分析看，AUFS mount() 方法很快，所以创建容器很快；读写访问都具有本机效率；顺序读写和随机读写的性能大于 KVM；并且 Docker 的 AUFS 可以有效的使用存储和内存 。 AUFS 性能稳定，并且有大量生产部署及丰富的社区支持。 不支持 rename 系统调用，执行 “copy” 和 “unlink” 时，会导致失败。 当写入大文件的时候（比如日志或者数据库等）动态 mount 多目录路径的问题，导致 branch 越多，查找文件的性能也就越慢。（解决办法：重要数据直接使用 -v 参数挂载。） Device mapperDevice mapper 是 Linux 内核 2.6.9 后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。Docker 的 Device mapper 利用 Thin provisioning snapshot 管理镜像和容器。 Thin-provisioning SnapshotSnapshot 是 Lvm 提供的一种特性，它可以在不中断服务运行的情况下为 the origin（ original device ）创建一个虚拟快照（ Snapshot ）。Thin-Provisioning 是一项利用虚拟化方法减少物理存储部署的技术。Thin-provisioning Snapshot 是结合 Thin-Provisioning 和 Snapshotin g 两种技术，允许多个虚拟设备同时挂载到一个数据卷以达到数据共享的目的。Thin-Provisioning Snapshot 的特点如下： 可以将不同的 snaptshot 挂载到同一个 the origin 上，节省了磁盘空间。 当多个 Snapshot 挂载到了同一个 the origin 上，并在 the origin 上发生写操作时，将会触发 COW 操作。这样不会降低效率。 Thin-Provisioning Snapshot 支持递归操作，即一个 Snapshot 可以作为另一个 Snapshot 的 the origin，且没有深度限制。 在 Snapshot 上可以创建一个逻辑卷，这个逻辑卷在实际写操作（ COW，Snapshot 写操作）发生之前是不占用磁盘空间的。 相比 AUFS 和 OverlayFS 是文件级存储，Device mapper 是块级存储，所有的操作都是直接对块进行操作，而不是文件。 Device mapper 驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照 。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并没有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用 CoW 为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个 100G 的文件包含镜像和容器。每一个容器被限制在 10G 大小的卷内，可以自己配置调整。结构如下图所示： 可以通过”docker info” 或通过 dmsetup ls 获取想要的更多信息。查看 Docker 的 Device mapper 的信息： 分析 Device mapper 文件系统兼容性比较好，并且存储为一个文件，减少了 inode 消耗。 每次一个容器写数据都是一个新块，块必须从池中分配，真正写的时候是稀松文件, 虽然它的利用率很高，但性能不好，因为额外增加了 VFS 开销。 每个容器都有自己的块设备时，它们是真正的磁盘存储，所以当启动 N 个容器时，它都会从磁盘加载 N 次到内存中，消耗内存大。 Docker 的 Device mapper 默认模式是 loop-lvm ，性能达不到生产要求。在生产环境推荐 direct-lvm 模式直接写原块设备，性能好。 OverlayFSOverlay 是 Linux 内核 3.18 后支持的，也是一种 Union FS，和 AUFS 的多层不同的是 Overlay 只有两层：一个 Upper 文件系统和一个 Lower 文件系统，分别代表 Docker 的镜像层和容器层。当需要修改一个文件时，使用 CoW 将文件从只读的 Lower 复制到可写的 Upper 进行修改，结果也保存在 Upper 层。在 Docker 中，底下的只读层就是 image，可写层就是 Container。结构如下图所示： 分析 从 kernel 3.18 进入主流 Linux 内核。设计简单，速度快，比 AUFS 和 Device mapper 速度快。在某些情况下，也比 Btrfs 速度快。是 Docker 存储方式选择的未来。因为 OverlayFS 只有两层，不是多层，所以 OverlayFS “copy-up” 操作快于 AUFS。以此可以减少操作延时。 OverlayFS 支持页缓存共享，多个容器访问同一个文件能共享一个页缓存，以此提高内存使用。 OverlayFS 消耗 inode，随着镜像和容器增加，inode 会遇到瓶颈。Overlay2 能解决这个问题。在 Overlay 下，为了解决 inode 问题，可以考虑将 / var/lib/docker 挂在单独的文件系统上，或者增加系统 inode 设置。 有兼容性问题。open(2) 只完成部分 POSIX 标准，OverlayFS 的某些操作不符合 POSIX 标准。例如： 调用 fd1=open(“foo”, O_RDONLY) ，然后调用 fd2=open(“foo”, O_RDWR) 应用期望 fd1 和 fd2 是同一个文件。然后由于复制操作发生在第一个 open(2) 操作后，所以认为是两个不同的文件。 不支持 rename 系统调用，执行 “copy” 和”unlink”时，将导致失败。 BtrfsBtrfs 被称为下一代写时复制文件系统，并入 Linux 内核，也是文件级级存储，但可以像 Device mapper 一直接操作底层设备。Btrfs 利用 Subvolumes 和 Snapshots 管理镜像容器分层。Btrfs 把文件系统的一部分配置为一个完整的子文件系统，称之为 Subvolume，Snapshot 是 Subvolumn 的实时读写拷贝，chunk 是分配单位，通常是 1GB。那么采用 Subvolume，一个大的文件系统可以被划分为多个子文件系统，这些子文件系统共享底层的设备空间，在需要磁盘空间时便从底层设备中分配，类似应用程序调用 malloc() 分配内存一样。 为了灵活利用设备空间，Btrfs 将磁盘空间划分为多个 chunk 。每个 chunk 可以使用不同的磁盘空间分配策略。比如某些 chunk 只存放 metadata，某些 chunk 只存放数据。这种模型有很多优点，比如 Btrfs 支持动态添加设备。用户在系统中增加新的磁盘之后，可以使用 Btrfs 的命令将该设备添加到文件系统中。Btrfs 把一个大的文件系统当成一个资源池，配置成多个完整的子文件系统，还可以往资源池里加新的子文件系统，而基础镜像则是子文件系统的快照，每个子镜像和容器都有自己的快照，这些快照则都是 subvolume 的快照。 分析 Btrfs 是替换 Device mapper 的下一代文件系统， 很多功能还在开发阶段，还没有发布正式版本，相比 EXT4 或其它更成熟的文件系统，它在技术方面的优势包括丰富的特征，如：支持子卷、快照、文件系统内置压缩和内置 RAID 支持等。 不支持页缓存共享，N 个容器访问相同的文件需要缓存 N 次。不适合高密度容器场景。 当前 Btrfs 版本使用 “small writes” , 导致性能问题。并且需要使用 Btrfs 原生命令 btrfs filesys show 替代 df。 Btrfs 使用 “journaling” 写数据到磁盘，这将影响顺序写的性能。 Btrfs 文件系统会有碎片，导致性能问题。当前 Btrfs 版本，能通过 mount 时指定 autodefrag 做检测随机写和碎片整理。 ZFSZFS 文件系统是一个革命性的全新的文件系统，它从根本上改变了文件系统的管理方式，ZFS 完全抛弃了 “卷管理”，不再创建虚拟的卷，而是把所有设备集中到一个存储池中来进行管理，用”存储池” 的概念来管理物理存储空间。过去，文件系统都是构建在物理设备之上的。为了管理这些物理设备，并为数据提供冗余，”卷管理”的概念提供了一个单设备的映像。而 ZFS 创建在虚拟的，被称为 “zpools” 的存储池之上。每个存储池由若干虚拟设备（virtual devices，vdevs）组成。这些虚拟设备可以是原始磁盘，也可能是一个 RAID1 镜像设备，或是非标准 RAID 等级的多磁盘组。于是 zpool 上的文件系统可以使用这些虚拟设备的总存储容量。Docker 的 ZFS 利用 snapshots 和 clones，它们是 ZFS 的实时拷贝，snapshots 是只读的，clones 是读写的，clones 从 snapshot 创建。 下面看一下在 Docker 里 ZFS 的使用。首先从 zpool 里分配一个 ZFS 文件系统给镜像的基础层，而其他镜像层则是这个 ZFS 文件系统快照的克隆，快照是只读的，而克隆是可写的，当容器启动时则在镜像的最顶层生成一个可写层。如下图所示： 分析 ZFS 同 Btrfs 类似是下一代文件系统。ZFS 在 Linux(ZoL)port 是成熟的，但不推荐在生产环境上使用 Docker 的 ZFS 存储方式，除非你有 ZFS 文件系统的经验。 警惕 ZFS 内存问题，因为，ZFS 最初是为了有大量内存的 Sun Solaris 服务器而设计 。 ZFS 的 “deduplication” 特性，因为占用大量内存，推荐关掉。但如果使用 SAN，NAS 或者其他硬盘 RAID 技术，可以继续使用此特性。 ZFS caching 特性适合高密度场景。 ZFS 的 128K 块写，intent log 及延迟写可以减少碎片产生。 和 ZFS FUSE 实现对比，推荐使用 Linux 原生 ZFS 驱动。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[仓库]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-registry%2F</url>
      <content type="text"><![CDATA[仓库（Repository）是集中 存放镜像 的地方。仓库概念的引入，为 Docker 镜像文件的分发和管理提供了便捷的途径。 一个容易混淆的概念是注册服务器（Registry）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 store.docker.com/ubuntu 来说，store.docker.com 是注册服务器地址，ubuntu 是仓库名。大部分时候，并 不需要严格区分这两者的概念 。 仓库分为公有仓库和私有仓库。 公有仓库地址： Docker Store 官方镜像库: https://store.docker.com/ 阿里云镜像库： https://dev.aliyun.com/search.html DaoClound 镜像库： https://hub.daocloud.io/ 私有仓库 搭建与使用见： Harbor 私有仓库部署章节。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[配置 HTTPS 访问的 Harbor:v05.0]]></title>
      <url>%2F2016%2F10%2F24%2Fharbor-configure-https-v05.0-rc2%2F</url>
      <content type="text"><![CDATA[Harbor 默认 HTTP 请求注册服务器，并不分发任何证书。使得其配置相对简单。然而，强烈建议在生产环境中增强安全性。Harbor 有一个 Nginx 实例为所有服务提供反向代理，你可以在 Nginx 配置中启用 HTTPS 。 获取证书假定你的仓库的 hostname 是 reg.yourdomain.com，并且其 DNS 记录指向主机正在运行的 Harbor。首先，你需要获取一个 CA 证书。证书通常包含一个 .crt 文件和 .key 文件，例如， yourdomain.com.crt 和 yourdomain.com.key。 在测试或开发环境中，你可能会选择使用自签名证书，而不是购买 CA 证书。下面的命令会生成自签名证书： 1) 生成自签名 CA 证书: 123openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout ca.key \ -x509 -days 365 -out ca.crt 2) 生成证书签名请求: 如果使用 FQDN (完全限定域名) 如 reg.yourdomain.com 作为你的注册服务器连接，那你必须使用 reg.yourdomain.com 作为 CN (Common Name)。另外，如果使用 IP 地址作为你的注册服务器连接，CN 可以是你的名字等等： 123openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key \ -out yourdomain.com.csr 3) 生成注册服务器证书: 假定你使用类似 reg.yourdomain.com 这样的 FQND 作为注册服务器连接，运行下面的命令为你的注册服务器生成证书： 1openssl x509 -req -days 365 -in yourdomain.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out yourdomain.com.crt 假定你使用 IP ， 如 192.168.1.101 作为注册服务器连接，你可以运行以下命令： 1234echo subjectAltName = IP:192.168.1.101 &gt; extfile.cnfopenssl x509 -req -days 365 -in yourdomain.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out yourdomain.com.crt 配置和安装在取得 yourdomain.com.crt 和 yourdomain.com.key 文件后，你可以将它们放置在如 目录下：1234```bashcp yourdomain.com.crt /root/cert/cp yourdomain.com.key /root/cert/ 接下来，编辑 harbor/make/harbor.cfg, 更新 ， ```protocol```， ```ssl_cert``` 和 ```ssl_cert_key``` 属性：12345678910```bash#set hostnamehostname = reg.yourdomain.com#set ui_url_protocolui_url_protocol = https......#The path of cert and key files for nginx, they are applied only the protocol is set to httpsssl_cert = /root/cert/yourdomain.com.crtssl_cert_key = /root/cert/yourdomain.com.key 为 Harbor 生成配置文件： 12# 工作目录 harbor/make./prepare 如果 Harbor 已经在运行，则停止并删除实例。你的镜像数据将保留在文件系统中。 1234# 工作目录 harbor/make# 在运行 compose 前， 需将 docker-compose.tpl 文件重命名为 docker-compose.yamlmv docker-compose.tpl docker-compose.yamldocker-compose down 最后，重启 Harbor: 1docker-compose up -d 在为 Harbor 配置 HTTPS 完成后，你可以通过以下步骤对它进行验证： 打开浏览器，并输入地址：https://reg.yourdomain.com。应该会显示 Harbor 的界面。 在装有 Docker daemon 的机器上，确保 Docker engine 配置文件中没有配置 “-insecure-registry”，并且，你必须将已生成的 ca.crt 文件放入 /etc/docker/certs.d/yourdomain.com(或 / etc/docker/certs.d/your registry host IP) 目录下，如果这个目录不存在，则创建它。如果你将 nginx 的 443 端口映射到其他端口上了，你需要创建目录的为 /etc/docker/certs.d/yourdomain.com:port(/etc/docker/certs.d/your registry host IP:port)。然后，运行如下命令验证是否安装成功。 1docker login reg.yourdomain.com 如果你将 nginx 443 端口映射到其他端口上，则需要在登录时添加端口号，如： 1docker login reg.yourdomain.com:port 疑难解答1. 你可能是通过证书发行商获取中间证书。在这种情况下，你应该合并中间证书与自签证书，通过如下命令即可实现： 1cat intermediate-certificate.pem &gt;&gt; yourdomain.com.crt 2. 在一些运行着 docker daemon 的系统中，你可能需要操作系统级别的信任证书。 在 Ubuntu 上, 可以通过以下命令来完成： 12345cp youdomain.com.crt /usr/local/share/ca-certificates/reg.yourdomain.com.crtupdate-ca-certificates ``` - 在 Red Hat (CentOS etc) 上, 命令如下: cp yourdomain.com.crt /etc/pki/ca-trust/source/anchors/reg.yourdomain.com.crtupdate-ca-trust```]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker 概述]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-overview%2F</url>
      <content type="text"><![CDATA[Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全 沙箱机制，相互之间不会有任何接口（类似 ios app ）。几乎没有任何性能开销，可以很容易地在机器和数据中心运行。最重要的是，他们不依赖于任何语言、框架或包括系统。 Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL 6.5 中集中支持 Docker，也在其 PaaS 产品中广泛应用。 Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。Docker 的基础是 Linux 容器（LXC）等技术。 在 LXC 的基础上 Docker 进一步封装，让用户不需要去关心容器的管理，使得操作更加简便，用户操作 Docker 的容器就像一个快速轻量级的虚拟机一样简单。 下面对比 Docker 和传统虚拟化（KVM、XEN 等）方式的不同之处，容器 是在操作系统层面上实现的虚拟化，直接复用本地主机的操作系统；而 传统方式 则是在硬件基础上，虚拟出自己的系统，再在系统上部署相关的 APP 应用。 下图为传统虚拟化方案与 Docker 虚拟化方案对比： 三个概念 镜像：Docker 的镜像其实就是模版，跟我们常见的 ISO 镜像类似，是一个样板。 容器：使用镜像构建常见的应用或者系统，我们称之为一个容器。 仓库：仓库是存放镜像的地方，分为公共仓库(Publish)和私有仓库（Private）两种形式。 Docker 虚拟化特点跟传统 VM 比较有如下特点： 操作启动快：运行时的性能可以获得极大提升，管理操作（启动，停止，开始，重启等）都是秒或毫秒为单位的。 轻量级虚拟化：你会拥有足够的 “操作系统”，仅需添加镜像即可。在一台服务器上部署 100~1000 个 Container 容器，但是传统虚拟化，虚拟 10~20 个虚拟机就不错了。 开源免费：开源的，免费的，低成本的。有现代 Linux 内核支持并驱动（ps：轻量级的 Container 必定可以在一个物理机上开启更多 “容器”）。 前景及云支持：越来越受欢迎，包括各大主流公司都在推动 Docker 的快速发展，性能上有很大的优势。 快速跳转 Docker 概述 Docker Engine 安装 镜像 容器 仓库 容器数据管理 使用网络 Dockerfile 高级网络配置 存储驱动]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker Swarm Mode]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-swarm-mode%2F</url>
      <content type="text"><![CDATA[Docker Swarm Mode特性 集群管理与 Docker 引擎相结合：使用 Docker 引擎 CLI 便可创建一个 Docker 引擎的 Swarm，在这个集群中进行应用服务的部署。对于Swarm 集群的创建和管理，无需其他额外的编排软件。 分散设计：Docker 引擎不是在部署时处理节点角色之间的差异化内容，而是在运行时处理特殊化内容。通过使用 Docker 引擎可以部署管理节点和工作节点。这就意味着你可从一个单一的磁盘映像上创建一个完整的 Swarm 集群。 支持面向服务的组件模型：Docker 引擎运用描述性方式，让用户在应用堆栈上定义各种服务的理想状态。例如，可以这样描述一个应用：包含一个配有信息排队服务的 Web 前端服务和一个数据库后端。 弹性放缩：对于每项服务，你都可以明确想要运行的任务数。当你增加或减少任务数时，Swarm 管理器可自动增加或删除任务，以保持理想状态。 理想状态的调整：Swarm 管理节点持续对集群状态进行监控，并对实际状态和你明确的理想状态之间的差异进行调整。例如，当你创建一个服务来运行一个容器的10个副本，托管其中两个副本的工作机崩溃时，管理器将创建两个新的副本，代替之前崩溃的两个。Swarm管理器会将新的副本分配给正在运行且可用的工作机。 多主机网络：你可以针对你的服务指定一个的 Overlay 网络。Swarm管理器初始化或更新应用时，它会自动将地址分配给 Overlay 网上的容器。 服务发现：Swarm管理节点给Swarm集群上的每项服务分配一个唯一的 DNS 名称以及负载均衡的运行容器。可通过嵌入 Swarm 的 DNS 服务器对集群上运行的各个容器进行查询。 负载均衡：可以把服务端口暴露给外部负载均衡器。Swarm 内部允许你明确如何分配节点间的服务容器。 默认安全：Swarm 上的各个节点强制 TLS 互相授权和加密，从而确保自身与其他所有节点之间的通讯安全。可选择使用自签的根证书或自定义根 CA 证书。 滚动升级：升级时，可以逐增地将服务更新应用到节点上。Swarm 管理器允许你对部署服务到不同节点集而产生的时滞进行控制。如果出错，可以回退任务到服务的前一个版本。 关键概念下面介绍了一些 Docker 引擎1.12集群管理及编排特点的专门概念。 SwarmDocker 引擎内置的集群管理和编排功能是利用 SwarmKit 工具进行开发。加入集群的引擎在 Swarm 模式下运行。可通过初始化 Swarm 集群或加入现有的 Swarm 集群启动引擎的 Swarm 模式。 Swarm 集群指的是一个可以部署服务的Docker引擎集群。Docker引擎 CLI 包含 Swarm 集群管理指令，如增加或删除节点。CLI 也包含将服务部署到 Swarm 集群以及对服务编排进行管理的指令。 在 Swarm 模式外运行 Docker 引擎时，需执行容器命令。在 Swarm模式下运行引擎时，则是对服务进行编排。 节点节点指加入Swarm集群的Docker引擎的一个实例。 为将应用部署到Swarm中，你需要向管理节点提交一个服务定义。管理节点会将被称为任务的工作单元分配给工作节点。 同时，管理节点也执行编排和集群管理功能，以维持Swarm集群的理想状态。管理节点选择一个单一的引导段来执行编排任务。 工作节点接收并执行管理节点分配的任务。默认管理节点也作为工作节点，但可将其设置为仅为管理节点。代理将所分配的任务的当前状态通知管理节点，以便管理节点能够维持理想状态。 服务与任务服务指的是在工作节点上执行的任务。服务是Swarm集群系统的中心结构，也是用户与Swarm互动的主根。 当创建一项服务时，需明确使用哪个容器映像以及在运行的容器中执行何种指令。 在复制型服务模型下，Swarm管理器依据在理想状态下所设定的规模将一定数目的副本任务在节点中进行分配。 对于global服务，Swarm在集群中各个可用的节点上运行任务。 一项任务承载一个Docker容器以及在容器内运行的指令。任务是Swarm集群的最小调度单元。管理节点按照在服务规模中所设定的副本数量将任务分配给工作节点。任务一旦被分配到一个节点，便不可能转移到另一个节点。它只能在被分配的节点上运行或失效。 负载均衡Swarm 管理器使用入口负载均衡来暴露服务，使这些服务对 Swarm 外部可用。Swarm 管理器能自动给服务分配一个 PublishedPort，或你可为服务在30000-32767范围内设置一个 PublishedPort。 外部组件，例如云负载均衡器，能够在集群中任一节点的PublishedPort 上访问服务，无论节点是否正在执行任务。Swarm 中的所有节点会将入口连接按路径发送到一个正在运行任务的实例中。 Swarm 模式有一个内部的 DNS 组件，可自动给集群中的每项服务分配一个 DNS 入口。Swarm 管理器依据服务的 DNS 名称，运用内部负载均衡在集群内的服务之间进行请求的分配。 先决条件1. 开放端口： TCP port 2377 for cluster management communications（集群管理） TCP and UDP port 7946 for communication among nodes（节点通信） TCP and UDP port 4789 for overlay network traffic（overlay 网络流量） 2. 节点类型： 管理节点：负责执行维护 Swarm 必要状态所必需的编排与集群管理功能。管理节点会选择单一主管理方执行编排任务。Swarm Mode 要求利用奇数台管理节点以维持容错能力。 工作节点：负责从管理节点处接收并执行各项任务。 在默认情况下，管理节点本身同时也作为工作节点存在，但大家可以通过配置保证其仅执行管理任务。 3. 查看端口开放信息 123$ iptables -L$ netstat -nulp (UDP类型的端口)$ netstat -ntlp (TCP类型的端口) 4. 查看防火墙信息 1$ systemctl status iptables RHEL 7 系列 iptables 已经 firewalld 取代。 Swarm 构建1. 当创建 Swarm 集群时，我们需要指定一个节点。在这个例子中，我们会使用主机名为manager0的主机作为 manager 节点。为了使 manager01 成为 manager 节点，我们需要首先在 manager0 执行命令来创建 Swarm 集群。这个命令就是 Docker 命令的 swarm init 选项。 1$ docker swarm init --advertise-addr &lt;MANAGER-IP&gt; 上述命令中，除了 swarm init 之外，我们还指定了 –advertise-addr为。Swarm manager 节点会使用该 IP 地址来广告 Swarm 集群服务。虽然该地址可以是私有地址，重要的是，为了使节点加入该集群，那些节点需要能通过该 IP 的2377端口来访问 manager 节点。 在运行 docker swarm init 命令之后，我们可以看到 manager0 被赋予了一个节点名字（ awwiap1z5vtxponawdqndl0e7 ），并被选为 Swarm 集群的管理器。输出中也提供了两个命令：一个命令可以添加 worker 节点到 swarm 中，另一个命令可以添加另一个 manager 节点到该 Swarm 中。 Docker Swarm Mode 可以支持多个manager 节点。然而，其中的一个会被选举为主节点服务器，它会负责 Swarm 的编排。 2. 根据 docker info 信息，查看 swarm 状态（ Active/Inactive ）。 3. 添加 worker节点到Swarm 集群中。 Swarm 集群建立之后，我们需要添加一个新的 worker 节点到集群中。在 manger0 节点执行如下指令。 1$ docker swarm join-token worker 根据上述命令输出，复制粘贴到 worker 节点并运行。Swarm 集群中的 worker 节点的角色是用来运行任务（ tasks ）的；这里所说的任务（ tasks ）就是容器（ containers ）。另一方面，manager 节点的角色是管理任务（容器）的编排，并维护 Swarm 集群本身。 4. 添加 manager 节点。在 manger0 节点执行如下指令，根据提示添加 manager 节点。 1$ docker swarm join-token manager 节点管理1. 查看当前 Swarm 节点。我们可以执行 Docker 命令的 node ls 选项来验证集群的状态。 1$ docker node ls 2. 查看节点信息 1$ docker node inspect &lt;NODE-ID&gt; [--pretty] 3. 提升节点 1$ docker node promote &lt;HOSTNAME&gt; 4. 降级节点 1$ docker node demote &lt;HOSTNAME&gt; 5. 删除节点 1$ docker node rm &lt;NODE_ID&gt; 6. 脱离docker swarm 1$ docker swarm leave [--force] 服务部署在 Docker Swarm Mode中，服务是指一个长期运行（ long-running ）的 Docker 容器，它可以被部署到任意一台 worker 节点上，可以被远端系统或者 Swarm 中其他容器连接和消费（ consume ）的。 1. 创建一个服务 1$ docker service create &lt;IMAGE ID&gt; 例如： 12$ docker service create --name nginx --replicas 2 --publish 8080:80 nginx 一个有副本的服务是一个 Docker Swarm 服务，运行了特定数目的副本（ --replicas ）。这些副本是由多个 Docker 容器的实例组成的。 2. Scale 1$ docker service scale &lt;SERVICE-ID&gt;=&lt;NUMBER-OF-TASKS&gt; 3. 查看所有服务 1$ docker service ls 4. 查看单个服务部署详情 1$ docker service ps &lt;service_id|service_name&gt; 5. 查看服务信息 1$ docker service inspect &lt;service_id|service_name&gt; 6. 删除服务 1$ docker service rm &lt;service_id&gt; 服务发布当我们创建了 nginx 服务时，我们使用了 –publish 选项。该选项用来告知 Docker 将端口8080发布为 nginx 服务的可用端口。 当 Docker 发布了服务端口时，它在 Swarm 集群上的所有节点上监听该端口。当流量到达该端口时，该流量将被路由到运行该服务的容器上。如果所有节点都运行着一个服务的容器，那么概念是相对标准的；然而，当我们的节点数比副本多时，概念就变得有趣了。 服务global化假设 Swarm 集群中有三个节点，此时，我们已经建立了 nginx 服务，运行了2个副本，这意味着，3个节点中的2个正在运行容器。 如果我们希望 nginx 服务在每一个 worker 节点上运行一个实例，我们可以简单地修改服务的副本数目，从2增加到3。这意味着，如果我们增加或者减少 worker 节点数目，我们需要调整副本数目。 我们可以自动化地做这件事，只要把我们的服务变成一个 Global Service。Docker Swarm Mode 中的 Global Service 使用了创建一个服务，该服务会自动地在每个 worker 节点上运行任务。这种方法对于像 nginx 这样的一般服务都是有效的。 让我们重新创建 redis 服务为 Global Service。 1$ docker service create --name nginx --mode global --publish 8080:80 nginx 同样是 docker service create 命令，唯一的区别是指定了 –mode 参数为 global。服务建立好之后，运行 docker service ps nginx 命令的，我们可以看到，Docker 是如何分发该服务的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker Engine 安装]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-install-docker%2F</url>
      <content type="text"><![CDATA[参考 Docker 官网用户指南：https://docs.docker.com/engine/installation/linux/ 环境依赖 Linux：64 位 Kernel 3.10 及以上（RHEL7 以上版本） Windows：64 位 在 Ubuntu 上安装 DockerDocker 支持以下的 Ubuntu 版本 Ubuntu Xenial 16.04 (LTS) Ubuntu Wily 15.10 Ubuntu Trusty 14.04 (LTS) Ubuntu Precise 12.04 (LTS) 先决条件Docker 需要在 64 位版本的 Ubuntu 上安装。此外，你还需要保证你的 Ubuntu 内核的最小版本不低于 3.10，其中 3.10 小版本和更新维护版也是可以使用的。 在低于 3.10 版本的内核上运行 Docker 会丢失一部分功能。在这些旧的版本上运行 Docker 会出现一些 BUG，这些 BUG 在一定的条件里会导致数据的丢失，或者报一些严重的错误。 打开控制台使用 uname -r 命令来查看你当前的内核版本。 12$ uname -r3.11.0-15-generic Docker 要求 Ubuntu 系统的内核版本高于 3.10，查看本页面的前提条件来验证你的 Ubuntu 版本是否支持 Docker 。 更新 APT 源通过下边的操作来升级你的内核和安装额外的包 1. 登录到主机，切换到 root 用户，取得权限。 2. 打开 Ubuntu 命令行控制台。 3. 升级你的包管理器，确保 apt 能够使用 htpps，安装 CA 证书 12$ sudo apt-get update$ sudo apt-get install apt-transport-https ca-certificates 4. 添加 Docker 公钥。 配置 Apt 来使用新仓库的第一步是想 Apt 缓存中添加该库的公钥。使用 apt-key 命令： 123$ sudo apt-key adv \ --keyserver hkp://ha.pool.sks-keyservers.net:80 \ --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 以上的 apt-key 命令向密钥服务器 hkp://ha.pool.sks-keyservers.net 请求一个特定的密钥（ 58118E89F3A912897C070ADBF76221572C52609D ）。公钥将会被用来验证从新仓库下载的所有包。 5. 依据你的 Ubuntu 发行版本，在下表中找到对应条目。这个将决定 APT 将搜索的 Docker 包。如果可能，请运行一个长期支持 (LTS) 版的 Ubuntu。 Ubuntu version Repository Precise 12.04 (LTS) deb https://apt.dockerproject.org/repo ubuntu-precise main Trusty 14.04 (LTS) deb https://apt.dockerproject.org/repo ubuntu-trusty main Wily 15.10 deb https://apt.dockerproject.org/repo ubuntu-wily main Xenial 16.04 (LTS) deb https://apt.dockerproject.org/repo ubuntu-xenial main 6. 指定 Docker 仓库的位置，根据第 5 步确定的发行版本，替换下面命令行中的 &lt;REPO&gt; 参数即可。 1234$ echo "&lt;REPO&gt;" | sudo tee /etc/apt/sources.list.d/docker.list## 例如：$ echo "deb https://apt.dockerproject.org/repo ubuntu-xenial" | sudo tee /etc/apt/sources.list.d/docker.list 引入 Docker 的公钥，我们可以配置 Apt 使用 Docker 的仓库服务器。新建文件：/etc/apt/sources.list.d/docker.list，在里面添加一个条目：deb https://apt.dockerproject.org/repo ubuntu-xenial main 7. 更新 APT 软件包索引 1$ sudo apt-get update 8. 验证 apt 拉取正确的 repo 12345678910111213$ apt-cache policy docker-engine docker-engine: Installed: 1.12.2-0~trusty Candidate: 1.12.2-0~trusty Version table: *** 1.12.2-0~trusty 0 500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages 100 /var/lib/dpkg/status 1.12.1-0~trusty 0 500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages 1.12.0-0~trusty 0 500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages Ubuntu Xenial 16.04 (LTS), Wily 15.10, Trusty 14.04 (LTS) 先决条件 对于支持 aufs 存储驱动的 Ubuntu Trusty, Wily, and Xenial 安装 linux-image-extra-* kernel 包 1. 打开 Ubuntu 命令行控制台 2. 更新 apt 包缓存 1$ sudo apt-get update 这会触发 apt 重新读取配置文件，刷新仓库列表，包含进我们添加的那个仓库。该命令也会查询这些仓库来缓存可用的包列表。 3. 安装 linux-image-extra-* 包 在安装 Docker Engine 之前，我们需要安装一个先决软件包（ prerequisite package ）。linux-image-extra 包是一个内核相关的包，Ubuntu 系统需要它来支持 aufs 存储设备驱动。Docker 会使用该驱动来加载卷。 1$ sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual 在 apt-get 命令中，$(uname -r) 将返回正在运行的内核的版本。任何对于该系统的内核更新应当包括安装 linux-image-extra，它的版本需要与新内核版本相一致。如果该包没有正确更新的话，Docker 加载卷的功能可能受到影响。 4. 安装 Docker 转到下一节 安装 Docker Engine Ubuntu Precise 12.04 (LTS) 先决条件 对于 Ubuntu Precise 发行版本， Docker 要求 Kernel 版本至少 3.13。如果你的 Kernel 版本老于 3.13，那你必须先升级内核。 1. 打开 Ubuntu 命令行控制台 2. 更新 apt 包缓存 1$ sudo apt-get update 3. 安装包 1$ sudo apt-get install linux-image-generic-lts-trusty 4. 重启宿主机 1$ sudo reboot 4. 安装 Docker 转到下一节 安装 Docker Engine 安装最新版本的 Docker Engine1. 以 sudo 的用户权限登录 Ubuntu。 2. 更新 APT 包索引 1$ sudo apt-get update 3. 安装 Docker 1$ sudo apt-get install docker-engine 4. 启动 Docker 守护进程 1$ sudo service docker start 5. 验证 Docker 是否安装成功 1$ sudo docker run hello-world 升级12$ sudo apt-get update$ sudo apt-get upgrade docker-engine 卸载1. 卸载 Docker 包 1$ sudo apt-get purge docker-engine 2. 卸载 docker 依赖包 1$ sudo apt-get autoremove --purge docker-engine 3. 删除相关文件 1$ rm -rf /var/lib/docker 在 RedHat 上安装 Docker先决条件Docker 需要在 64 位且内核版本不低于 3.10 的 Linux 上运行。 打开控制台使用 uname -r 命令来查看你当前的内核版本。 12$ uname -r3.10.0-229.el7.x86_64 安装 Docker Engine如下两种安装方式： Yum 安装 Script 安装 Yum 安装1. 以 sudo 或 root 权限登录宿主机 2. yum 更新。 1$ sudo yum update 3. 添加 yum 源 12345678$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF 4. 安装。 1$ sudo yum install docker-engine 5. 启用服务。 1$ sudo systemctl enable docker.service 6. 现在 Docker 已经安装好了，我来启动 Docker 守护进程。 12$ sudo systemctl start dockerRedirecting to /bin/systemctl start docker.service ps: sudo systemctl start docker.service 7. 验证 Docker 是否安装成功 123456789101112131415161718192021222324252627$ sudo docker run --rm hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-worldc04b14da8d14: Pull completeDigest: sha256:0256e8a36e2070f7bf2d0b0763dbabdd67798512411de4cdcf9431a1feb60fd9Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker Hub account: https://hub.docker.comFor more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 脚本安装1. 以 sudo 或 root 权限登录宿主机 2. yum 更新。 1$ sudo yum update 3. 运行 Docker 安装脚本。 1$ curl -fsSL https://get.docker.com/ | sh 4. 然后启用 Docker 服务 1$ sudo systemctl enable docker.service 5. 运行 Docker 线程 1$ sudo systemctl start docker 6. 验证 Docker 是否安装成功 1$ sudo docker run hello-world 卸载1. 列出已安装的 Docker 包 1$ yum list installed | grep docker 2. 删除包 1$ sudo yum -y remove docker-engine.x86_64 3. 删除镜像，容器，数据卷 1$ rm -rf /var/lib/docker 4. 删除用户数据 WindowsDokcer for Windows 64 位 windows10 pro。 开启 Hyper-V（控制面板 \ 程序 \ 程序和功能 \ 启用或关闭 Windows 功能 \ Hyper-V）。 官网下载并安装 Docker for Windows（https://docs.docker.com/docker-for-windows/）。 Docker Toolbox Windows10 pro 之前的 windows 系统。 官网下载并安装 Docker Toolbox（https://www.docker.com/products/docker-toolbox），包括了 docker 可视化软件Docker Kitematic]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[镜像]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-image%2F</url>
      <content type="text"><![CDATA[镜像是 Docker 的三大组件之一。Docker 运行容器前需要本地存在对应的镜像，如果镜像不存在本地，Docker 会从镜像仓库下载（Docker Store ，阿里云，Harbor…）。下面主要从如下两个方面介绍镜像相关内容： 拉取仓库镜像。 管理本地主机上的镜像。 拉取镜像使用 docker pull 命令来从仓库获取所需要的镜像。例如，将从 Docker Store 仓库下载一个 Ubuntu 12.04 操作系统的镜像。 123456789$ sudo docker pull ubuntu:12.04Pulling repository ubuntuab8e2728644c: Pulling dependent layers511136ea3c5a: Download complete5f0ffaa9455e: Download completea300658979be: Download complete904483ae0c30: Download completeffdaafd1ca50: Download completed047ae21eeaf: Download complete 下载过程中，会输出获取镜像的每一层信息。该命令实际上相当于 $ sudo docker pull store.docker.com/ubuntu:12.04 命令，即从注册服务器 store.docker.com 中的 ubuntu 仓库来下载标记为 12.04 的镜像。通常官方仓库注册服务器下载较慢，可以从其他仓库下载（阿里镜像库，daoclound 以及自己搭建的私有仓库等）。从其它仓库下载时需要指定完整的仓库注册服务器地址。例如： 123456789$ sudo docker pull 192.168.1.40/library/ubuntu:12.04Pulling 192.168.1.40/library/ubuntuab8e2728644c: Pulling dependent layers511136ea3c5a: Download complete5f0ffaa9455e: Download completea300658979be: Download complete904483ae0c30: Download completeffdaafd1ca50: Download completed047ae21eeaf: Download complete 完成后，即可随时使用该镜像了，例如创建一个容器，让其中运行 bash 应用。 12$ sudo docker run -ti 192.168.1.40/library/ubuntu:12.04 /bin/bashroot@fe7fc4bd8fc9:/# 查看本地镜像使用 docker images 命令显示本地已有的镜像。 12345678$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE192.168.1.40/library/ubuntu 12.04 74fe38d11401 4 weeks ago 209.6 MBubuntu precise 74fe38d11401 4 weeks ago 209.6 MBubuntu 14.04 99ec81b80c55 4 weeks ago 266 MBubuntu latest 99ec81b80c55 4 weeks ago 266 MBubuntu trusty 99ec81b80c55 4 weeks ago 266 MB…… 在列出信息中，可以看到几个字段信息 REPOSITORY: 来自于哪个仓库，比如 192.168.1.40/library/ubuntu IMAGES 镜像的标记，比如 14.04 ID: 它的 ID 号（唯一） CREATED: 创建时间 SIZE: 镜像大小 其中镜像的 ID 唯一标识了镜像，注意到 ubuntu:14.04 和 ubuntu:trusty 具有相同的镜像 ID，说明它们实际上是同一镜像。 TAG 信息用来标记来自同一个仓库的不同镜像。例如 ubuntu 仓库中有多个镜像，通过 TAG 信息来区分发行版本，例如 10.04、12.04、12.10、13.04、14.04 等。例如下面的命令指定使用镜像 ubuntu:14.04 来启动一个容器。 1$ sudo docker run -ti ubuntu:14.04 /bin/bash 如果不指定具体的标记，则默认使用 latest 标记信息。 搜索镜像 docker search [–automated=false] [–no-trunc=false] [-s] [IMAGE_NAME] –automated=false: 仅显示自动创建的镜像。 –no-trunc-false: 输出信息不截断显示。 -s, –stars=0: 指定仅显示评价为指定星级以上的镜像。 使用 docker search 命令搜索镜像仓库（Docker Store ，阿里云，Harbor…）中共享的镜像，默认搜索 Docker Store 官方仓库中的镜像。例如： 1$ docker search ubuntu 删除镜像 docker rmi [–force] IMAGE [IMAGE…] 使用 docker rmi 命令删除镜像，例如。 1$ docker rmi 192.168.1.40/library/ubuntu:12.04 镜像构建镜像构建 的方法很多，用户可以从 Docker Store 获取已有镜像并更新，也可以利用本地文件系统创建一个。修改已有镜像，先使用已下载的镜像启动容器。 1$ sudo docker run -t -I ubuntu:14.04 /bin/bash 注意：记住容器的 ID，稍后还会用到。在容器中添加 gcc。 1root@0b2616b0e5a8:/# apt-get install gcc 当结束后，我们使用 exit 来退出，现在我们的容器已经被我们改变了，使用 docker commit 命令来提交更新后的副本。 12$ sudo docker commit -m "Added gcc" -a "Docker Newbee" 0b2616b0e5a8 test/ubuntu:v14f177bd27a9ff0f6dc2a830403925b5360bfe0b93d476f7fc3231110e7f71b1c 其中，-m 来指定提交的说明信息，跟我们使用的版本控制工具一样；-a 可以指定更新的用户信息；之后是用来创建镜像的容器的 ID；最后指定目标镜像的仓库名和 tag 信息。 创建成功后会返回这个镜像的 ID 信息。使用 docker images 来查看新创建的镜像。 1234$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEubuntu 14.04 99ec81b80c55 4 weeks ago 266 MBtest/ubuntu:v1 v1 4f177bd27a9ff0f6 10 hours ago 287 MB 之后，可以使用新的镜像来启动容器 1$ sudo docker run -t -i test/ubuntu:v1 /bin/bash 利用 Dockerfile 来创建镜像使用 docker commit 来扩展一个镜像比较简单，但是不方便在一个团队中分享。我们可以使用 docker build 来创建一个新的镜像。为此，首先需要创建一个 Dockerfile，包含一些如何创建镜像的指令。新建一个目录和一个 Dockerfile。 123$ mkdir ubuntu-test$ cd ubuntu-test$ touch Dockerfile Dockerfile 中每一条指令都创建镜像的一层，例如： 12345# This is a commentFROM ubuntu:14.04MAINTAINER tangpeng tang.peng@hotmail.comRUN apt-get updateRUN apt-get install -y gcc Dockerfile 基本的语法是使用 # 注释内容，FROM 指令告诉 Docker 使用哪个镜像作为基础，接着是维护者的信息 RUN 开头的指令会在创建中运行，比如安装一个软件包，在这里使用 apt-get 来安装了一些软件，编写完成 Dockerfile 后可以使用 docker build 命令来生成镜像。 123456789101112131415161718$ sudo docker build –t test/ubuntu:v2 .Uploading context 2.56 kBUploading contextStep 0 : FROM ubuntu:14.04 ---&gt; 99ec81b80c55Step 1 : MAINTAINER tangpeng tang.peng@hotmail.com ---&gt; Running in 7c5664a8a0c1 ---&gt; 2fa8ca4e2a13Removing intermediate container 7c5664a8a0c1Step 2 : RUN apt-get update ---&gt; Running in b07cc3fb4256 ---&gt; 50d21070ec0cRemoving intermediate container b07cc3fb4256Step 3 : RUN apt-get install -y gcc ---&gt; Running in a5b038dd127e ---&gt; 324104cde6adRemoving intermediate container a5b038dd127eSuccessfully built 324104cde6ad 其中 -t 标记来添加 tag，指定新的镜像的用户信息。”.“ 是 Dockerfile 所在的路径（当前目录），也可以替换为一个具体的 Dockerfile 的路径。可以看到 build 进程在执行操作。它要做的第一件事情就是上传这个 Dockerfile 内容，因为所有的操作都要依据 Dockerfile 来进行。然后，Dockerfile 中的指令被一条一条的执行。每一步都创建了一个新的容器，在容器中执行指令并提交修改（就跟之前介绍过的 docker commit 一样）。当所有的指令都执行完毕之后，返回了最终的镜像 id。所有的中间步骤所产生的容器都被删除和清理了。 注意一个镜像不能超过 127 层。此外，还可以利用 ADD 或 COPY 命令复制本地文件到镜像；用 EXPOSE 命令来向外部开放端口；用 CMD 命令来描述容器启动后运行的程序等。例如: 1234567891011# put my local web site in myApp folder to /var/wwwADD myApp /var/www# expose httpd portEXPOSE 80# the command to runCMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"] 现在可以利用新创建的镜像来启动一个容器。 12$ sudo docker run -ti test/ubuntu:v2 /bin/bashroot@8196968dac35:/# 载出与载入镜像语法 docker save -o [IMAGE_FILE_NAME] [IMAGE_NAME:TAG]docker load &lt;[IMAGE_FILE_NAME] 或 docker load –input [IMAGE_FILE_NAME] 载出镜像使用 docker save 命令打包镜像到本地。例如，载出一个 ubuntu:v2 镜像为文件 ubuntu_v2.tar。 1$ docker save -o ubuntu_v2.tar ubuntu:v2 载入镜像使用 docker load 命令将本地镜像文件导入到本地镜像库中。例如，载入一个 ubuntu_v2.tar 镜像文件。 1$ docker load &lt; ubuntu:v2.tar 或 1$ docker load --input ubuntu:v2.tar 标记镜像使用 docker tag 命令来修改镜像的标签。 123456$ sudo docker tag 324104cde6ad test/ubuntu:v2.1$ sudo docker images test/ubuntuREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtest/ubuntu v1 5db5f8471261 11 hours ago 287 MBtest/ubuntu v2 5db5f8471261 11 hours ago 446.7 MBtest/ubuntu v2.1 5db5f8471261 11 hours ago 446.7 MB 更多用法，参考 Dockerfile 章节。 上传镜像上传镜像用户可以通过 docker push 命令，把自己创建的镜像上传到仓库中来共享。例如，用户在 Docker Store 上完成注册并审核后，可以推送自己的镜像到仓库中。 1234567$ sudo docker push test/ubuntu:v2.1The push refers to a repository [test/ubuntu:v2.1] (len: 1)Sending image listPushing repository test/ubuntu:v2.1 (3 tags)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dockerfile]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-dockerfile-reference%2F</url>
      <content type="text"><![CDATA[原文链接：https://docs.docker.com/engine/reference/builder/ Dockerfile 最佳实践：https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/ FROM：指定基础 image构建指令，必须指定且需要在 Dockerfile 其他指令的前面。后续的指令都依赖于该指令指定的 image。FROM 指令指定的基础 image 可以是官方远程仓库中的，也可以位于本地仓库。该指令有两种格式： 1FROM &lt;image&gt; 指定基础 image 为该 image 的最后修改的版本。或者： 1FROM &lt;image&gt;:&lt;tag&gt; 指定基础 image 为该 image 的一个 tag 版本。 MAINTAINER：用来指定镜像创建者信息构建指令，用于将 image 的制作者相关的信息写入到 image 中。当我们对该 image 执行 docker inspect 命令时，输出中有相应的字段记录该信息。 格式： 1MAINTAINER &lt;name&gt; RUN：安装软件用构建指令，RUN 可以运行任何被基础 image 支持的命令。如基础 image 选择了 ubuntu，那么软件管理部分只能使用 ubuntu 的命令。 该指令有两种格式： 12RUN &lt;command&gt; (the command is run in a shell - `/bin/sh -c`) RUN ["executable", "param1", "param2" ...] (exec form) CMD：设置 container 启动时执行的操作设置指令，用于 container 启动时指定的操作。该操作可以是执行自定义脚本，也可以是执行系统命令。该指令只能在文件中存在一次，如果有多个，则只执行最后一条。 该指令有三种格式： 12CMD ["executable","param1","param2"] (like an exec, this is the preferred form) CMD command param1 param2 (as a shell) 当 Dockerfile 指定了 ENTRYPOINT，那么使用下面的格式： 1CMD ["param1","param2"] (as default parameters to ENTRYPOINT) ENTRYPOINT 指定的是一个可执行的脚本或者程序的路径，该指定的脚本或者程序将会以 param1 和 param2 作为参数执行。所以如果 CMD 指令使用上面的形式，那么 Dockerfile 中必须要有配套的 ENTRYPOINT。 ENTRYPOINT：设置 container 启动时执行的操作设置指令，指定容器启动时执行的命令，可以多次设置，但是只有最后一个有效。两种格式: 12ENTRYPOINT ["executable", "param1", "param2"] (like an exec, the preferred form) ENTRYPOINT command param1 param2 (as a shell) 该指令的使用分为两种情况，一种是独自使用，另一种和 CMD 指令配合使用。 当独自使用时，如果你还使用了 CMD 命令且 CMD 是一个完整的可执行的命令，那么 CMD 指令和 ENTRYPOINT 会互相覆盖只有最后一个 CMD 或者 ENTRYPOINT 有效。 123# CMD 指令将不会被执行，只有 ENTRYPOINT 指令被执行 CMD echo "Hello, World!" ENTRYPOINT ls -l 另一种用法和 CMD 指令配合使用来指定 ENTRYPOINT 的默认参数，这时 CMD 指令不是一个完整的可执行命令，仅仅是参数部分；ENTRYPOINT 指令只能使用 JSON 方式指定执行命令，而不能指定参数。 123FROM ubuntu CMD ["-l"] ENTRYPOINT ["/usr/bin/ls"] USER：设置 container 容器的用户设置指令，设置启动容器的用户，默认是 root 用户。 12345# 指定 memcached 的运行用户 ENTRYPOINT ["memcached"] USER daemon 或 ENTRYPOINT ["memcached", "-u", "daemon"] EXPOSE：指定容器需要映射到宿主机器的端口设置指令，该指令会将容器中的端口映射成宿主机器中的某个端口。当你需要访问容器的时候，可以不是用容器的 IP 地址而是使用宿主机器的 IP 地址和映射后的端口。要完成整个操作需要两个步骤，首先在 Dockerfile 使用 EXPOSE 设置需要映射的容器端口，然后在运行容器的时候指定 –p 选项加上 EXPOSE 设置的端口，这样 EXPOSE 设置的端口号会被随机映射成宿主机器中的一个端口号。也可以指定需要映射到宿主机器的那个端口，这时要确保宿主 机器上的端口号没有被使用。EXPOSE 指令可以一次设置多个端口号，相应的运行容器的时候，可以配套的多次使用 –p 选项。格式: 12345678910111213EXPOSE &lt;port&gt; [&lt;port&gt;...] # 映射一个端口 EXPOSE port1 # 相应的运行容器使用的命令 docker run -p port1 image # 映射多个端口 EXPOSE port1 port2 port3 # 相应的运行容器使用的命令 $ docker run -p port1 -p port2 -p port3 image # 还可以指定需要映射到宿主机器上的某个端口号$ docker run -p host_port1:port1 -p host_port2:port2 -p host_port3:port3 image 端口映射是 Docker 比较重要的一个功能，原因在于我们每次运行容器的时候容器的 IP 地址不能指定而是在桥接网卡的地址范围内随机生成的。宿主机器的 IP 地址是固定的，我们可以将容器的端口的映射到宿主机器上的一个端口，免去每次访问容器中的某个服务时都要查看容器的 IP 的地址。对于一个运行的容器，可以使用 docker port 加上容器中需要映射的端口和容器的 ID 来查看该端口号在宿主机器上的映射端口。语法： docker port CONTAINER_ID [PRIVATE_PORT[/PROTO]] ENV：用于设置环境变量构建指令，在 image 中设置一个环境变量。格式: 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; [&lt;key&gt;=&lt;value&gt;...] 设置了后，后续的 RUN 命令都可以使用，container 启动后，可以通过 docker inspect 查看这个环境变量，也可以通过在 docker run –env key=value 时设置或修改环境变量。 假如你安装了 JAVA 程序，需要设置 JAVA_HOME，那么可以在 Dockerfile 中这样写： 1ENV JAVA_HOME /path/to/java/dirent ADD：从 src 复制文件到 container 的 dest 路径构建指令，所有拷贝到 container 中的文件和文件夹权限为 0755，uid 和 gid 为 0；如果是一个目录，那么会将该目录下的所有文件添加到 container 中，不包括目录；如果文件是可识别的压缩格式，则 Docker 会帮忙解压缩（注意压缩格式）；如果 是文件且 中不使用斜杠结束，则会将 视为文件， 的内容会写入 ； 如果 是文件且 中使用斜杠结束，则会 文件拷贝到 目录下。 格式: 1ADD &lt;src&gt; &lt;dest&gt; 是相对被构建的源目录的相对路径，可以是文件或目录的路径，也可以是一个远程的文件 url; 是 container 中的绝对路径 VOLUME：指定挂载点设置指令，使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以共享给其他容器使用。我们知道容器使用的是 AUFS，这种文件系 统不能持久化数据，当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在 Dockerfile 中使用该指令。 格式: 123VOLUME ["&lt;mountpoint&gt;"]VOLUME mountpointVOLUME mountpoint_1 mountpoint_2…… 实例： 12FROM base VOLUME ["/tmp/data"] 运行通过该 Dockerfile 生成 image 的容器，/tmp/data 目录中的数据在容器关闭后，里面的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的 / tmp/data 目录，那么可以运行下面的命令启动一个容器： 1docker run -t -i -rm -volumes-from container1 image2 bash container1 为第一个容器的 ID，image2 为第二个容器运行 image 的名字。 WORKDIR：切换目录设置指令，可以多次切换 (相当于 cd 命令)，对 RUN,CMD,ENTRYPOINT 生效。 格式: 123WORKDIR /path/to/workdir # 在 /p1/p2 下执行 vim a.txt WORKDIR /p1 WORKDIR p2 RUN vim a.txt ONBUILD：在子镜像中执行12ONBUILD &lt;Dockerfile 关键字&gt;ONBUILD 指定的命令在构建镜像时并不执行，而是在它的子镜像中执行。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[容器数据管理]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-data-volume%2F</url>
      <content type="text"><![CDATA[用户在使用 Docker 的过程中，往往需要能查看容器内应用产生的数据，或者需要把容器内的数据进行备份，甚至多个容器之间进行数据的共享，这必然涉及容器的数据管理操作。 在容器中管理数据主要有如下两种方式： 数据卷（Data volumes） 数据卷容器（Data volume containers） 数据卷数据卷是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 数据卷可以在容器之间 共享 和 重用。 对数据卷的修改会立马生效 对数据卷的更新，不会影响镜像 数据卷默认会一直存在，即使容器被删除 数据卷的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉，能显示看的是挂载的数据卷。 创建一个数据卷在用 docker run 命令的时候，使用 -v 标记来创建一个数据卷并挂载到容器里。在一次 run 中多次使用可以挂载多个数据卷。 下面创建一个名为 web 的容器，并加载一个数据卷到容器的 /webapp 目录。 1$ sudo docker run -d -P --name web -v /webapp training/webapp python app.py 也可以在 Dockerfile 中使用 VOLUME 来添加一个或者多个新的卷到由该镜像创建的任意容器。 删除数据卷数据卷是被设计用来 持久化数据 的，它的 生命周期独立于容器 ，Docker 不会在容器被删除后自动删除数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v 这个命令。无主的数据卷可能会占据很多空间，要清理会很麻烦。Docker 官方正在试图解决这个问题。 挂载一个主机目录作为数据卷使用 -v 标记也可以指定挂载一个本地主机的目录到容器中去。 1$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py 上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp 目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，如果目录不存在 Docker 会自动为你创建它。 Dockerfile 中不支持这种用法，这是因为 Dockerfile 是为了移植和分享用的。然而，不同操作系统的路径格式不一样，所以目前还不能支持。 Docker 挂载数据卷的默认权限是读写，用户也可以通过 :ro（read only）指定为只读。 12$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp:rotraining/webapp python app.py 加了 :ro 之后，就挂载为只读了。 查看数据卷的具体信息在主机里使用以下命令可以查看指定容器的信息 12$ docker inspect web... 在输出的内容中找到其中和数据卷相关的部分，可以看到所有的数据卷都是创建在主机的 /var/lib/docker/volumes/ 下面的 12345678"Volumes": &#123; "/webapp": "/var/lib/docker/volumes/fac362...80535"&#125;,"VolumesRW": &#123; "/webapp": true&#125;... 挂载一个本地主机文件作为数据卷-v 标记也可以从主机挂载单个文件到容器中 1$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash 这样就可以记录在容器输入过的命令了。 如果直接挂载一个文件，很多文件编辑工具，包括 vi 或者 sed –in-place，可能会造成文件 inode 的改变，从 Docker 1.1.0 起，这会导致报错误信息。所以最简单的办法就直接挂载文件的父目录。 数据卷容器如果你有一些持续更新的数据需要在容器之间共享，最好创建数据卷容器。数据卷容器，其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的。首先，创建一个名为 dbstore 的数据卷容器： 1$ docker create -v /dbdata --name dbstore training/postgres /bin/true 然后，在其他容器中使用 –volumes-from 来挂载 dbdata 容器中的数据卷。 12$ docker run -d --volumes-from dbstore --name db1 training/postgres$ docker run -d --volumes-from dbstore --name db2 training/postgres 可以使用超过一个的 --volumes-from 参数来指定从多个容器挂载不同的数据卷。也可以从其他已经挂载了数据卷的容器来级联挂载数据卷。 1$ docker run -d --name db3 --volumes-from db1 training/postgres 使用 –volumes-from 参数所挂载数据卷的容器自己并不需要保持在运行状态。如果删除了挂载的容器（包括 dbstore、db1 和 db2），数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载着它的容器时使用 docker rm -v 命令来指定同时删除关联的容器。这可以让用户在容器之间升级和移动数据卷。 利用数据卷容器来备份、恢复、迁移数据卷可以利用数据卷对其中的数据进行进行备份、恢复和迁移。 备份首先使用 --volumes-from 标记来创建一个加载 dbdata 容器卷的容器，并从主机挂载当前目录到容器的 /backup 目录。命令如下： 1$ sudo docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata 容器启动后，使用了 tar 命令来将 dbdata 卷备份为容器中 /backup/backup.tar 文件，也就是主机当前目录下的名为 backup.tar 的文件。 恢复如果要恢复数据到一个容器，首先创建一个带有空数据卷的容器 dbstore 。 1$ sudo docker run -v /dbdata --name dbstore ubuntu /bin/bash 然后创建另一个容器，挂载 dbstore 容器卷中的数据卷，并使用 un-tar 解压备份文件到挂载的容器卷中。 1$ docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c "cd /dbdata &amp;&amp; tar xvf /backup/backup.tar --strip 1" 为了查看/验证恢复的数据，可以再启动一个容器挂载同样的容器卷来查看。 1$ sudo docker run --volumes-from dbdata2 busybox /bin/ls /dbdata 共享数据卷虽然多个容器可以共享一个或多个数据卷，但是多个容器同时对一个数据卷进行写操作，会造成数据损坏。慎用共享数据卷。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[容器]]></title>
      <url>%2F2016%2F10%2F24%2Fdocker-container%2F</url>
      <content type="text"><![CDATA[容器是 Docker 又一核心概念。Docker 的容器十分轻量级，用户可以随时创建或删除容器。容器是直接提供应用服务的组件，也是已经掌握了对容器整个生命周期进行管理的各项操作命令。 简单的说，容器是独立运行的一个或一组应用，以及它们的运行态环境。而虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和运行在上面的应用。 下面将具体介绍如何来管理一个容器，包括创建、启动、停止和删除等。 新建容器语法： docker create [-i] [-t] [-d] [IMAGE_NAME] 可以使用 docker create 命令新建一个处于停止状态的容器。然后可以通过 docker start 命令来启动这个容器。 新建并启动容器启动容器的两种方式： 基于镜像新建一个容器并启动 – docker run 。 重新启动已处于终止/退出状态（Stopped/Exited）的容器 – docker start 。 docker run 命令等价于先执行 docker create 命令，再执行 docker start 命令。当用 docker run 命令创建并启动容器时，Docker 在后台运行的标准操作包括： 检查本地时候存在指定的镜像，不存在就会从 Docker Store 下载。 利用镜像创建并启动一个容器。 分配一个文件系统，并在只读的镜像层外面挂载一个可读写层。 从宿主机配置的网桥接口中桥接一个虚拟接口到容器中去。 从地址池配置一个 IP 地址给容器。 执行用户指定的应用程序。 执行完毕后容器被终止。 交互模式运行容器，例如： 12$ docker run -it ubuntu:14.04 /bin/bashroot@0b2616b0e5a8:/# -t：让 Docker 分配一个伪终端（ pseudo-tty ) 并绑定到容器的标准输入上。 -i：保持容器的标准输入处于打开状态。 用户可以通过 Ctrl+d 或输入 exit 命令来退出容器。 12root@0b2616b0e5a8:/#exitexit 对于所创建的 bash 容器， 当使用 exit 命令退出后，该容器就自动处于终止状态。这是应为对于 Docker 容器来说，当运行的应用（此例中为 bash）退出后，容器也就没有必要运行了。 守护态运行很多时候，需要让 Docker 容器在后台以守护态形式（Daemonized）运行。用户可以通过添加 -d 参数来实现。 12$ docker run -d ubuntu:14.04 /bin/sh -c "while true; do echo hello world; sleep 1; done" cce55… 启动容器后会返回一个唯一的 ID。通过 docker logs 命令来获取容器的输出信息。 1$ docker logs ce55 终止，重启容器使用 Docker stop 命令终止的一个或多个处于运行态的容器。 1$ docker stop cce55 使用 Docker restart 命令重启一个或多个处于运行态的容器。 1$ docker restart cce55 进入容器语法： docker exec -it [CONTAINER_ID] [COMMAND] 使用 -d 参数时，容器启动后会进入后台，用户无法看到容器中的信息。如果需要进入容器进行操作，有如下两种方法： docker attach 命令：不推荐使用，容易阻塞。 docker exec 命令： 是 1.3 版本提供的一个更加方便的工具。 删除容器语法： docker rm [CONTAINER_ID/CONTINER_NAME] [CONTAINER_ID/CONTINER_NAME…]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[配置 HTTPS 访问的 Harbor:v0.45]]></title>
      <url>%2F2016%2F10%2F24%2Fharbor-configure-https-v0.4.5%2F</url>
      <content type="text"><![CDATA[Harbor 默认 HTTP 请求注册服务器，并不分发任何证书。使得其配置相对简单。然而，强烈建议在生产环境中增强安全性。Harbor 有一个 Nginx 实例为所有服务提供反向代理，你可以在 Nginx 配置中启用 HTTPS 。 获取证书假定你的仓库的 hostname 是 reg.yourdomain.com，并且其 DNS 记录指向主机正在运行的 Harbor。首先，你需要获取一个 CA 证书。证书通常包含一个 .crt 文件和 .key 文件，例如， yourdomain.com.crt 和 yourdomain.com.key。 在测试或开发环境中，你可能会选择使用自签名证书，而不是购买 CA 证书。下面的命令会生成自签名证书： 1) 生成自签名 CA 证书: 123openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout ca.key \ -x509 -days 365 -out ca.crt 2) 生成证书签名请求: 如果使用 FQDN (完全限定域名) 如 reg.yourdomain.com 作为你的注册服务器连接，那你必须使用 reg.yourdomain.com 作为 CN (Common Name)。另外，如果使用 IP 地址作为你的注册服务器连接，CN 可以是你的名字等等： 123openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key \ -out yourdomain.com.csr 3) 生成注册服务器证书: 假定你使用类似 reg.yourdomain.com 这样的 FQND 作为注册服务器连接，运行下面的命令为你的注册服务器生成证书： 1openssl x509 -req -days 365 -in yourdomain.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out yourdomain.com.crt 假定你使用 IP ， 如 192.168.1.101 作为注册服务器连接，你可以运行以下命令： 1234echo subjectAltName = IP:192.168.1.101 &gt; extfile.cnfopenssl x509 -req -days 365 -in yourdomain.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out yourdomain.com.crt 配置和安装在取得 yourdomain.com.crt 和 yourdomain.com.key 文件后，你可以将它们放置在如 目录下：1234```bashcp yourdomain.com.crt /root/cert/cp yourdomain.com.key /root/cert/ 接下来，编辑 harbor/make/harbor.cfg, 更新 ， ```protocol```， ```ssl_cert``` 和 ```ssl_cert_key``` 属性：12345678910```bash#set hostnamehostname = reg.yourdomain.com#set ui_url_protocolui_url_protocol = https......#The path of cert and key files for nginx, they are applied only the protocol is set to httpsssl_cert = /root/cert/yourdomain.com.crtssl_cert_key = /root/cert/yourdomain.com.key 为 Harbor 生成配置文件： 12# 工作目录 harbor/make./prepare 如果 Harbor 已经在运行，则停止并删除实例。你的镜像数据将保留在文件系统中。 1234# 工作目录 harbor/make# 在运行 compose 前， 需将 docker-compose.tpl 文件重命名为 docker-compose.yamlmv docker-compose.tpl docker-compose.yamldocker-compose down 最后，重启 Harbor: 1docker-compose up -d 在为 Harbor 配置 HTTPS 完成后，你可以通过以下步骤对它进行验证： 打开浏览器，并输入地址：https://reg.yourdomain.com。应该会显示 Harbor 的界面。 在装有 Docker daemon 的机器上，确保 Docker engine 配置文件中没有配置 “-insecure-registry”，并且，你必须将已生成的 ca.crt 文件放入 /etc/docker/certs.d/yourdomain.com(或 / etc/docker/certs.d/your registry host IP) 目录下，如果这个目录不存在，则创建它。如果你将 nginx 的 443 端口映射到其他端口上了，你需要创建目录的为 /etc/docker/certs.d/yourdomain.com:port(/etc/docker/certs.d/your registry host IP:port)。然后，运行如下命令验证是否安装成功。 1docker login reg.yourdomain.com 如果你将 nginx 443 端口映射到其他端口上，则需要在登录时添加端口号，如： 1docker login reg.yourdomain.com:port 疑难解答1. 你可能是通过证书发行商获取中间证书。在这种情况下，你应该合并中间证书与自签证书，通过如下命令即可实现： 1cat intermediate-certificate.pem &gt;&gt; yourdomain.com.crt 2. 在一些运行着 docker daemon 的系统中，你可能需要操作系统级别的信任证书。 在 Ubuntu 上, 可以通过以下命令来完成： 123456789cp youdomain.com.crt /usr/local/share/ca-certificates/reg.yourdomain.com.crtupdate-ca-certificates ``` - 在 Red Hat (CentOS etc) 上, 命令如下: ```bashcp yourdomain.com.crt /etc/pki/ca-trust/source/anchors/reg.yourdomain.com.crtupdate-ca-trust]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2014%2F12%2F22%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
